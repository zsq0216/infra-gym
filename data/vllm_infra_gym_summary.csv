instance_id,issue_url,pr_url,issue_title,difficulty,phase,symptoms,component,root_causes,num_source_files,source_additions,source_deletions,num_test_files,base_sha
vllm__10324__10164,https://github.com/vllm-project/vllm/issues/10324,https://github.com/vllm-project/vllm/pull/10164,"[Bug] custom chat template sends to model [{'type': 'text', 'text': '...'}]",hard,Inference/Serving,Unexpected Output,Request Parser,Misconfiguration,2,108,34,1,803f37eaaa11568f65acbf0bcd1044fb9b1610bf
vllm__10693__10705,https://github.com/vllm-project/vllm/issues/10693,https://github.com/vllm-project/vllm/pull/10705,[Bug]: MambaCacheManager Can Possibly Run Out of Free Slots,medium,Inference/Serving,Crash,Resource Manager,Concurrency Issue,2,10,4,2,e2251109c746f0d08ab9b37b5abcf44ca105d426
vllm__2059__2088,https://github.com/vllm-project/vllm/issues/2059,https://github.com/vllm-project/vllm/pull/2088,mixtral-8x7B-Instruct-v0.1 giving garbage output on long prompts,medium,Inference/Serving,Unexpected Output,Input Processor,Incorrect Algorithm Implementation,1,6,6,4,096827c2846e7a769cf20e34c46b8eada444cc1e
vllm__2064__2088,https://github.com/vllm-project/vllm/issues/2064,https://github.com/vllm-project/vllm/pull/2088,[BUG] Mistral/Mixtral generate nonsense past 4096 tokens in prompt,medium,Inference/Serving,Unexpected Output,Input Processor,Incorrect Algorithm Implementation,1,6,6,4,096827c2846e7a769cf20e34c46b8eada444cc1e
vllm__3572__3672,https://github.com/vllm-project/vllm/issues/3572,https://github.com/vllm-project/vllm/pull/3672,[Bug]: Should check whether stop sequence appears anyway in the output text instead of just endswith,hard,Inference/Serving,Unexpected Output,Output Processor,Incorrect Algorithm Implementation,5,89,35,3,0258b7a94b08321ca01cf170f867b67c1920af87
vllm__3574__3672,https://github.com/vllm-project/vllm/issues/3574,https://github.com/vllm-project/vllm/pull/3672,[Bug]: During streaming tokens should not be emitted if they could form a stop sequence,hard,Inference/Serving,Unexpected Output,Output Processor,Incorrect Algorithm Implementation,5,89,35,3,0258b7a94b08321ca01cf170f867b67c1920af87
vllm__3593__6751,https://github.com/vllm-project/vllm/issues/3593,https://github.com/vllm-project/vllm/pull/6751,ModuleNotFoundError: No module named 'transformers_modules' with API serving using phi-2b,hard,Inference/Serving,Crash,Model Loader,Dependent Module Issue,3,68,0,1,711f3a7806de8729e8e9cedf04e056c374d8e626
vllm__3707__3731,https://github.com/vllm-project/vllm/issues/3707,https://github.com/vllm-project/vllm/pull/3731,[Bug]: logprobs=0 behavior inconsistent with OpenAI spec,hard,Inference/Serving,Unexpected Output,Output Processor,Misused API,4,5,7,1,d8658c8cc16d67e6690cb0f3f340e365652ce80b
vllm__3793__5036,https://github.com/vllm-project/vllm/issues/3793,https://github.com/vllm-project/vllm/pull/5036,[Bug]: RuntimeError: No suitable kernel. h_in=16 h_out=3424 dtype=Float out_dtype=BFloat16,hard,Inference/Serving,Crash,Operators@@Operators,Incorrect Algorithm Implementation@@Incompatible Backend,38,2138,3783,9,f05840368335aa9c8184239d3c8bd986e44692f7
vllm__3793__5356,https://github.com/vllm-project/vllm/issues/3793,https://github.com/vllm-project/vllm/pull/5356,[Bug]: RuntimeError: No suitable kernel. h_in=16 h_out=3424 dtype=Float out_dtype=BFloat16,hard,Inference/Serving,Crash,Operators@@Operators,Incorrect Algorithm Implementation@@Incompatible Backend,11,767,311,4,16620f439db1f2cc91b5582b59fc8845cbb02881
vllm__3793__5498,https://github.com/vllm-project/vllm/issues/3793,https://github.com/vllm-project/vllm/pull/5498,[Bug]: RuntimeError: No suitable kernel. h_in=16 h_out=3424 dtype=Float out_dtype=BFloat16,easy,Inference/Serving,Crash,Operators@@Operators,Incorrect Algorithm Implementation@@Incompatible Backend,1,8,0,1,0f0d8bc065f3608e7657a9696f5d2d7c0d6722d1
vllm__4019__4079,https://github.com/vllm-project/vllm/issues/4019,https://github.com/vllm-project/vllm/pull/4079,[Bug][ROCm]: Process killed during tensor-parallel inference,hard,Inference/Serving,Crash,Communicator@@Communicator,Incompatible Backend@@Concurrency Issue,6,93,8,1,705578ae14b648782a8a321dd0903c163bd77375
vllm__4054__4197,https://github.com/vllm-project/vllm/issues/4054,https://github.com/vllm-project/vllm/pull/4197,[Bug]: Incorrect Data Type Conversion for MultiModalData,hard,Inference/Serving,Feature Failure,Input Processor@@Input Processor,Incorrect Type@@Misused API,23,858,214,6,dfbe60dc62409f03aa9eebc70ab2582ae64f0e1f
vllm__4100__4105,https://github.com/vllm-project/vllm/issues/4100,https://github.com/vllm-project/vllm/pull/4105,[Bug]: --engine-use-ray is broken.,easy,Inference/Serving,Crash,Engine Initializer,Misconfiguration,1,3,4,1,37e84a403d6d11b670a42e84153204cd8b76b849
vllm__4127__4128,https://github.com/vllm-project/vllm/issues/4127,https://github.com/vllm-project/vllm/pull/4128,[Bug][Chunked prefill]: head size has to be power of two,medium,Inference/Serving,Crash,Operators,Incorrect Conditional Logic,1,27,17,1,a53222544c6385ee314a26fdf42eb14f5b4e5ad9
vllm__4171__4573,https://github.com/vllm-project/vllm/issues/4171,https://github.com/vllm-project/vllm/pull/4573,"[Bug]: Server crash for bloom-3b while use prefix_caching, `AssertionError assert Lk in {16, 32, 64, 128}`",medium,Inference/Serving,Crash,Operators,Incorrect Algorithm Implementation,1,25,16,1,cc466a32903d53d0ceca459b766d74ad668c8f87
vllm__4293__4412,https://github.com/vllm-project/vllm/issues/4293,https://github.com/vllm-project/vllm/pull/4412,[Bug]: Engine iteration timed out. This should never happen occurred when vllm 0.4.1 deployed llama3. ,hard,Inference/Serving,System Hang,Resource Manager,Incorrect Cache Management,67,875,360,15,15aba081f33e6d048422df6dcdb94301d08d13e6
vllm__4339__4355,https://github.com/vllm-project/vllm/issues/4339,https://github.com/vllm-project/vllm/pull/4355,[Bug]: Invalid inputs do not result in error,hard,Inference/Serving,Feature Failure,Request Parser,Incorrect Request Parsing,7,97,55,1,96e90fdeb3c4ebacfe24513556afccb918722b7c
vllm__4362__5233,https://github.com/vllm-project/vllm/issues/4362,https://github.com/vllm-project/vllm/pull/5233,[Bug]: v0.4.1 VLLM_USE_MODELSCOPE not working,medium,Inference/Serving,Feature Failure,Engine Initializer,Incorrect Type,2,11,2,1,f775a07e30fdeafc14f53fe502b262b00540dd71
vllm__4365__4389,https://github.com/vllm-project/vllm/issues/4365,https://github.com/vllm-project/vllm/pull/4389,"[Bug]:  When I specify `max-tokens` and `min-tokens` at the same time, the service reports an error in `_apply_min_tokens_penalty`",medium,Inference/Serving,Crash,Output Processor,Incorrect Conditional Logic,3,11,12,1,603ad8481594321ceae7d54e2c0050b3638c6502
vllm__4422__4451,https://github.com/vllm-project/vllm/issues/4422,https://github.com/vllm-project/vllm/pull/4451,[Bug]: `assert num_new_tokens == 1` fails when `SamplingParams.n` is not `1` and `max_tokens` is large.,medium,Inference/Serving,Crash,Resource Manager,Incorrect Algorithm Implementation,2,29,8,4,6ad58f42c59eaee0a57c89f1feb08757524b93cf
vllm__4742__5178,https://github.com/vllm-project/vllm/issues/4742,https://github.com/vllm-project/vllm/pull/5178,[Bug]: ValueError when using LoRA with CohereForCausalLM model,hard,Inference/Serving,Crash,Model Loader,Incompatible Model,2,52,6,1,856c990041bf6cf4b2397401d4b18531382ecb50
vllm__4756__5169,https://github.com/vllm-project/vllm/issues/4756,https://github.com/vllm-project/vllm/pull/5169,[Bug]: CUDA error when running mistral-7b + lora with tensor_para=8,medium,Inference/Serving,Crash,Resource Manager,Incorrect Multi-device Management,1,13,11,1,519e8e4182af8e25d78b062ba5e613df661e6e5d
vllm__4789__5654,https://github.com/vllm-project/vllm/issues/4789,https://github.com/vllm-project/vllm/pull/5654,[Bug]: Async engine hangs with 0.4.* releases,hard,Inference/Serving,System Hang,Job Manager,Concurrency Issue,2,192,2,3,8a173382c80d6730e1bbc81f932ac3721ab2cd9d
vllm__4883__5194,https://github.com/vllm-project/vllm/issues/4883,https://github.com/vllm-project/vllm/pull/5194,"[Bug]: assert parts[0] == ""base_model"" AssertionError",medium,Inference/Serving,Crash,Model Loader,Incompatible Model,1,7,8,1,5884c2b454d9a6e16646e949d7308a4cfae3ac12
vllm__4904__6223,https://github.com/vllm-project/vllm/issues/4904,https://github.com/vllm-project/vllm/pull/6223,[Bug]: llm_engine_example.py (more requests) get stuck,medium,Inference/Serving,System Hang,Output Processor@@Output Processor,Concurrency Issue@@Misused API,3,30,10,1,4f0e0ea131ef40654faa26fa21196031754df53a
vllm__5067__5355,https://github.com/vllm-project/vllm/issues/5067,https://github.com/vllm-project/vllm/pull/5355,[Bug]: The VRAM usage of calculating log_probs is not considered in profile run,hard,Inference/Serving,Crash,Output Processor,Incorrect Algorithm Implementation,4,138,13,1,519e8e4182af8e25d78b062ba5e613df661e6e5d
vllm__5088__5121,https://github.com/vllm-project/vllm/issues/5088,https://github.com/vllm-project/vllm/pull/5121,[Bug]: Gemma model fails with GPTQ marlin,medium,Inference/Serving,Feature Failure,Model Conversion Tools,Incompatible Model,1,4,8,1,5bf185a1c48fdca524dd76aec4a1424b3a09c9a1
vllm__5088__5122,https://github.com/vllm-project/vllm/issues/5088,https://github.com/vllm-project/vllm/pull/5122,[Bug]: Gemma model fails with GPTQ marlin,easy,Inference/Serving,Feature Failure,Model Conversion Tools,Incompatible Model,0,0,0,1,5bf185a1c48fdca524dd76aec4a1424b3a09c9a1
vllm__5088__5145,https://github.com/vllm-project/vllm/issues/5088,https://github.com/vllm-project/vllm/pull/5145,[Bug]: Gemma model fails with GPTQ marlin,easy,Inference/Serving,Feature Failure,Model Conversion Tools,Incompatible Model,0,0,0,1,856c990041bf6cf4b2397401d4b18531382ecb50
vllm__5152__8975,https://github.com/vllm-project/vllm/issues/5152,https://github.com/vllm-project/vllm/pull/8975,[Bug] [spec decode] [flash_attn]: CUDA illegal memory access when calling flash_attn_cuda.fwd_kvcache,easy,Inference/Serving,Crash,Operators,Incorrect Algorithm Implementation,1,0,2,1,1cabfcefb64a489c8ff9dcb289b4dd47cf8f89cf
vllm__5274__5355,https://github.com/vllm-project/vllm/issues/5274,https://github.com/vllm-project/vllm/pull/5355,[Bug]: high gpu_memory_utilization with 'OOM' and low gpu_memory_utilization with 'No available memory for the cache blocks',hard,Inference/Serving,Abnormal Performance,Resource Manager,Incorrect Algorithm Implementation,4,138,13,1,519e8e4182af8e25d78b062ba5e613df661e6e5d
vllm__5334__6223,https://github.com/vllm-project/vllm/issues/5334,https://github.com/vllm-project/vllm/pull/6223,[Bug]: Unexpected prompt token logprob behaviors of llama 2 when setting echo=True for openai-api server,medium,Inference/Serving,Unexpected Output,Output Processor,Incorrect Formatting,3,30,10,1,4f0e0ea131ef40654faa26fa21196031754df53a
vllm__5337__5347,https://github.com/vllm-project/vllm/issues/5337,https://github.com/vllm-project/vllm/pull/5347,[Bug]: non-deterministic Python gc order leads to flaky tests,easy,Inference/Serving,Abnormal Performance,Resource Manager,Concurrency Issue,0,0,0,14,dc49fb892ca32cb364dfc39d711ab84d3b35a28f
vllm__5544__5546,https://github.com/vllm-project/vllm/issues/5544,https://github.com/vllm-project/vllm/pull/5546,[Bug]: Distribute Tests PR test fails,easy,Inference/Serving,Feature Failure,Tester / Benchmarker,Incorrect Algorithm Implementation,0,0,0,1,319ad7f1d386699e94f629341c9988a926821f24
vllm__5550__5355,https://github.com/vllm-project/vllm/issues/5550,https://github.com/vllm-project/vllm/pull/5355,[Bug]: OOM when setting prompt_logprobs=1,hard,Inference/Serving,Crash,Resource Manager,Incorrect Resource Allocation,4,138,13,1,519e8e4182af8e25d78b062ba5e613df661e6e5d
vllm__5657__5810,https://github.com/vllm-project/vllm/issues/5657,https://github.com/vllm-project/vllm/pull/5810,[Bug]: Ray distributed backend does not support out-of-tree models via ModelRegistry APIs,hard,Inference/Serving,Feature Failure,Model Loader,Incompatible Backend,8,43,8,3,ba991d5c84adbc0685075af88333c688ddb06011
vllm__5736__5772,https://github.com/vllm-project/vllm/issues/5736,https://github.com/vllm-project/vllm/pull/5772,[Bug]: which torchvision version required,hard,Engine Setup,Crash,Setup Tools,Incompatible Version,4,68,5,1,832ea88fcb4819037b685fb47b3a0de37f2804d3
vllm__5767__5772,https://github.com/vllm-project/vllm/issues/5767,https://github.com/vllm-project/vllm/pull/5772,[Bug]: Different Image Size support with Phi-3-Vision and torchvision dependency,hard,Inference/Serving,Feature Failure,Input Processor,Insufficient Multimodal Support,4,68,5,1,832ea88fcb4819037b685fb47b3a0de37f2804d3
vllm__5793__6795,https://github.com/vllm-project/vllm/issues/5793,https://github.com/vllm-project/vllm/pull/6795,[Bug]: Different quality responses using GPTQ / marlin kernels on A10 vs A100 GPUs,hard,Inference/Serving,Unexpected Output,Operators@@Operators,Incorrect Algorithm Implementation@@Incompatible Backend,5,158,41,1,1ad86acf1789650e2ff27586e36a8159d52755dd
vllm__590__941,https://github.com/vllm-project/vllm/issues/590,https://github.com/vllm-project/vllm/pull/941,GPTJ output not consistent with that of transformers,hard,Inference/Serving,Unexpected Output,Operators,Incorrect Algorithm Implementation,4,84,54,1,fbd80ad4092c4bc48ce672f0435c1d1362aee052
vllm__5938__6029,https://github.com/vllm-project/vllm/issues/5938,https://github.com/vllm-project/vllm/pull/6029,[Bug]: Illegal memory access for MoE kernel with large workloads,hard,Inference/Serving,Crash,Operators,Incorrect Algorithm Implementation,2,73,47,1,80ca1e6a3a28a0373dc00c5b4fe956c16de952fa
vllm__5982__6089,https://github.com/vllm-project/vllm/issues/5982,https://github.com/vllm-project/vllm/pull/6089,[Bug]: AttributeError: 'NoneType' object has no attribute 'prefill_metadata',hard,Inference/Serving,Crash,Job Manager,Incorrect Exception Handling,38,324,334,5,7cd2ebb0251fd1fd0eec5c93dac674603a22eddd
vllm__5983__6089,https://github.com/vllm-project/vllm/issues/5983,https://github.com/vllm-project/vllm/pull/6089,[Bug]: TypeError: FlashAttentionMetadata.__init__() missing 10 required positional arguments,hard,Inference/Serving,Crash,Operators,Misused API,38,324,334,5,7cd2ebb0251fd1fd0eec5c93dac674603a22eddd
vllm__6004__6007,https://github.com/vllm-project/vllm/issues/6004,https://github.com/vllm-project/vllm/pull/6007,[Bug]: `distributed_executor_backend=mp` does not work with GPTQ tp>1,hard,Inference/Serving,Crash,Engine Initializer,Concurrency Issue,11,82,66,2,af9ad46fca6e594797b83e5ecb2e1f31ca5e9fac
vllm__6015__10184,https://github.com/vllm-project/vllm/issues/6015,https://github.com/vllm-project/vllm/pull/10184,embedings  error                    python -m vllm.entrypoints.openai.api_server --trust-remote-code --model gte_Qwen2-7B-instruct --seed 48 --max-model-len 1000 --tensor-parallel-size 2 --gpu-memory-utilization 1 --dtype float16 ,hard,Inference/Serving,Feature Failure,Model Loader,Incompatible Model,8,147,78,11,972112d82f00e1396c0376cde78c083208b77127
vllm__6059__6080,https://github.com/vllm-project/vllm/issues/6059,https://github.com/vllm-project/vllm/pull/6080,[Bug][CI/Build]: Missing attribute 'nvmlDeviceGetHandleByIndex' in AMD tests,hard,Engine Setup,Crash,Setup Tools,Misused API,14,109,25,2,9d6a8daa87e2e0af3ff45d03d08ad5a94ec089a8
vllm__6137__6138,https://github.com/vllm-project/vllm/issues/6137,https://github.com/vllm-project/vllm/pull/6138,[Bug]: Spec. decode fails for requests with n>1 or best_of>1,hard,Inference/Serving,Feature Failure,Job Manager@@Job Manager,Incorrect Conditional Logic@@Incorrect Algorithm Implementation,6,71,20,2,22e79ee8f3930c39f40f6a1529e41594a607c6b4
vllm__6152__7411,https://github.com/vllm-project/vllm/issues/6152,https://github.com/vllm-project/vllm/pull/7411,"[Bug]: When tensor_parallel_size>1,  RuntimeError: Cannot re-initialize CUDA in forked subprocess.",medium,Inference/Serving,Crash,Engine Initializer,Concurrency Issue,2,34,9,1,86ab567bae0698425095e28cce67e9a31a261b72
vllm__6176__7998,https://github.com/vllm-project/vllm/issues/6176,https://github.com/vllm-project/vllm/pull/7998,"[Bug]: LLaVA 1.6 in 0.5.1: Exceptions after some bigger image request, stuck in faulty mode",medium,Inference/Serving,Crash,Input Processor,Insufficient Multimodal Support,1,18,3,1,f205c09854853172a446c92aa81eb7199da324ab
vllm__6231__6234,https://github.com/vllm-project/vllm/issues/6231,https://github.com/vllm-project/vllm/pull/6234,[Bug]: relative path doesn't work for Lora adapter model,hard,Inference/Serving,Crash,Model Loader,Misconfiguration,6,95,12,5,b5af8c223c3d70557e7d73ba3c4c2e9d56fc9694
vllm__6322__6339,https://github.com/vllm-project/vllm/issues/6322,https://github.com/vllm-project/vllm/pull/6339,[Bug]: VLLM 0.5.1 with LLaVA 1.6 exceptions,medium,Inference/Serving,Crash,Input Processor,Incorrect Tokenization,1,10,8,1,8a1415cf776b2b902f6429ecfc325877b57cbefe
vllm__6342__6345,https://github.com/vllm-project/vllm/issues/6342,https://github.com/vllm-project/vllm/pull/6345,[Bug]: OpenAI batch file format pydantic validation error,easy,Inference/Serving,Feature Failure,Request Parser,Incompatible Version,1,1,1,1,f8f9ff57ee365891fe9f54cd46df65cc9d5ccca0
vllm__6366__6367,https://github.com/vllm-project/vllm/issues/6366,https://github.com/vllm-project/vllm/pull/6367,[Bug]: Paligemma loading issues: RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same,medium,Inference/Serving,Crash,Resource Manager,Incorrect Type,2,11,4,1,6047187cd854eef114bd70c76469d5a839a07ef4
vllm__6449__6698,https://github.com/vllm-project/vllm/issues/6449,https://github.com/vllm-project/vllm/pull/6698,[Bug]: Seed issue with Pipeline Parallel,hard,Inference/Serving,Unexpected Output,Communicator,Incorrect Synchronization,15,124,119,6,150a1ffbfd3d0429d30fa5ab841f53903a0a8a62
vllm__6461__6463,https://github.com/vllm-project/vllm/issues/6461,https://github.com/vllm-project/vllm/pull/6463,"[Bug]: No metrics exposed at /metrics with 0.5.2 (0.5.1 is fine), possible regression?",medium,Inference/Serving,Feature Failure,Monitor & Logger,Misconfiguration,1,9,5,1,5bf35a91e465278b51b0c98fa1b13beb8b04d431
vllm__6607__6751,https://github.com/vllm-project/vllm/issues/6607,https://github.com/vllm-project/vllm/pull/6751,[Bug]: Phi-3-mini does not work when using Ray,hard,Inference/Serving,Feature Failure,Communicator,Incorrect Conditional Logic,3,68,0,1,711f3a7806de8729e8e9cedf04e056c374d8e626
vllm__6703__6757,https://github.com/vllm-project/vllm/issues/6703,https://github.com/vllm-project/vllm/pull/6757,[Bug]: Flash-attn on-GPU advance step optimization bug with spec decode on LLama 405B,medium,Inference/Serving,Crash,Operators,Incorrect Conditional Logic,3,30,4,1,5448f67635570cee6fc23c7cd166e9d8f7595009
vllm__6756__7445,https://github.com/vllm-project/vllm/issues/6756,https://github.com/vllm-project/vllm/pull/7445,[Bug]: Unable to run meta-llama/Llama-Guard-3-8B-INT8,hard,Inference/Serving,Crash,Model Loader,Misconfiguration,4,333,123,2,029c71de11bc3bcf84a1b3cf9d91e79ab6949799
vllm__6765__6960,https://github.com/vllm-project/vllm/issues/6765,https://github.com/vllm-project/vllm/pull/6960,[Bug]: FP8 Quantization (static and dynamic) incompatible with `--cpu-offload-gb`,hard,Inference/Serving,Feature Failure,Model Loader,Incompatible Backend,2,79,27,1,d7a299edaa5d23f3d7d5c98b53872a8ced9aad80
vllm__6952__6960,https://github.com/vllm-project/vllm/issues/6952,https://github.com/vllm-project/vllm/pull/6960,[Bug]: `RuntimeError: b_q_weight is not on GPU` CPU Offloading,hard,Inference/Serving,Crash,Operators,Misused API,2,79,27,1,d7a299edaa5d23f3d7d5c98b53872a8ced9aad80
vllm__7149__7153,https://github.com/vllm-project/vllm/issues/7149,https://github.com/vllm-project/vllm/pull/7153,"[Bug]: The deployment function requires a divisible relationship, but the model structure does not meet this requirement. What should I do?",hard,Inference/Serving,Crash,Model Loader,Incompatible Model,10,413,255,4,9118217f58c39040aa935b7c85250c7364ffa72d
vllm__7151__7411,https://github.com/vllm-project/vllm/issues/7151,https://github.com/vllm-project/vllm/pull/7411,"[Bug]: After updating to vllm 0.5.3.post1, ""Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.""",medium,Inference/Serving,Crash,Engine Initializer,Incompatible Backend,2,34,9,1,86ab567bae0698425095e28cce67e9a31a261b72
vllm__7160__7164,https://github.com/vllm-project/vllm/issues/7160,https://github.com/vllm-project/vllm/pull/7164,[Bug]: InternVL2 Mismatch in number of image tokens and image embedding size,hard,Inference/Serving,Feature Failure,Input Processor,Mismatched Shape,1,54,30,1,e9630458c7b11732e147c120817c53420280d471
vllm__7188__7279,https://github.com/vllm-project/vllm/issues/7188,https://github.com/vllm-project/vllm/pull/7279,[Bug]: No metrics exposed at /metrics with 0.5.4 and metrics completely unavailable,hard,Inference/Serving,Feature Failure,Monitor & Logger,Misconfiguration,5,187,107,2,ab7165f2c7ea358df969d68a0fb0ce9bb184a083
vllm__7204__7225,https://github.com/vllm-project/vllm/issues/7204,https://github.com/vllm-project/vllm/pull/7225,[Bug]: GPTQ Marlin with cpu-offload-gb fails on `0.5.4`,medium,Inference/Serving,Crash,Model Loader,Incorrect Conditional Logic,2,9,8,2,fd95e026e0f9f50bacf1a63ef419df8bacfc99c0
vllm__7272__7164,https://github.com/vllm-project/vllm/issues/7272,https://github.com/vllm-project/vllm/pull/7164,[Bug]: OpenGVLab/InternVL2-26B AsyncEngineDeadError: Task finished unexpectedly - value error in merge_vision_embeddings,hard,Inference/Serving,Crash,Input Processor,Insufficient Multimodal Support,1,54,30,1,e9630458c7b11732e147c120817c53420280d471
vllm__7373__7392,https://github.com/vllm-project/vllm/issues/7373,https://github.com/vllm-project/vllm/pull/7392,"[Bug]: Phi-3-vision: ERROR 08-09 11:41:40 async_llm_engine.py:56] RuntimeError: stack expects each tensor to be equal size, but got [1933, 4096] at entry 0 and [2509, 4096] at entry 1",medium,Inference/Serving,Crash,Input Processor,Mismatched Shape,2,19,16,2,baa240252ed8d44ae51455e24ebe644220d52116
vllm__7388__7410,https://github.com/vllm-project/vllm/issues/7388,https://github.com/vllm-project/vllm/pull/7410,[Bug]: `facebook/chameleon-30b` triggers assertion error while loading weights,hard,Inference/Serving,Crash,Model Loader,Incompatible Model,52,299,128,7,9ba85bc1527928f30ec56961520d7e07ee385167
vllm__7505__7508,https://github.com/vllm-project/vllm/issues/7505,https://github.com/vllm-project/vllm/pull/7508,[Bug]: Error in how HiddenStates are handled for speculative decoding,hard,Inference/Serving,Crash,Operators,Mismatched Shape,4,81,32,2,ab7165f2c7ea358df969d68a0fb0ce9bb184a083
vllm__7516__7521,https://github.com/vllm-project/vllm/issues/7516,https://github.com/vllm-project/vllm/pull/7521,[Bug]: ImportError related to compressed tensors module,hard,Inference/Serving,Crash,Model Loader@@Model Loader,Incompatible Backend@@Dependent Module Issue,7,83,12,1,d3d9cb6e4b8185b4e56e1dda92c6fc31cdc05de1
vllm__7718__7710,https://github.com/vllm-project/vllm/issues/7718,https://github.com/vllm-project/vllm/pull/7710,[Bug]: Error loading microsoft/Phi-3.5-vision-instruct,hard,Inference/Serving,Feature Failure,Model Loader,Incompatible Model,5,36,12,1,53328d7536b3e8ea4863e351b09e4284f5601cae
vllm__7742__8047,https://github.com/vllm-project/vllm/issues/7742,https://github.com/vllm-project/vllm/pull/8047,[Bug]: Requesting Prompt Logprobs with an MLP Speculator Crashes the Server,hard,Inference/Serving,Crash,Operators,Incorrect Algorithm Implementation,7,113,42,7,a8c1d161a7d87dbc6c7cccfce303dcbe2e4ed6be
vllm__7920__8157,https://github.com/vllm-project/vllm/issues/7920,https://github.com/vllm-project/vllm/pull/8157,"[Bug]: OpenAI server errors out with ""ZMQError Too many open files"" under heavy load",hard,Inference/Serving,Crash,Communicator,Misused API,21,981,868,15,acd5511b6d0e196b273b6250201115b5c5cfd1ca
vllm__8194__7928,https://github.com/vllm-project/vllm/issues/8194,https://github.com/vllm-project/vllm/pull/7928,[Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError,hard,Inference/Serving,Crash,Job Manager,Concurrency Issue,8,362,81,1,7de49aa86c7f169eb0962b6db29ad53fff519ffb
vllm__8275__8299,https://github.com/vllm-project/vllm/issues/8275,https://github.com/vllm-project/vllm/pull/8299,"[Bug]: RuntimeError: shape mismatch: value tensor of shape [3328, 7168] cannot be broadcast to indexing result of shape [3328] for OpenGVLab/InternVL2-40B",easy,Inference/Serving,Crash,Input Processor@@Input Processor,Insufficient Multimodal Support@@Mismatched Shape,1,2,1,1,a1d874224d9c29ae84f3850474b4816f0ed9574b
vllm__8321__8357,https://github.com/vllm-project/vllm/issues/8321,https://github.com/vllm-project/vllm/pull/8357,[Bug]: 段错误 (核心已转储),hard,Inference/Serving,Crash,Operators,Incorrect Algorithm Implementation,7,422,162,1,1ef0d2efd07f93bc7b0cfb597d8947b49e2fdaac
vllm__8361__8375,https://github.com/vllm-project/vllm/issues/8361,https://github.com/vllm-project/vllm/pull/8375,[Bug]: internvl2 multi-prompt input with one image each get RuntimeError,easy,Inference/Serving,Crash,Input Processor@@Input Processor,Mismatched Shape@@Insufficient Multimodal Support,1,4,3,1,5a60699c452c0b9b8086a978d8572c257c2c3cc4
vllm__8369__8375,https://github.com/vllm-project/vllm/issues/8369,https://github.com/vllm-project/vllm/pull/8375,[Bug]: Crash after few multi image calls,easy,Inference/Serving,Crash,Input Processor,Mismatched Shape,1,4,3,1,5a60699c452c0b9b8086a978d8572c257c2c3cc4
vllm__8382__8415,https://github.com/vllm-project/vllm/issues/8382,https://github.com/vllm-project/vllm/pull/8415,[Bug]: Pixtral fails when limit_mm_per_prompt not set,hard,Inference/Serving,Crash,Model Loader,Incompatible Model,1,49,34,4,c6202daeedb22cd675942c37ae5e194549803c89
vllm__8411__8415,https://github.com/vllm-project/vllm/issues/8411,https://github.com/vllm-project/vllm/pull/8415,[Bug]: Pixtral inference not working correctly with LLMEngine/AsyncEngine,hard,Inference/Serving,Feature Failure,Model Loader,Incompatible Model,1,49,34,4,c6202daeedb22cd675942c37ae5e194549803c89
vllm__8461__9248,https://github.com/vllm-project/vllm/issues/8461,https://github.com/vllm-project/vllm/pull/9248,[Bug]: Model architectures ['Qwen2AudioForConditionalGeneration'] are not supported for now.,hard,Inference/Serving,Crash,Model Loader,Incompatible Model,6,514,17,1,e7116c017c86cb547f4d1888edaf13a9be2a4562
vllm__8578__8583,https://github.com/vllm-project/vllm/issues/8578,https://github.com/vllm-project/vllm/pull/8583,[Bug]: lm-format-enforcer guided decoding kills MQLLMEngine,hard,Inference/Serving,Crash,Operators,Incorrect Algorithm Implementation,3,83,52,1,2467b642dd9bde32a334fe5967efd78a53aa49da
vllm__8923__7168,https://github.com/vllm-project/vllm/issues/8923,https://github.com/vllm-project/vllm/pull/7168,[Bug]: Model multimodal config initialisation unhandled and irrelevant error when no architectures found,hard,Inference/Serving,Crash,Model Loader,Incorrect Exception Handling,66,2275,1274,3,83caf35e082b2657dce5f71ff965a13653a763b0
vllm__8947__8960,https://github.com/vllm-project/vllm/issues/8947,https://github.com/vllm-project/vllm/pull/8960,[Bug]: vllm serve --config.yaml - Order of arguments matters?,medium,Inference/Serving,Feature Failure,Engine Initializer,Misconfiguration,2,12,2,2,2ae25f79cf1e8d21f7bcba097e4c039463c22be4
vllm__8983__8986,https://github.com/vllm-project/vllm/issues/8983,https://github.com/vllm-project/vllm/pull/8986,[Bug]: Distributed inference fails on certain multimodal models,medium,Inference/Serving,Crash,Operators,Mismatched Shape,2,12,11,1,82f3937e599a4f088a62e59abe81d51e11bb8f83
vllm__8988__9034,https://github.com/vllm-project/vllm/issues/8988,https://github.com/vllm-project/vllm/pull/9034,[Bug]: openai.serving_chat tries to call _create_chat_logprobs when the output.text is empty,medium,Inference/Serving,Crash,Output Processor,Incorrect Exception Handling,3,14,2,1,e9d517f27673ec8736c026f2311d3c250d5f9061
vllm__9019__9020,https://github.com/vllm-project/vllm/issues/9019,https://github.com/vllm-project/vllm/pull/9020,[Bug]: ToolCall IDs generated by Mistral tool call parser do not comply with Mistral tool calls and template constraints,medium,Inference/Serving,Feature Failure,Output Processor,Incorrect Formatting,1,18,2,2,7f60520deb05d2e097b408e3310f1d383fbf1de6
vllm__905__1154,https://github.com/vllm-project/vllm/issues/905,https://github.com/vllm-project/vllm/pull/1154,vLLM doesn't support context length exceeding about 13k,hard,Inference/Serving,Feature Failure,Operators,Incorrect Conditional Logic,6,80,2,1,8d926e91f183a73b92aa3e254e894ecd02018dd5
vllm__9076__9083,https://github.com/vllm-project/vllm/issues/9076,https://github.com/vllm-project/vllm/pull/9083,[Bug]: vllm overrides transformer's Autoconfig for mllama,easy,Inference/Serving,Crash,Model Loader,Misconfiguration,1,0,8,2,05d686432f2e13296127962861b21c25cdcdfc8b
vllm__9834__10198,https://github.com/vllm-project/vllm/issues/9834,https://github.com/vllm-project/vllm/pull/10198,[Bug]: Sampling parameter fixed issue while doing speculative sampling verification step,medium,Inference/Serving,Unexpected Output,Operators,Incorrect Algorithm Implementation,1,1,13,2,0a4d96850013eb2c295b25df53177ad2302110ca
