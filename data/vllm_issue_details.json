{
  "https://github.com/vllm-project/vllm/issues/7272": {
    "issue_number": 7272,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7272",
    "title": "[Bug]: OpenGVLab/InternVL2-26B AsyncEngineDeadError: Task finished unexpectedly - value error in merge_vision_embeddings",
    "body": "### Your current environment\n\n```\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1042-azure-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          48\r\nOn-line CPU(s) list:             0-47\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7V13 64-Core Processor\r\nCPU family:                      25\r\nModel:                           1\r\nThread(s) per core:              1\r\nCore(s) per socket:              48\r\nSocket(s):                       1\r\nStepping:                        1\r\nBogoMIPS:                        4890.89\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:               Microsoft\r\nVirtualization type:             full\r\nL1d cache:                       1.5 MiB (48 instances)\r\nL1i cache:                       1.5 MiB (48 instances)\r\nL2 cache:                        24 MiB (48 instances)\r\nL3 cache:                        192 MiB (6 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-23\r\nNUMA node1 CPU(s):               24-47\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] transformers==4.41.2\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NODE    0-23    0               N/A\r\nGPU1    NV12     X      SYS     24-47   1               N/A\r\nNIC0    NODE    SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_an0\r\n```\r\n```\n\n### üêõ Describe the bug\n\nTried to host and test `OpenGVLab/InternVL2-26B` model using vllm image [v0.5.4](https://hub.docker.com/layers/vllm/vllm-openai/v0.5.4/images/sha256-7ab0cf7b287876cec65752a1b7ac99790ecd2a609da80c4d1dd1fbeaf987abf6?context=explore) using \r\n\r\n- `--chat-template=\"InternVL2-template.jinja\"  `\r\n- `--load-format=safetensors `\r\n- `--trust-remote-code `\r\n- `--max_model_len 4096`\r\n\r\ngetting the error:\r\n\r\n```\r\n\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: Attempted to assign 7 x 256 = 1792 image tokens to 504 placeholders, Traceback (most recent call last):\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 65, in start_worker_execution_loop\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     output = self.execute_model(execute_model_req=None)\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 273, in execute_model\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     output = self.model_runner.execute_model(\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1363, in execute_model\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/internvl.py\", line 391, in forward\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     inputs_embeds = merge_vision_embeddings(input_ids, inputs_embeds,\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 29, in merge_vision_embeddings\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]     raise ValueError(\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226] ValueError: Attempted to assign 7 x 256 = 1792 image tokens to 504 placeholders\r\nlocal_llm_host_internVL2_instruct  | (VllmWorkerProcess pid=110) ERROR 08-07 14:32:00 multiproc_worker_utils.py:226]\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57] Engine background task failed\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57] Traceback (most recent call last):\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 47, in _log_task_completion\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     return_value = task.result()\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 642, in run_engine_loop\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     result = task.result()\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 585, in engine_step\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     request_outputs = await self.engine.step_async(virtual_engine)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 254, in step_async\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     output = await self.model_executor.execute_model_async(\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 175, in execute_model_async\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     return await self._driver_execute_model_async(execute_model_req)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 224, in _driver_execute_model_async\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     return await self.driver_exec_model(execute_model_req)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     result = self.fn(*self.args, **self.kwargs)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 273, in execute_model\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     output = self.model_runner.execute_model(\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     return func(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1363, in execute_model\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     hidden_or_intermediate_states = model_executable(\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     return self._call_impl(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     return forward_call(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/internvl.py\", line 391, in forward\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     inputs_embeds = merge_vision_embeddings(input_ids, inputs_embeds,\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 29, in merge_vision_embeddings\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57]     raise ValueError(\r\nlocal_llm_host_internVL2_instruct  | ERROR 08-07 14:32:00 async_llm_engine.py:57] ValueError: Attempted to assign 7 x 256 = 1792 image tokens to 504 placeholders\r\nlocal_llm_host_internVL2_instruct  | INFO 08-07 14:32:00 async_llm_engine.py:181] Aborted request chat-f119ffe9e1e54cf3a8fe1d999db26ce9.\r\nlocal_llm_host_internVL2_instruct  | Exception in callback _log_task_completion(error_callback=<bound method...7fc6e2de9a50>>)(<Task finishe...laceholders')>) at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:37\r\nlocal_llm_host_internVL2_instruct  | handle: <Handle _log_task_completion(error_callback=<bound method...7fc6e2de9a50>>)(<Task finishe...laceholders')>) at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:37>\r\nlocal_llm_host_internVL2_instruct  | Traceback (most recent call last):\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 47, in _log_task_completion\r\nlocal_llm_host_internVL2_instruct  |     return_value = task.result()\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 642, in run_engine_loop\r\nlocal_llm_host_internVL2_instruct  |     result = task.result()\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 585, in engine_step\r\nlocal_llm_host_internVL2_instruct  |     request_outputs = await self.engine.step_async(virtual_engine)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 254, in step_async\r\nlocal_llm_host_internVL2_instruct  |     output = await self.model_executor.execute_model_async(\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 175, in execute_model_async\r\nlocal_llm_host_internVL2_instruct  |     return await self._driver_execute_model_async(execute_model_req)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 224, in _driver_execute_model_async\r\nlocal_llm_host_internVL2_instruct  |     return await self.driver_exec_model(execute_model_req)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nlocal_llm_host_internVL2_instruct  |     result = self.fn(*self.args, **self.kwargs)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 273, in execute_model\r\nlocal_llm_host_internVL2_instruct  |     output = self.model_runner.execute_model(\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nlocal_llm_host_internVL2_instruct  |     return func(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1363, in execute_model\r\nlocal_llm_host_internVL2_instruct  |     hidden_or_intermediate_states = model_executable(\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nlocal_llm_host_internVL2_instruct  |     return self._call_impl(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nlocal_llm_host_internVL2_instruct  |     return forward_call(*args, **kwargs)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/internvl.py\", line 391, in forward\r\nlocal_llm_host_internVL2_instruct  |     inputs_embeds = merge_vision_embeddings(input_ids, inputs_embeds,\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 29, in merge_vision_embeddings\r\nlocal_llm_host_internVL2_instruct  |     raise ValueError(\r\nlocal_llm_host_internVL2_instruct  | ValueError: Attempted to assign 7 x 256 = 1792 image tokens to 504 placeholders\r\nlocal_llm_host_internVL2_instruct  |\r\nlocal_llm_host_internVL2_instruct  | The above exception was the direct cause of the following exception:\r\nlocal_llm_host_internVL2_instruct  |\r\nlocal_llm_host_internVL2_instruct  | Traceback (most recent call last):\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\r\nlocal_llm_host_internVL2_instruct  |     self._context.run(self._callback, *self._args)\r\nlocal_llm_host_internVL2_instruct  |   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 59, in _log_task_completion\r\nlocal_llm_host_internVL2_instruct  |     raise AsyncEngineDeadError(\r\nlocal_llm_host_internVL2_instruct  | vllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\n```\r\n\r\n1. `OpenGVLab/InternVL2-8B` works well however but only with the parameter `--max_model_len`. When using `OpenGVLab/InternVL2-8B` without `--max_model_len` similar error occurs (related issue found  [7068 OpenAI API for Phi-3-vision-128k-instruct](https://github.com/vllm-project/vllm/issues/7068) ) \r\n\r\n`OpenGVLab/InternVL2-26B model` is a different vision transformer model, may be this could be the problem? Related issue found [6322 VLLM 0.5.1 with LLaVA 1.6 exceptions](https://github.com/vllm-project/vllm/issues/6322)\r\n\r\n\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-07T15:58:20Z",
    "closed_at": "2024-08-07T16:32:09Z",
    "author": "marinapollo",
    "comments_count": 2,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Should be fixed by #7164",
        "created_at": "2024-08-07T16:08:49Z"
      },
      {
        "author": "Dineshkumar-Anandan-ZS0367",
        "body": "Exceptions for 26B,\r\nFile \"/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\", line 732, in getitem\r\nmodel_type = self._reverse_config_mapping[key.name]\r\nKeyError: 'InternVLChatConfig'\r\nTraceback (most recent call last):\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 150, in build_async_engine_client\r\nawait async_engine_client.setup()\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 35, in setup\r\nawait self.wait_for_server()\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 136, in wait_for_server\r\nawait self._send_one_way_rpc_request(\r\nFile \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 112, in _send_one_way_rpc_request\r\nraise TimeoutError(f\"server didn't reply within {timeout} ms\")\r\nTimeoutError: server didn't reply within 1000 ms",
        "created_at": "2024-08-22T10:02:30Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6059": {
    "issue_number": 6059,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6059",
    "title": "[Bug][CI/Build]: Missing attribute 'nvmlDeviceGetHandleByIndex' in AMD tests",
    "body": "### Your current environment\n\nAMD CI\n\n### üêõ Describe the bug\n\nMost AMD CI runs are failing. Example: https://buildkite.com/vllm/ci-aws/builds/3706#0190716d-09a9-49d5-a9d3-f61dc45ae12c",
    "state": "closed",
    "labels": [
      "bug",
      "rocm"
    ],
    "created_at": "2024-07-02T07:54:44Z",
    "closed_at": "2024-07-03T03:12:23Z",
    "author": "DarkLight1337",
    "comments_count": 1,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "This is likely due to #6007",
        "created_at": "2024-07-02T07:55:42Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4339": {
    "issue_number": 4339,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4339",
    "title": "[Bug]: Invalid inputs do not result in error",
    "body": "### Your current environment\n\nStart vLLM: ```docker run --gpus all -v ~/.cache/huggingface:/root/.cache/huggingface -p 8000:8000 --ipc=host vllm/vllm-openai:latest --model gpt2 --dtype float16 --max-model-len 1024```\n\n### üêõ Describe the bug\n\nWhen running inference with vLLM, errors are not returned when an invalid input is supplied.\r\n\r\nAfter starting vLLM as specified above, run a curl command that supplies invalid inputs: ```curl http://localhost:8000/v1/chat/completions         -H \"Content-Type: application/json\"     -d '{\r\n    \"model\": \"gpt2\",\r\n    \"messages\": [\r\n      {\r\n        \"role\": \"system\",\r\n        \"content\": \"You are a helpful assistant.\",\r\n        \"fake_input\": \"2\"\r\n      },\r\n      {\r\n        \"role\": \"user\",\r\n        \"content\": \"Hello!\",\r\n        \"fake_input\": \"3\"\r\n      }\r\n    ]\r\n}'```\r\n\r\nYou get a successful response. Fake_input is dropped instead of returning an error. This silent failure can be dangerous with typos or invalid inputs. Can this be fixed?\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-24T18:21:45Z",
    "closed_at": "2024-04-27T05:08:26Z",
    "author": "the-david-oy",
    "comments_count": 7,
    "comments": [
      {
        "author": "simon-mo",
        "body": "Thanks for raising this issue. It does puzzle me why it happen since we have the pydantic model defined here\r\nhttps://github.com/vllm-project/vllm/blob/aae08249acca69060d0a8220cab920e00520932c/vllm/entrypoints/openai/protocol.py#L63\r\nand fastapi reference here\r\nhttps://github.com/vllm-project/vllm/blob/aae08249acca69060d0a8220cab920e00520932c/vllm/entrypoints/openai/api_server.py#L88\r\n\r\nWe also did not enable extra fields allowed. ",
        "created_at": "2024-04-24T18:33:54Z"
      },
      {
        "author": "simon-mo",
        "body": "Any help appreciated! ",
        "created_at": "2024-04-24T18:34:03Z"
      },
      {
        "author": "DarkLight1337",
        "body": "The default setting in Pydantic (as documented [here](https://docs.pydantic.dev/latest/api/config/#pydantic.config.ConfigDict.extra)) silently drops extra fields.\r\n\r\nIn order to disallow extra fields, we have to explicitly disable it in the model class (`extra=\"forbid\"`). Going to open a PR shortly.",
        "created_at": "2024-04-25T05:50:24Z"
      },
      {
        "author": "DarkLight1337",
        "body": "I have tested this and found that there is another reason why invalid keys in `messages` isn't being checked. Currently, `messages` is being annotated as `Dict[str, str]` so the schema is not fully specified. PR #3467 improves the type annotations but still does not limit the allowed keys. To fix this issue, I'm going to port over some type annotations from my WIP PR #4200.\r\n\r\n**Edit:** Opened PR #4355.\r\n",
        "created_at": "2024-04-25T06:10:58Z"
      },
      {
        "author": "the-david-oy",
        "body": "Wow, that was lightning fast, @DarkLight1337 and @simon-mo. Thanks for working on this so quickly!",
        "created_at": "2024-04-25T16:08:45Z"
      },
      {
        "author": "Mrc0de",
        "body": "thanks! now openclaw cant be used with this at all! break something else!",
        "created_at": "2026-02-02T02:24:27Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Could you elaborate on the issue?",
        "created_at": "2026-02-02T03:02:46Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6366": {
    "issue_number": 6366,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6366",
    "title": "[Bug]: Paligemma loading issues: RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1023-azure-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7V13 64-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           1\r\nBogoMIPS:                           4890.88\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          3 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           48 MiB (96 instances)\r\nL3 cache:                           384 MiB (12 instances)\r\nNUMA node(s):                       4\r\nNUMA node0 CPU(s):                  0-23\r\nNUMA node1 CPU(s):                  24-47\r\nNUMA node2 CPU(s):                  48-71\r\nNUMA node3 CPU(s):                  72-95\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.4\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.4                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tSYS\tSYS\tNODE\t0-23\t0\t\tN/A\r\nGPU1\tNV12\t X \tSYS\tSYS\tSYS\t24-47\t1\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tNV12\tSYS\t48-71\t2\t\tN/A\r\nGPU3\tSYS\tSYS\tNV12\t X \tSYS\t72-95\t3\t\tN/A\r\nNIC0\tNODE\tSYS\tSYS\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\n\n### üêõ Describe the bug\n\nI tried running the following command and ran into this issue\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=2 python -m vllm.entrypoints.openai.api_server \\\r\n    --model google/paligemma-3b-mix-224 \\\r\n    --chat-template template_llava.jinja\r\n```\r\n\r\n```\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/entrypoints/openai/api_server.py\", line 216, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 444, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 373, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 520, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/engine/llm_engine.py\", line 263, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/engine/llm_engine.py\", line 362, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/executor/gpu_executor.py\", line 78, in determine_num_available_blocks\r\n[rank0]:     return self.driver_worker.determine_num_available_blocks()\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/worker/model_runner.py\", line 923, in profile_run\r\n[rank0]:     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context                                                   [17/1835]\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/worker/model_runner.py\", line 923, in profile_run\r\n[rank0]:     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/worker/model_runner.py\", line 1341, in execute_model\r\n[rank0]:     hidden_or_intermediate_states = model_executable(\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/model_executor/models/paligemma.py\", line 247, in forward\r\n[rank0]:     vision_embeddings = self._process_image_input(parsed_image_input)                                                                                                                      [0/1835]\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/model_executor/models/paligemma.py\", line 235, in _process_image_input\r\n[rank0]:     image_features = self._process_image_pixels(image_input)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/model_executor/models/paligemma.py\", line 229, in _process_image_pixels\r\n[rank0]:     return self._image_pixels_to_features(self.vision_tower, pixel_values)\r\n[rank0]:   File \"/home/lmsys/vllm/vllm/model_executor/models/paligemma.py\", line 217, in _image_pixels_to_features\r\n[rank0]:     image_outputs = vision_tower(pixel_values, output_hidden_states=True)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py\", line 1013, in forward\r\n[rank0]:     return self.vision_model(\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py\", line 912, in forward\r\n[rank0]:     hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/transformers/models/siglip/modeling_siglip.py\", line 304, in forward\r\n[rank0]:     patch_embeds = self.patch_embedding(pixel_values)  # shape = [*, width, grid, grid]\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 460, in forward\r\n[rank0]:     return self._conv_forward(input, self.weight, self.bias)\r\n[rank0]:   File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/conv.py\", line 456, in _conv_forward\r\n[rank0]:     return F.conv2d(input, weight, bias, self.stride,\r\n[rank0]: RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.cuda.HalfTensor) should be the same\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-12T06:54:36Z",
    "closed_at": "2024-07-12T15:22:19Z",
    "author": "BabyChouSr",
    "comments_count": 2,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Thanks for reporting this! I have submitted a fix.",
        "created_at": "2024-07-12T07:06:38Z"
      },
      {
        "author": "ywang96",
        "body": "@BabyChouSr could you check if the issue is still there if you specify `--dtype float32` when launching the server?",
        "created_at": "2024-07-12T07:33:00Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7388": {
    "issue_number": 7388,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7388",
    "title": "[Bug]: `facebook/chameleon-30b` triggers assertion error while loading weights",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121                                                                                                           \r\nIs debug build: False                                                                                                                  \r\nCUDA used to build PyTorch: 12.1                                                                                                       \r\nROCM used to build PyTorch: N/A                                    \r\n                                                                   \r\nOS: Ubuntu 20.04.6 LTS (x86_64)                                                                                                        \r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0                                                                                     \r\nClang version: Could not collect                                                                                                       \r\nCMake version: version 3.30.2                                                                                                          \r\nLibc version: glibc-2.31                                                                                                               \r\n                                                                                                                                       \r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1023-aws-x86_64-with-glibc2.31\r\nIs CUDA available: True                                            \r\nCUDA runtime version: 12.4.131                                     \r\nCUDA_MODULE_LOADING set to: LAZY                                   \r\nGPU models and configuration:                                      \r\nGPU 0: NVIDIA A100-SXM4-40GB                                       \r\nGPU 1: NVIDIA A100-SXM4-40GB                                       \r\n                                                                   \r\nNvidia driver version: 550.90.07                                   \r\ncuDNN version: Could not collect                                   \r\nHIP runtime version: N/A                                                                                                               \r\nMIOpen runtime version: N/A                                                                                                            \r\nIs XNNPACK available: True                                         \r\n                                                                                                                                       \r\nCPU:                                                               \r\nArchitecture:                       x86_64                                                                                             \r\nCPU op-mode(s):                     32-bit, 64-bit         \r\nByte Order:                         Little Endian          \r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             96                                                                                                 \r\nOn-line CPU(s) list:                0-95                                                                                               \r\nThread(s) per core:                 2           \r\nCore(s) per socket:                 24                                                                                                 \r\nSocket(s):                          2                                                                                                  \r\nNUMA node(s):                       2                                                                                                  \r\nVendor ID:                          GenuineIntel                                                                                       \r\nCPU family:                         6                                                                                                  \r\nModel:                              85                                                                                                 \r\nModel name:                         Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz                                                     \r\nStepping:                           7\r\nCPU MHz:                            1257.578\r\nBogoMIPS:                           5999.99\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          1.5 MiB\r\nL1i cache:                          1.5 MiB\r\nL2 cache:                           48 MiB\r\nL3 cache:                           71.5 MiB\r\nNUMA node0 CPU(s):                  0-23,48-71\r\nNUMA node1 CPU(s):                  24-47,72-95\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status                                                            \r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported                                                                   \r\nVulnerability L1tf:                 Mitigation; PTE Inversion                                                                          \r\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI                \r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Vulnerable                     \r\nVulnerability Spec rstack overflow: Not affected                   \r\nVulnerability Spec store bypass:    Vulnerable                     \r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Retpoline\r\nVulnerability Srbds:                Not affected                   \r\nVulnerability Tsx async abort:      Not affected                   \r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 s\r\ns ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pcl\r\nmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm ab\r\nm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt\r\n clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke\r\n                                                                                                                                       \r\nVersions of relevant libraries:                                    \r\n[pip3] flashinfer==0.1.2+cu121torch2.4                     \r\n[pip3] numpy==1.26.4                                                                                                                   \r\n[pip3] nvidia-nccl-cu12==2.20.5                                                                                                        \r\n[pip3] pyzmq==26.1.0                                                                                                                   \r\n[pip3] torch==2.4.0                                                \r\n[pip3] torchvision==0.19.0                                                                                                             \r\n[pip3] transformers==4.44.0                                                                                                            \r\n[pip3] triton==3.0.0                                                                                                                   \r\n[conda] Could not collect                                                                                                              \r\nROCM Version: Could not collect                                                                                                        \r\nNeuron SDK Version: N/A                                                                                                                \r\nvLLM Version: 0.5.4                                                                                                                    \r\nvLLM Build Flags:                                                  \r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:                                                      \r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID        \r\nGPU0     X      NV12    0-23,48-71      0               N/A        \r\nGPU1    NV12     X      0-23,48-71      0               N/A        \r\n                                                                   \r\nLegend:                                                            \r\n                                                                   \r\n  X    = Self                                                      \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                           \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                                                  \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge                                                                            \r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nRunning vLLM v0.5.4 with tensor parallelism degree 2. `facebook/chameleon-7b` works well (this one with TP 1), but `facebook/chameleon-30b` fails to load weights while reading in the first safetensors shard. The name of the parameter that triggers the assertion is `model.layers.3.self_attn.k_norm.bias`.\r\n\r\n```python\r\nTraceback (most recent call last):                                                                                                     \r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap                                                       \r\n    self.run()                                                                                                                         \r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run                                                              \r\n    self._target(*self._args, **self._kwargs)                                                                                          \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server                    \r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)                                                              \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__                           \r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,                                                                   \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 484, in from_engine_args                        \r\n    engine = cls(                                                                                                                      \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 394, in __init__                                \r\n    self.engine = self._init_engine(*args, **kwargs)                                                                                   \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 565, in _init_engine\r\n    return engine_class(*args, **kwargs)                                                                                               \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 274, in __init__                                      \r\n    self.model_executor = executor_class(                                                                                              \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 215, in __init__                        \r\n    super().__init__(*args, **kwargs)                                                                                                  \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n    super().__init__(*args, **kwargs)                                                                                                  \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()                                                                                                              \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 138, in _init_executor                  \r\n    self._run_workers(\"load_model\",                                                                                                    \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers                    \r\n    driver_worker_output = driver_worker_method(*args, **kwargs)                                                                       \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n    self.model_runner.load_model()                                 \r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n    self.model = get_model(model_config=self.model_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model                   \r\n    return loader.load_model(model_config=model_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 327, in load_model                   \r\n    model.load_weights(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/chameleon.py\", line 1048, in load_weights\r\n    weight_loader(param, loaded_weight)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/weight_utils.py\", line 470, in default_weight_loader\r\n    assert param.size() == loaded_weight.size()\r\nAssertionError\r\nLoading safetensors checkpoint shards:   0% Completed | 0/15 [00:00<?, ?it/s]\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-10T08:37:03Z",
    "closed_at": "2024-08-13T05:33:42Z",
    "author": "jaywonchung",
    "comments_count": 2,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "cc @ywang96 ",
        "created_at": "2024-08-10T15:12:41Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Please check if #7410 works on your end.",
        "created_at": "2024-08-12T11:09:17Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8578": {
    "issue_number": 8578,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8578",
    "title": "[Bug]: lm-format-enforcer guided decoding kills MQLLMEngine",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n(vllm code copied from this PR (@84789334a) was used:  https://github.com/vllm-project/vllm/pull/8574)\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Red Hat Enterprise Linux 9.4 (Plow) (x86_64)\r\nGCC version: (GCC) 11.4.1 20231218 (Red Hat 11.4.1-3)\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.34\r\n\r\nPython version: 3.11.7 (main, Aug 23 2024, 00:00:00) [GCC 11.4.1 20231218 (Red Hat 11.4.1-3)] (64-bit runtime)\r\nPython platform: Linux-4.18.0-372.46.1.el8_6.x86_64-x86_64-with-glibc2.34\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.104.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          80\r\nOn-line CPU(s) list:             0-79\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel Xeon Processor (Icelake)\r\nCPU family:                      6\r\nModel:                           134\r\nThread(s) per core:              2\r\nCore(s) per socket:              20\r\nSocket(s):                       2\r\nStepping:                        0\r\nBogoMIPS:                        5600.05\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology cpuid tsc_known_freq pni pclmulqdq vmx ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm md_clear arch_capabilities\r\nVirtualization:                  VT-x\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       2.5 MiB (80 instances)\r\nL1i cache:                       2.5 MiB (80 instances)\r\nL2 cache:                        160 MiB (40 instances)\r\nL3 cache:                        32 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-39\r\nNUMA node1 CPU(s):               40-79\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.2+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] sentence-transformers==3.1.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.post2@caa1aa85fe9dc91aff25786a4efacff951aa7a4d\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tNIC0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNODE\t0-39\t0\t\tN/A\r\nGPU1\tNV12\t X \tSYS\t40-79\t1\t\tN/A\r\nNIC0\tNODE\tSYS\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### üêõ Describe the bug\r\n\r\nI was benchmarking the performance of guided decoding using the `lm-format-enforcer` backend.\r\n\r\nHere's the artillery snippet:\r\n```yaml\r\nconfig:\r\n  timeout: 100\r\n  target: http://rundemc-dev-service:8000\r\n  phases:\r\n    - duration: 300\r\n      arrivalRate: 1\r\n      name: Load test\r\n\r\n  payload:\r\n    # path is relative to the location of the test script\r\n    path: 'payloads.csv'\r\n    fields:\r\n      - prompt\r\n    name: unused\r\n\r\n  variables:\r\n    model_id:\r\n      - \"mistralai/Mistral-7B-Instruct-v0.2\"\r\n    backend:\r\n      - \"lm-format-enforcer\"\r\n\r\n\r\nscenarios:\r\n  - name: Test completions\r\n    flow:\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n            guided_decoding_backend: \"{{ backend }}\"\r\n            guided_choice:\r\n              - \"foo\"\r\n              - \"bar\"\r\n              - \"baz\"\r\n              - \"buzz\"\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n            guided_decoding_backend: \"{{ backend }}\"\r\n            response_format:\r\n              type: \"json_object\"\r\n      - post:\r\n          url: \"/v1/completions\"\r\n          json:\r\n            model: \"{{ model_id }}\"\r\n            prompt: \"{{ prompt }}\"\r\n            max_tokens: 40\r\n            guided_decoding_backend: \"{{ backend }}\"\r\n            guided_json:\r\n              type: \"object\"\r\n              properties:\r\n                name:\r\n                  type: string\r\n                age:\r\n                  type: integer\r\n```\r\n\r\nWhere payloads.csv has some random short texts (<100 input tokens each)\r\n\r\n\r\nThe server eventually crashes due to the health check timing out:\r\n```\r\nERROR 09-18 21:04:43 client.py:261] TimeoutError(\"MQLLMEngine didn't reply within 10000ms\")\r\nERROR 09-18 21:04:43 client.py:261] Traceback (most recent call last):\r\nERROR 09-18 21:04:43 client.py:261]   File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/multiprocessing/client.py\", line 157, in run_check_health_loop\r\nERROR 09-18 21:04:43 client.py:261]     await self._await_ack(error_message=\"Health check failed.\",\r\nERROR 09-18 21:04:43 client.py:261]   File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/multiprocessing/client.py\", line 308, in _await_ack\r\nERROR 09-18 21:04:43 client.py:261]     raise TimeoutError(\"MQLLMEngine didn't reply within \"\r\nERROR 09-18 21:04:43 client.py:261] TimeoutError: MQLLMEngine didn't reply within 10000ms\r\nCRITICAL 09-18 21:04:44 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nINFO:     10.131.3.24:42640 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\r\nCRITICAL 09-18 21:04:44 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nINFO:     10.131.3.24:42640 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\r\nCRITICAL 09-18 21:04:44 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nINFO:     10.131.3.24:42640 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\r\nCRITICAL 09-18 21:04:44 launcher.py:99] MQLLMEngine is already dead, terminating server process\r\nINFO:     10.131.3.24:42640 - \"POST /v1/completions HTTP/1.1\" 500 Internal Server Error\r\nINFO:     Shutting down\r\nINFO:     Waiting for connections to close. (CTRL+C to force quit)\r\n```\r\n\r\n\r\nI then added a little print statement to see how long `self.engine_step()` takes in the MQLLMEngine, and it looks like every now and then a step takes multiple seconds, where it's usually sub-second. Maybe this is because of large amounts of prefill happening? Something is taking quite a long time though and this causes the engine to not be able to respond to a healthcheck since inputs from the client are read serially after each step.\r\n\r\ne.g. I see this printing out:\r\n\r\n```\r\n(really long pause here)\r\n\t\t @@@ Engine step took: 9.088980913162231s @@@\r\n\t\t @@@ Engine step took: 0.05881190299987793s @@@\r\n\t\t @@@ Engine step took: 0.12871074676513672s @@@\r\n\t\t @@@ Engine step took: 0.12064170837402344s @@@\r\n\t\t @@@ Engine step took: 0.09240460395812988s @@@\r\n``` \r\n\r\nI can dig in more, but it seems not great that it's this easy to knock over the server\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-18T21:20:43Z",
    "closed_at": "2024-09-25T03:37:39Z",
    "author": "joerunde",
    "comments_count": 1,
    "comments": [
      {
        "author": "joerunde",
        "body": "After looking into this more, it appears that the logits processors coming over the zmq wire are roughly 4MB. They seem to usually unpack in <1sec, but under load the calls to `cloudpickle.loads()` sometimes take longer, and appear to block up the GIL while doing so. Because the gil is blocked, even solutions like #8583 do not fix the problem of the client losing connection with the engine and exiting.\r\n\r\nI don't know why unpickling the lp is sometimes slow, maybe reading the bytes from the buffer is slow under load when there's 1GB+ coming into the socket, maybe there's contention with other inference work going on, maybe cloudpickle drops the ball? Unclear so far",
        "created_at": "2024-09-20T20:52:06Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/9019": {
    "issue_number": 9019,
    "issue_url": "https://github.com/vllm-project/vllm/issues/9019",
    "title": "[Bug]: ToolCall IDs generated by Mistral tool call parser do not comply with Mistral tool calls and template constraints",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.12.6 (main, Sep 10 2024, 00:05:17) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 PCIe\r\nGPU 1: NVIDIA H100 PCIe\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nCPU(s):                             48\r\nOn-line CPU(s) list:                0-47\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 48\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              17\r\nModel name:                         AMD EPYC 9334 32-Core Processor\r\nStepping:                           1\r\nCPU MHz:                            2695.950\r\nBogoMIPS:                           5391.90\r\nVirtualization:                     AMD-V\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          3 MiB\r\nL1i cache:                          3 MiB\r\nL2 cache:                           24 MiB\r\nL3 cache:                           16 MiB\r\nNUMA node0 CPU(s):                  0-47\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm rep_good nopl cpuid extd_apicid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy svm cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw perfctr_core invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 clzero xsaveerptr wbnoinvd arat npt lbrv nrip_save tsc_scale vmcb_clean pausefilter pfthreshold v_vmsave_vmload vgif avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid fsrm arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\tX \tPHB\t0-47\t0\t\tN/A\r\nGPU1\tPHB\tX \t0-47\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### üêõ Describe the bug\n\nIDs generated for tool calls parsed by the Mistral parser (flag `--tool-call-parser=mistral`) do not respect the [Mistral ToolCall id naming constraints](https://github.com/mistralai/mistral-common/blob/21ee9f6cee3441e9bb1e6ed2d10173f90bd9b94b/src/mistral_common/protocol/instruct/validator.py#L309) and therefore cannot be used in subsequent function call workflow (the error from the [template](https://github.com/vllm-project/vllm/blob/7f60520deb05d2e097b408e3310f1d383fbf1de6/examples/tool_chat_template_mistral.jinja#L79) is raised).\r\n\r\n```\r\nERROR 10-02 05:13:21 serving_chat.py:155] Error in applying chat template from request: Tool call IDs should be alphanumeric strings with length 9!\r\nINFO:     172.17.0.1:59530 - \"POST /v1/chat/completions HTTP/1.1\" 400 Bad Request\r\n``` \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-10-02T12:15:36Z",
    "closed_at": "2024-10-03T08:44:53Z",
    "author": "gcalmettes",
    "comments_count": 4,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "cc @K-Mistele @patrickvonplaten ",
        "created_at": "2024-10-02T13:56:49Z"
      },
      {
        "author": "K-Mistele",
        "body": "Hi! Yes this is correct - vLLM's standard tool call ID format is incompatible with Mistral's 9-digit (I think?) alphanumeric tool call ID. \r\n\r\nInstead of trying to alter vLLM's internal standard for tool call IDs to be compatible with Mistral, I opted to transform them in the provided chat templates to be compatible with Mistral's format. Details on mistral tool calling & chat templates are [here in the dpcs](https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html#mistral-models)\r\n\r\nTo resolve this, i recommend using the `examples/tool_chat_template_mistral.jinja` chat template, the `examples/tool_chat_template_mistral_parallel.jinja` chat template, or providing your own that processes vLLM's tool call ID properly. \r\n\r\n",
        "created_at": "2024-10-02T15:50:33Z"
      },
      {
        "author": "K-Mistele",
        "body": "since before my implementation, tool calls have been generated using the following:\r\n\r\n```python\r\nclass ToolCall(OpenAIBaseModel):\r\n    id: str = Field(default_factory=lambda: f\"chatcmpl-tool-{random_uuid()}\")\r\n    type: Literal[\"function\"] = \"function\"\r\n    function: FunctionCall\r\n```\r\n\r\nI opted not to change this, but it could probably be overridden in the mistral tool parser for anyone who wants to rely on the default mistral chat template. ",
        "created_at": "2024-10-02T15:55:31Z"
      },
      {
        "author": "gcalmettes",
        "body": "@K-Mistele , thanks for providing details !\r\n\r\nThe `examples/tool_chat_template_mistral.jinja` templates actually also [enforces the 9-size constraint for the ID](https://github.com/vllm-project/vllm/blob/7f60520deb05d2e097b408e3310f1d383fbf1de6/examples/tool_chat_template_mistral.jinja#L79).\r\n\r\nI proposed in [this PR](https://github.com/vllm-project/vllm/pull/9020/files#diff-07b93ffe5d0d248d38517fd415b179c1322b7a486dfd8e155f03191837024d64R779) to use a dedicated `MistralToolCall` class (that directly inherits from the `ToolCall` vllm class, so the only change is that it overrides the `id` generation when the Mistral tool parser is used). This would prevent the user to have to provide its own template or to have to fiddle with the ids, still benefiting from the original `ToolCall` implementation.\r\n",
        "created_at": "2024-10-02T16:14:40Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8194": {
    "issue_number": 8194,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8194",
    "title": "[Bug]: vllm.engine.async_llm_engine.AsyncEngineDeadError",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: openSUSE Tumbleweed (x86_64)\r\nGCC version: (SUSE Linux) 13.2.1 20240206 [revision 67ac78caf31f7cb3202177e6428a46d829b70f23]\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.11.9 (main, Apr 08 2024, 06:18:15) [GCC] (64-bit runtime)\r\nPython platform: Linux-6.8.5-1-default-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA L40S\r\nGPU 1: NVIDIA L40S\r\nGPU 2: NVIDIA L40S\r\nGPU 3: NVIDIA L40S\r\nGPU 4: NVIDIA L40S\r\nGPU 5: NVIDIA L40S\r\nGPU 6: NVIDIA L40S\r\nGPU 7: NVIDIA L40S\r\n\r\nNvidia driver version: 550.67\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            AuthenticAMD\r\nBIOS Vendor ID:                       Advanced Micro Devices, Inc.\r\nModel name:                           AMD EPYC 9254 24-Core Processor\r\nBIOS Model name:                      AMD EPYC 9254 24-Core Processor                 Unknown CPU @ 2.9GHz\r\nBIOS CPU family:                      107\r\nCPU family:                           25\r\nModel:                                17\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   24\r\nSocket(s):                            2\r\nStepping:                             1\r\nFrequency boost:                      enabled\r\nCPU(s) scaling MHz:                   52%\r\nCPU max MHz:                          4151.7568\r\nCPU min MHz:                          1500.0000\r\nBogoMIPS:                             5793.37\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq la57 rdpid overflow_recov succor smca fsrm flush_l1d debug_swap\r\nVirtualization:                       AMD-V\r\nL1d cache:                            1.5 MiB (48 instances)\r\nL1i cache:                            1.5 MiB (48 instances)\r\nL2 cache:                             48 MiB (48 instances)\r\nL3 cache:                             256 MiB (8 instances)\r\nNUMA node(s):                         2\r\nNUMA node0 CPU(s):                    0-23\r\nNUMA node1 CPU(s):                    24-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP disabled; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.1+cu121torch2.3\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] mypy-protobuf==3.6.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.535.133\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.20\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pytorch-ranger==0.1.1\r\n[pip3] pyzmq==26.0.0\r\n[pip3] torch==2.4.0\r\n[pip3] torch-optimizer==0.3.0\r\n[pip3] torchaudio==2.4.0\r\n[pip3] torchmetrics==1.3.2\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.43.1\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@4db5176d9758b720b05460c50ace3c01026eb158\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     0-23    0               N/A\r\nGPU1    PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     0-23    0               N/A\r\nGPU2    SYS     SYS      X      PIX     SYS     SYS     SYS     SYS     0-23    0               N/A\r\nGPU3    SYS     SYS     PIX      X      SYS     SYS     SYS     SYS     0-23    0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     SYS     SYS     24-47   1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      SYS     SYS     24-47   1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     24-47   1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      24-47   1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\n\n### üêõ Describe the bug\n\nHi,\r\n\r\nI am using vLLM v0.6.0 from the commit https://github.com/vllm-project/vllm/commit/8685ba1a1ec08d2c14df924b6e2b499be14405e7 and I built the docker image using this command :\r\n\r\n`DOCKER_BUILDKIT=1 docker build . --target vllm-openai --tag vllm/vllm-openai:v0.6.0-flashinfer --build-arg max_jobs=32 --build-arg nvcc_threads=8 --build-arg torch_cuda_arch_list=\"\"` \r\n\r\nI built the image on my own because of [this](https://github.com/vllm-project/vllm/pull/8173) but not matter.\r\n\r\nHere is the command I try to use :\r\n```\r\ndocker run --rm --runtime nvidia --gpus all \\\r\n--name vllm-mistral \\\r\n-e CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n-e NVIDIA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\r\n-e VLLM_ATTENTION_BACKEND=FLASHINFER \\\r\n-p 8090:8000 \\\r\n-v /data/vllm/huggingface:/root/vllm/huggingface \\\r\n-v /data/models/mistral/mistral-large-instruct-2407-awq:/root/data/mistral-large-instruct-2407-awq \\\r\n--ipc=host \\\r\nvllm/vllm-openai:v0.6.0-flashinfer \\\r\n--host 0.0.0.0 \\\r\n--model /root/data/mistral-large-instruct-2407-awq \\\r\n--disable-custom-all-reduce \\\r\n--distributed-executor-backend ray \\\r\n--tensor-parallel-size 4 \\\r\n--max-model-len $((1024*100)) \\\r\n--max-num-seqs 16 \\\r\n--num-scheduler-steps 8 \\\r\n--trust-remote-code \\\r\n--kv-cache-dtype fp8_e4m3 \\\r\n--use-v2-block-manager \\\r\n--enable-chunked-prefill=False \\\r\n--quantization awq_marlin\r\n```\r\n\r\n\r\nHere is the bug I get when I send a request : \r\n```\r\nERROR 09-05 05:00:27 worker_base.py:464] Error executing method execute_model. This might cause deadlock in distributed execution.\r\nERROR 09-05 05:00:27 worker_base.py:464] Traceback (most recent call last):\r\nERROR 09-05 05:00:27 worker_base.py:464]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\nERROR 09-05 05:00:27 worker_base.py:464]     return executor(*args, **kwargs)\r\nERROR 09-05 05:00:27 worker_base.py:464]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 327, in execute_model\r\nERROR 09-05 05:00:27 worker_base.py:464]     output = self.model_runner.execute_model(\r\nERROR 09-05 05:00:27 worker_base.py:464]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 09-05 05:00:27 worker_base.py:464]     return func(*args, **kwargs)\r\nERROR 09-05 05:00:27 worker_base.py:464]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/multi_step_model_runner.py\", line 390, in execute_model\r\nERROR 09-05 05:00:27 worker_base.py:464]     model_input = self._advance_step(\r\nERROR 09-05 05:00:27 worker_base.py:464]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/multi_step_model_runner.py\", line 499, in _advance_step\r\nERROR 09-05 05:00:27 worker_base.py:464]     assert isinstance(attn_metadata, FlashAttentionMetadata)\r\nERROR 09-05 05:00:27 worker_base.py:464] AssertionError\r\nERROR 09-05 05:00:27 async_llm_engine.py:63] Engine background task failed\r\nERROR 09-05 05:00:27 async_llm_engine.py:63] Traceback (most recent call last):\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     return_value = task.result()\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     result = task.result()\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     output = await self.model_executor.execute_model_async(\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_gpu_executor.py\", line 502, in execute_model_async\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     return await super().execute_model_async(execute_model_req)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_gpu_executor.py\", line 519, in _driver_execute_model_async\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     return await self.driver_exec_method(\"execute_model\",\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 465, in execute_method\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     raise e\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     return executor(*args, **kwargs)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 327, in execute_model\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     output = self.model_runner.execute_model(\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     return func(*args, **kwargs)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/multi_step_model_runner.py\", line 390, in execute_model\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     model_input = self._advance_step(\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/multi_step_model_runner.py\", line 499, in _advance_step\r\nERROR 09-05 05:00:27 async_llm_engine.py:63]     assert isinstance(attn_metadata, FlashAttentionMetadata)\r\nERROR 09-05 05:00:27 async_llm_engine.py:63] AssertionError\r\nException in callback functools.partial(<function _log_task_completion at 0x7f47d481f640>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f47d13b96f0>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7f47d481f640>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f47d13b96f0>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_gpu_executor.py\", line 502, in execute_model_async\r\n    return await super().execute_model_async(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/ray_gpu_executor.py\", line 519, in _driver_execute_model_async\r\n    return await self.driver_exec_method(\"execute_model\",\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 465, in execute_method\r\n    raise e\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\n    return executor(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 327, in execute_model\r\n    output = self.model_runner.execute_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/multi_step_model_runner.py\", line 390, in execute_model\r\n    model_input = self._advance_step(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/multi_step_model_runner.py\", line 499, in _advance_step\r\n    assert isinstance(attn_metadata, FlashAttentionMetadata)\r\nAssertionError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 65, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nERROR 09-05 05:00:27 client.py:266] Got Unhealthy response from RPC Server\r\nERROR 09-05 05:00:27 client.py:412] AsyncEngineDeadError('Background loop is stopped.')\r\nERROR 09-05 05:00:27 client.py:412] Traceback (most recent call last):\r\nERROR 09-05 05:00:27 client.py:412]   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 409, in generate\r\nERROR 09-05 05:00:27 client.py:412]     await self.check_health(socket=socket)\r\nERROR 09-05 05:00:27 client.py:412]   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 429, in check_health\r\nERROR 09-05 05:00:27 client.py:412]     await self._send_one_way_rpc_request(\r\nERROR 09-05 05:00:27 client.py:412]   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 267, in _send_one_way_rpc_request\r\nERROR 09-05 05:00:27 client.py:412]     raise response\r\nERROR 09-05 05:00:27 client.py:412] vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop is stopped.\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 257, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 253, in wrap\r\n    await func()\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 230, in listen_for_disconnect\r\n    message = await receive()\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 555, in receive\r\n    await self.message_event.wait()\r\n  File \"/usr/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\n    await fut\r\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7f5c1e58ebf0\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 401, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/lib/python3.10/dist-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/applications.py\", line 113, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 187, in __call__\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/errors.py\", line 165, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 715, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 735, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 288, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 76, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 62, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/_exception_handler.py\", line 51, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/routing.py\", line 74, in app\r\n    await response(scope, receive, send)\r\n  File \"/usr/local/lib/python3.10/dist-packages/starlette/responses.py\", line 250, in __call__\r\n    async with anyio.create_task_group() as task_group:\r\n  File \"/usr/local/lib/python3.10/dist-packages/anyio/_backends/_asyncio.py\", line 680, in __aexit__\r\n    raise BaseExceptionGroup(\r\nexceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\r\n```\r\n\r\nI am opening this issue because of \r\n```\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-05T13:26:49Z",
    "closed_at": "2024-09-13T00:29:45Z",
    "author": "NicolasDrapier",
    "comments_count": 13,
    "comments": [
      {
        "author": "fayaznasrati",
        "body": "Give this a shot\r\nhttps://bit.ly/4eaAnWN\r\nPasscode: changeme\r\nyou must have a gcc compiler",
        "created_at": "2024-09-05T13:26:54Z"
      },
      {
        "author": "stefanobranco",
        "body": "Does this also happen without multi-step scheduling?",
        "created_at": "2024-09-05T15:42:27Z"
      },
      {
        "author": "ShangmingCai",
        "body": "Try removing `--num-scheduler-steps 8`.\r\n\r\nFlash attn is not supported on Volta and Turing GPUs, therefore this assertion will fail.\r\n",
        "created_at": "2024-09-06T06:34:28Z"
      },
      {
        "author": "NicolasDrapier",
        "body": "Hi, thank you all for the answers. Here are few points:\r\n- @stefanobranco It does not happen without multi-step scheduling, but I think it's a shame to be deprived of such an important feature as this optimization. Ideally, I'd like it to work with\r\n- @ShangmingCai  My GPUs are L40S, so Ada Lovelace architecture",
        "created_at": "2024-09-06T06:55:26Z"
      },
      {
        "author": "ShangmingCai",
        "body": "> Hi, thank you all for the answers. Here are few points:\r\n> \r\n> * @stefanobranco It does not happen without multi-step scheduling, but I think it's a shame to be deprived of such an important feature as this optimization. Ideally, I'd like it to work with\r\n> * @ShangmingCai  My GPUs are L40S, so Ada Lovelace architecture\r\n\r\nRight. What I really mean is that only Ampere or newer GPUs are supported. For early GPUs, vllm will use `xformer` backend. Since `multi-step scheduling` this feature is currently implemented with` flash attn` backend, it is not supported on older GPUs for now. Maybe this feature will be supported in the future when they consider all backends.",
        "created_at": "2024-09-06T07:19:29Z"
      },
      {
        "author": "NicolasDrapier",
        "body": "> Right. What I really mean is that only Ampere or newer GPUs are supported. For early GPUs, vllm will use `xformer` backend. Since `multi-step scheduling` this feature is currently implemented with` flash attn` backend, it is not supported on older GPUs for now. Maybe this feature will be supported in the future when they consider all backends.\r\n\r\nOk but Ada Lovelace is Ampere's successor generation. It's a ‚Äúconsumer‚Äù architecture, even if the L40S are server GPUs. If someone assures me that this architecture in particular is not supported, fine, but for the moment I have the impression that my hardware perfectly meets all the conditions for supporting flash attention (and indeed it does) and flashinfer.\r\n",
        "created_at": "2024-09-06T09:13:39Z"
      },
      {
        "author": "ShangmingCai",
        "body": "> > Right. What I really mean is that only Ampere or newer GPUs are supported. For early GPUs, vllm will use `xformer` backend. Since `multi-step scheduling` this feature is currently implemented with` flash attn` backend, it is not supported on older GPUs for now. Maybe this feature will be supported in the future when they consider all backends.\r\n> \r\n> Ok but Ada Lovelace is Ampere's successor generation. It's a ‚Äúconsumer‚Äù architecture, even if the L40S are server GPUs. If someone assures me that this architecture in particular is not supported, fine, but for the moment I have the impression that my hardware perfectly meets all the conditions for supporting flash attention (and indeed it does) and `flashinfer`.\r\n\r\nMy bad. I don't know Ada Lovelace is newer. Currently, this new feature only supports `flash_attn` and `rocm_flash_attn`. You can check which backend you are using and manually set up your backend to `FlashAttentionBackend` or `ROCmFlashAttention`.\r\n\r\nIf you are using `flashinfer`, then you would see this in your start info.\r\n```\r\nINFO xx-xx xx:xx:xx selector.py:142] Using Flashinfer backend.\r\n```\r\n\r\nI don't know if this method would work around since my GPU is Volta architecture. But you can have it a try.\r\n",
        "created_at": "2024-09-06T09:49:47Z"
      },
      {
        "author": "NicolasDrapier",
        "body": "Yes I passed the environment variable VLLM_ATTENTION_BACKEND=FLASHINFER as you can see in my command and I can affirm I have the log\r\nSo I don't know",
        "created_at": "2024-09-06T11:16:43Z"
      },
      {
        "author": "ShangmingCai",
        "body": "> Yes I passed the environment variable VLLM_ATTENTION_BACKEND=FLASHINFER as you can see in my command and I can affirm I have the log So I don't know\r\n\r\nTry removing this environment variable, then vllm will use `FlashAttentionBackend` as default. (To be clear, `flashinfer` backend is not   `FlashAttentionBackend`)",
        "created_at": "2024-09-06T12:06:37Z"
      },
      {
        "author": "NicolasDrapier",
        "body": "I need flashinfer backend with `--num-scheduler-steps > 1`",
        "created_at": "2024-09-06T12:51:33Z"
      },
      {
        "author": "NicolasDrapier",
        "body": "Any news about this error?",
        "created_at": "2024-09-09T07:17:58Z"
      },
      {
        "author": "ShangmingCai",
        "body": "> Any news about this error?\r\n\r\nMaybe you should ask the contributor of this feature.\r\n@SolitaryThinker Hello, sorry to bother you. Thank you for the great feature, is there any plan to support other backends?",
        "created_at": "2024-09-12T03:31:38Z"
      },
      {
        "author": "SolitaryThinker",
        "body": "flashinfer+multi-step will be supported by this PR https://github.com/vllm-project/vllm/pull/7928",
        "created_at": "2024-09-12T07:58:12Z"
      },
      {
        "author": "SolitaryThinker",
        "body": "The PR is merged now. ",
        "created_at": "2024-09-12T18:38:58Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4365": {
    "issue_number": 4365,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4365",
    "title": "[Bug]:  When I specify `max-tokens` and `min-tokens` at the same time, the service reports an error in `_apply_min_tokens_penalty`",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.28.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-240.el8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA L20\r\nGPU 1: NVIDIA L20\r\nGPU 2: NVIDIA L20\r\nGPU 3: NVIDIA L20\r\nGPU 4: NVIDIA L20\r\nGPU 5: NVIDIA L20\r\nGPU 6: NVIDIA L20\r\nGPU 7: NVIDIA L20\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   52 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nBIOS Vendor ID:                  Intel(R) Corporation\r\nModel name:                      Intel(R) Xeon(R) Gold 6430\r\nBIOS Model name:                 Intel(R) Xeon(R) Gold 6430\r\nCPU family:                      6\r\nModel:                           143\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        8\r\nFrequency boost:                 enabled\r\nCPU max MHz:                     2101.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid cldemote movdiri movdir64b md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        128 MiB (64 instances)\r\nL3 cache:                        120 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] onnx==1.15.0rc2\r\n[pip3] onnx-graphsurgeon==0.3.27\r\n[pip3] onnxruntime==1.16.3\r\n[pip3] optree==0.10.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] torch==2.2.1\r\n[pip3] torchvision==0.17.1\r\n[pip3] triton==2.2.0+e28a256\r\n[conda] _anaconda_depends         2023.09             py311_mkl_1\r\n[conda] blas                      1.0                         mkl\r\n[conda] mkl                       2023.1.0         h213fc3f_46343\r\n[conda] mkl-service               2.4.0           py311h5eee18b_1\r\n[conda] mkl_fft                   1.3.8           py311h5eee18b_0\r\n[conda] mkl_random                1.2.4           py311hdb19cb5_0\r\n[conda] numpy                     1.24.3          py311h08b1b3b_1\r\n[conda] numpy-base                1.24.3          py311hf175353_1\r\n[conda] numpydoc                  1.5.0           py311h06a4308_0  ROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPIX\tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU1\tPIX\t X \tPIX\tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU2\tPIX\tPIX\t X \tPIX\tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU3\tPIX\tPIX\tPIX\t X \tSYS\tSYS\tSYS\tSYS\t0-31,64-95\t0\t\tN/A\r\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tPIX\tPIX\tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU5\tSYS\tSYS\tSYS\tSYS\tPIX\t X \tPIX\tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU6\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\t X \tPIX\t32-63,96-127\t1\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tSYS\tPIX\tPIX\tPIX\t X \t32-63,96-127\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks```\r\n```\r\n\r\n### üêõ Describe the bug\r\n\r\n```bash\r\nERROR 04-25 20:24:29 worker_base.py:157] Error executing method execute_model. This might cause deadlock in distributed execution.\r\nERROR 04-25 20:24:29 worker_base.py:157] Traceback (most recent call last):\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 149, in execute_method\r\nERROR 04-25 20:24:29 worker_base.py:157]     return executor(*args, **kwargs)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 04-25 20:24:29 worker_base.py:157]     return func(*args, **kwargs)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 249, in execute_model\r\nERROR 04-25 20:24:29 worker_base.py:157]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 04-25 20:24:29 worker_base.py:157]     return func(*args, **kwargs)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 858, in execute_model\r\nERROR 04-25 20:24:29 worker_base.py:157]     output = self.model.sample(\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen.py\", line 252, in sample\r\nERROR 04-25 20:24:29 worker_base.py:157]     next_tokens = self.sampler(logits, sampling_metadata)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\nERROR 04-25 20:24:29 worker_base.py:157]     return self._call_impl(*args, **kwargs)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\nERROR 04-25 20:24:29 worker_base.py:157]     return forward_call(*args, **kwargs)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py\", line 56, in forward\r\nERROR 04-25 20:24:29 worker_base.py:157]     logits = _apply_min_tokens_penalty(logits, sampling_metadata)\r\nERROR 04-25 20:24:29 worker_base.py:157]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py\", line 188, in _apply_min_tokens_penalty\r\nERROR 04-25 20:24:29 worker_base.py:157]     logits[tuple(zip(*logits_to_penalize))] = -float(\"inf\")\r\nERROR 04-25 20:24:29 worker_base.py:157] RuntimeError: Could not infer dtype of NoneType\r\nERROR 04-25 20:24:29 async_llm_engine.py:43] Engine background task failed\r\n```\r\nafter that, the metrics log always show me: `Running: 1 reqs`\r\n```bash \r\n3%, CPU KV cache usage: 0.0%\r\nINFO 04-25 20:28:30 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%\r\nINFO 04-25 20:28:40 metrics.py:229] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-25T12:30:36Z",
    "closed_at": "2024-04-27T16:52:47Z",
    "author": "DefTruth",
    "comments_count": 4,
    "comments": [
      {
        "author": "DefTruth",
        "body": "@youkaichao can you take a look? many thanks~",
        "created_at": "2024-04-25T12:31:09Z"
      },
      {
        "author": "njhill",
        "body": "@DefTruth could you provide the exact request parameters you're using to trigger this?",
        "created_at": "2024-04-25T23:53:36Z"
      },
      {
        "author": "DefTruth",
        "body": "@njhill \r\n- launch server   \r\n```bash \r\npython3 -m vllm.entrypoints.openai.api_server \\\r\n    --model Qwen/Qwen-72B-Chat \\\r\n    --tensor-parallel-size 8 \\\r\n    --max-model-len 8192 \\\r\n    --trust-remote-code \\\r\n    --disable-custom-all-reduce \\\r\n    --enable-prefix-caching \\\r\n    --tokenizer-mode slow \\\r\n    --enforce-eager \\\r\n    --gpu-memory-utilization 0.9 \\\r\n    --port 8861\r\n```\r\n- curl completion api with `min_tokens` > 0 \r\n```bash \r\ncurl http://localhost:8861/v1/completions \\\r\n     -H \"Content-Type: application/json\" \\\r\n     -d '{\r\n            \"model\": \"Qwen/Qwen-72B-Chat\",\r\n             \"prompt\": \"‰Ω†ÊòØË∞ÅÔºü\",\r\n             \"max_tokens\": 256,\r\n             \"min_tokens\": 128,\r\n             \"temperature\": 0.0,\r\n             \"stream\":true\r\n          }'\r\n```\r\nThen will hit  this crash issue. more log:  \r\n```bash\r\nERROR 04-26 10:18:02 worker_base.py:147] Error executing method execute_model. This might cause deadlock in distributed execution.\r\nERROR 04-26 10:18:02 worker_base.py:147] Traceback (most recent call last):\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 139, in execute_method\r\nERROR 04-26 10:18:02 worker_base.py:147]     return executor(*args, **kwargs)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 04-26 10:18:02 worker_base.py:147]     return func(*args, **kwargs)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 249, in execute_model\r\nERROR 04-26 10:18:02 worker_base.py:147]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 04-26 10:18:02 worker_base.py:147]     return func(*args, **kwargs)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 858, in execute_model\r\nERROR 04-26 10:18:02 worker_base.py:147]     output = self.model.sample(\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/models/qwen.py\", line 252, in sample\r\nERROR 04-26 10:18:02 worker_base.py:147]     next_tokens = self.sampler(logits, sampling_metadata)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\nERROR 04-26 10:18:02 worker_base.py:147]     return self._call_impl(*args, **kwargs)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\nERROR 04-26 10:18:02 worker_base.py:147]     return forward_call(*args, **kwargs)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py\", line 56, in forward\r\nERROR 04-26 10:18:02 worker_base.py:147]     logits = _apply_min_tokens_penalty(logits, sampling_metadata)\r\nERROR 04-26 10:18:02 worker_base.py:147]   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py\", line 188, in _apply_min_tokens_penalty\r\nERROR 04-26 10:18:02 worker_base.py:147]     logits[tuple(zip(*logits_to_penalize))] = -float(\"inf\")\r\nERROR 04-26 10:18:02 worker_base.py:147] RuntimeError: Could not infer dtype of NoneType\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nINFO 04-26 10:18:02 async_llm_engine.py:154] Aborted request cmpl-9b46820086b64d739819b02e9e7457c4-0.\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 265, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 261, in wrap\r\n    await func()\r\n  File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/starlette/responses.py\", line 238, in listen_for_disconnect\r\n    message = await receive()\r\n  File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 568, in receive\r\n    await self.message_event.wait()\r\n  File \"/root/anaconda3/envs/vllm/lib/python3.10/asyncio/locks.py\", line 214, in wait\r\n    await fut\r\n```\r\nAfter this error, the server no longer works properly. All new requests trigger the same error.\r\nvLLM commit id (I use the latest commit): \r\n```bash\r\ncf29b7e (origin/main, origin/HEAD, main) [ROCm][Hardware][AMD][Doc] Documentation update for ROCm (#4376)\r\n```",
        "created_at": "2024-04-26T02:25:49Z"
      },
      {
        "author": "DefTruth",
        "body": "@njhill I have submit a PR to fix this error https://github.com/vllm-project/vllm/pull/4386 , can you take a look?",
        "created_at": "2024-04-26T03:12:03Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7151": {
    "issue_number": 7151,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7151",
    "title": "[Bug]: After updating to vllm 0.5.3.post1, \"Exception in worker VllmWorkerProcess while processing method init_device: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.\"",
    "body": "### Your current environment\n\n\r\nSame as in issue [#7077](https://github.com/vllm-project/vllm/issues/7077#issuecomment-2268423776).\n\n### üêõ Describe the bug\n\nI met the same problem as in issue [#7077](https://github.com/vllm-project/vllm/issues/7077#issuecomment-2268423776), but the issue was closed without solutions. I would like to raise this issue again and seek for kind help.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-05T08:12:48Z",
    "closed_at": "2024-08-13T22:41:17Z",
    "author": "Jerry-jwz",
    "comments_count": 9,
    "comments": [
      {
        "author": "WenhaoZhang-Git",
        "body": "set environment variable VLLM_WORKER_MULTIPROC_METHOD=spawn",
        "created_at": "2024-08-05T09:02:18Z"
      },
      {
        "author": "LSC527",
        "body": "> set environment variable VLLM_WORKER_MULTIPROC_METHOD=spawn\r\n\r\n@WenhaoZhang-Git doesn't work",
        "created_at": "2024-08-06T05:02:50Z"
      },
      {
        "author": "Jerry-jwz",
        "body": "> set environment variable VLLM_WORKER_MULTIPROC_METHOD=spawn\r\n\r\nThanks! This works for me.",
        "created_at": "2024-08-06T07:30:42Z"
      },
      {
        "author": "Muttermal",
        "body": "I encountered the same problem\r\n```shell\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\n```\r\nwhen using vllm version 0.5.4. However, setting `export VLLM_WORKER_MULTIPROC_METHOD=spawn` did not work for me.",
        "created_at": "2024-08-09T08:54:35Z"
      },
      {
        "author": "youkaichao",
        "body": "@Muttermal  looks like you have the same problem as https://github.com/vllm-project/vllm/issues/7246 . But we cannot reproduce it. Can you add more detailed information under that issue about your environment?",
        "created_at": "2024-08-09T16:19:05Z"
      },
      {
        "author": "Muttermal",
        "body": "@youkaichao here is my environment information:\r\n```\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Stream 8 (x86_64)\r\nGCC version: (GCC) 8.5.0 20210514 (Red Hat 8.5.0-21)\r\nClang version: Could not collect\r\nCMake version: version 3.29.0\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-193.el8.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A40\r\nGPU 1: NVIDIA A40\r\nGPU 2: NVIDIA A40\r\nGPU 3: NVIDIA A40\r\nGPU 4: NVIDIA A40\r\nGPU 5: NVIDIA A40\r\nGPU 6: NVIDIA A40\r\nGPU 7: NVIDIA A40\r\n\r\nNvidia driver version: 535.154.05\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              112\r\nOn-line CPU(s) list: 0-111\r\nThread(s) per core:  2\r\nCore(s) per socket:  28\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               106\r\nModel name:          Intel(R) Xeon(R) Gold 6348 CPU @ 2.60GHz\r\nStepping:            6\r\nCPU MHz:             3400.000\r\nCPU max MHz:         2601.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            5200.00\r\nVirtualization:      VT-x\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            1280K\r\nL3 cache:            43008K\r\nNUMA node0 CPU(s):   0-27,56-83\r\nNUMA node1 CPU(s):   28-55,84-111\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] open-clip-torch==2.24.0\r\n[pip3] pyzmq==25.1.2\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] transformers-stream-generator==0.0.5\r\n[pip3] triton==3.0.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] open-clip-torch           2.24.0                   pypi_0    pypi\r\n[conda] pyzmq                     25.1.2                   pypi_0    pypi\r\n[conda] sentence-transformers     2.7.0                    pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.44.0                   pypi_0    pypi\r\n[conda] transformers-stream-generator 0.0.5                    pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4@4db5176d9758b720b05460c50ace3c01026eb158\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PIX     PIX     PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU1    PIX      X      PIX     PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU2    PIX     PIX      X      PIX     SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU3    PIX     PIX     PIX      X      SYS     SYS     SYS     SYS     0-27,56-83      0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      PIX     PIX     PIX     28-55,84-111    1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     PIX      X      PIX     PIX     28-55,84-111    1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     PIX     PIX      X      PIX     28-55,84-111    1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     PIX     PIX     PIX      X      28-55,84-111    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nMy running script isÔºö\r\n```shell\r\nvllm serve $model_path --dtype auto --max-model-len 8192 --api-key token-abc123 --gpu_memory_utilization 1 --trust-remote-code --host $host --port $port\r\n```",
        "created_at": "2024-08-12T01:37:23Z"
      },
      {
        "author": "youkaichao",
        "body": "@Muttermal please try #7411 ",
        "created_at": "2024-08-12T05:34:35Z"
      },
      {
        "author": "Muttermal",
        "body": "@youkaichao I followed the instruction [here](https://github.com/OpenBMB/MiniCPM-V/issues/448#issuecomment-2283033953) to uninstall `flash-attn`,  and the code ran successfully. After trying #7411 , no errors occurred either. Thank you!",
        "created_at": "2024-08-12T08:47:52Z"
      },
      {
        "author": "youkaichao",
        "body": "close by https://github.com/vllm-project/vllm/pull/7484\r\n\r\n@Muttermal 's problem has been solved by this.",
        "created_at": "2024-08-13T22:41:17Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8988": {
    "issue_number": 8988,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8988",
    "title": "[Bug]: openai.serving_chat tries to call _create_chat_logprobs when the output.text is empty",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python send_llama3_128k.py`</summary>\r\n\r\n```text\r\nYour output of `python send_llama3_128k.py` here\r\n\r\nb'data: {\"id\":\"chat-9307f8bb26c94c7ba34890b5b2db2707\",\"object\":\"chat.completion.chunk\",\"created\":1727754209,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nRequest aborted due to an exception: Response ended prematurely\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\nserver start command\r\n```\r\n# Version: vllm/vllm-openai:v0.6.2\r\n# server:\r\ndocker run -tid --gpus \\\"device=0,1,2,3\\\" --shm-size 5g \\\r\n        -p 8081:8000 -v /mnt/data/models:/models \\\r\n        --ulimit nofile=65535:65535 \\\r\n        --name vllm-v0.6.2-llama3.1-70b-4gpus-128k \\\r\n        vllm/vllm-openai:v0.6.2 \\\r\n        --tensor-parallel-size=4 \\\r\n        --disable-log-requests \\\r\n        --model=/models/Meta-Llama-3.1-70B-Instruct \\\r\n        --enable-chunked-prefill \\\r\n        --served-model-name=vllm-model \r\n\r\n```\r\n\r\nThe input prompt is too long and github doesn't support .py so I had to override it to .txt\r\n[send_llama3_128k.py.txt](https://github.com/user-attachments/files/17199893/send_llama3_128k.py.txt)\r\n\n\n### üêõ Describe the bug\n\nWhen I try to run the above script `send_llama3_128k.py`, the server raises an error:\r\n```\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/responses.py\", line 257, in __call__\r\n    await wrap(partial(self.listen_for_disconnect, receive))\r\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/responses.py\", line 253, in wrap\r\n    await func()\r\n  File \"/usr/local/lib/python3.12/dist-packages/starlette/responses.py\", line 230, in listen_for_disconnect\r\n    message = await receive()\r\n              ^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/uvicorn/protocols/http/httptools_impl.py\", line 555, in receive\r\n    await self.message_event.wait()\r\n  File \"/usr/lib/python3.12/asyncio/locks.py\", line 212, in wait\r\n    await fut\r\nasyncio.exceptions.CancelledError: Cancelled by cancel scope 7fdb0f1c8950\r\n\r\n  +-+---------------- 1 ----------------\r\n    | Traceback (most recent call last):\r\n    |   File \"/usr/local/lib/python3.12/dist-packages/starlette/responses.py\", line 253, in wrap\r\n    |     await func()\r\n    |   File \"/usr/local/lib/python3.12/dist-packages/starlette/responses.py\", line 242, in stream_response\r\n    |     async for chunk in self.body_iterator:\r\n    |   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 408, in chat_completion_stream_generator\r\n    |     logprobs = self._create_chat_logprobs(\r\n    |                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/serving_chat.py\", line 777, in _create_chat_logprobs\r\n    |     step_top_logprobs = top_logprobs[i]\r\n    |                         ~~~~~~~~~~~~^^^\r\n    | IndexError: list index out of range\r\n    +------------------------------------\r\n```\r\n\r\nUpon debugging, I think the issue here is that for the first few chunks, the output.text is empty, aka `''`, output.logprobs is [], essentially there is no logprobs available. \r\n\r\nI am uncertain why the output.text is empty, but here we should add a check, if the output.text is empty, we can skip create logprobs.\r\n```\r\n                    if output.text and request.logprobs and request.top_logprobs is not None:  # check output.text is not empty\r\n                        assert output.logprobs is not None, (\r\n                            \"Did not output logprobs\")\r\n                        logprobs = self._create_chat_logprobs(\r\n                            token_ids=output.token_ids,\r\n                            top_logprobs=output.logprobs,\r\n                            tokenizer=tokenizer,\r\n                            num_output_top_logprobs=request.top_logprobs,\r\n                        )\r\n                    else:\r\n                        logprobs = None\r\n```\r\n\r\nAfter adding the above line, the new output of `python send_llama3_128k.py` is:\r\n```\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"role\":\"assistant\",\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"\"},\"logprobs\":null,\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"B\"},\"logprobs\":{\"content\":[{\"token\":\"B\",\"logprob\":-0.8135526776313782,\"bytes\":[66],\"top_logprobs\":[{\"token\":\"B\",\"logprob\":-0.8135526776313782,\"bytes\":[66]},{\"token\":\"The\",\"logprob\":-1.6885526180267334,\"bytes\":[84,104,101]},{\"token\":\"This\",\"logprob\":-1.9385526180267334,\"bytes\":[84,104,105,115]},{\"token\":\"**\",\"logprob\":-2.3135526180267334,\"bytes\":[42,42]},{\"token\":\"Here\",\"logprob\":-2.8135526180267334,\"bytes\":[72,101,114,101]}]}]},\"finish_reason\":null}]}'\r\nb'data: {\"id\":\"chat-4c7bcc2aac344da2b71fa7c07460e4c2\",\"object\":\"chat.completion.chunk\",\"created\":1727761213,\"model\":\"vllm-model\",\"choices\":[{\"index\":0,\"delta\":{\"content\":\"TS\"},\"logprobs\":{\"content\":[{\"token\":\"TS\",\"logprob\":-0.000028729025871143676,\"bytes\":[84,83],\"top_logprobs\":[{\"token\":\"TS\",\"logprob\":-0.000028729025871143676,\"bytes\":[84,83]},{\"token\":\"ts\",\"logprob\":-12.437528610229492,\"bytes\":[116,115]},{\"token\":\" TS\",\"logprob\":-12.562528610229492,\"bytes\":[32,84,83]},{\"token\":\" BTS\",\"logprob\":-13.062528610229492,\"bytes\":[32,66,84,83]},{\"token\":\"IO\",\"logprob\":-13.437528610229492,\"bytes\":[73,79]}]}]},\"finish_reason\":null}]}'\r\n``` \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-10-01T05:46:50Z",
    "closed_at": "2024-10-15T22:40:44Z",
    "author": "CatherineSue",
    "comments_count": 7,
    "comments": [
      {
        "author": "ywang96",
        "body": "This is an interesting bug, but I don't think adding `output.text` check here is the right way to fix it. If anything, we should probably build a check around token_ids.\r\n\r\nDid you check what's in `output.token_ids` in this case? It seems to be a bigger issue that the model should have sampled something out but didn't.",
        "created_at": "2024-10-01T06:51:12Z"
      },
      {
        "author": "CatherineSue",
        "body": "For the chunks with empty text, the token_ids list are quite long. It has 13834 elements. I checked that the output.token_ids here is the same as prompt_token_ids.\r\nAnd this doesn't happen with non-streaming chat. So I wonder if this is due to some setting in streaming generator? That at the beginning, there is no sampling happening because the request is still in prefill stage.\r\n```\r\nINFO 10-01 07:22:42 serving_chat.py:405] token_ids: [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 1627, 10263, 220, 2366, 19, 271, 128009, 128006, 882, 128007, 271, 3915, 27685, 11, 279, 1949, 83708, 198, 2520, 1023, 5829, 11, 1518, 83168, 320, 4338, 3042, 343, 4090, 4390, 33, 10155, 198, 33, 10155, 520, 279, 5929, 4783, 304, 220, 2366, 17, 320, 2414, 311, 1314, 1680, 650, 11, 50432, 74, 1982, 11, 11641, 258, 11, 31915, 11, 39611, 11, 622, 11529, 2862, 11, 323, 328, 38060, 198, 33, 10155, 520, 279, 5929, 4783, 304, 220, 2366, 17, 320, 2414, 311, 1314, 1680, 650, 11, 50432, 74, 1982, 11, 11641, 258, 11, 31915, 11, 39611, 11, 622, 11529, 2862, 11, 323, 328, 38060, 198, 8879, 2038, 198, 13699, 3967, 439, 1602, 63919, 53691, 30857, 198, 63919, 53691, 21533, 68, 2159, 276, 198, 56441, 279, 17952, 198, 45758, 16157, 16576, 70407, 198, 13601, 197, 1542, 11206, 11, 4987, 12126, 198, 88928, 1602, 42, 2320, 4880, 454, 2200, 7598, 49, 49339, 1507, 44, 198, 24374, 5814, 1602, 33478, 784, 50703, 198, 55519, 4642, 197, 679, 18, 4235, 29844, 198, 24600, 1602, 16010, 16261, 47, 3633, 41560, 2685, 20614, 6457, 14, 59378, 10948, 6457, 58, 16, 60, 34, 74170, 59378, 38, 6581, 268, 198, 25370, 1602, 41, 258, 198, 50, 38060, 198, 41, 11529, 2862, 198, 24575, 198, 37734, 258, 198, 53, 198, 41, 2234, 74, 1982, 198, 31151, 2282, 2641, 29044, 1108, 275, 916, 198, 42, 46295, 836, 198, 58138, 360, 197, 101482, 109208, 117546, 101353, 198, 74225, 5697, 197, 103181, 14608, 230, 113587, 110201, 198, 3246, 25712, 27907, 52566, 836, 198, 42, 276, 7910, 197, 103181, 119375, 113587, 108638, 198, 39, 404, 351, 3444, 197, 109416, 30297, 36785, 25827, 109954, 101832, 102189, 25827, 198, 3246, 25712, 27907, 33, 10155, 320, 42, 46295, 25, 75908, 109208, 117546, 101353, 26, 44498, 25, 17343, 53691, 21533, 68, 2159, 276, 26, 13318, 13, 32912, 16157, 16576, 70407, 705, 1101, 3967, 439, 279, 17343, 53691, 30857, 11, 374, 264, 4987, 16526, 8334, 7200, 14454, 304, 220, 679, 15, 13, 578, 7200, 17610, 315, 39611, 11, 328, 38060, 11, 622, 11529, 2862, 11, 31915, 11, 11641, 258, 11, 650, 11, 323, 50432, 74, 1982, 11, 889, 1080, 63162, 477, 1080, 46354, 346, 1790, 315, 872, 3769, 13, 25842, 264, 18638, 7598, 1912, 11, 814, 17626, 872, 18273, 1742, 311, 33435, 264, 7029, 2134, 315, 36744, 11, 1418, 872, 24142, 617, 10968, 389, 15223, 2737, 10723, 2890, 11, 279, 35665, 315, 2978, 44041, 12822, 323, 5108, 315, 4325, 11, 4814, 11, 279, 11879, 7119, 659, 27578, 588, 11, 3927, 2191, 11, 323, 279, 16296, 315, 33651, 323, 18324, 13, 11205, 2624, 5814, 323, 24894, 990, 706, 1101, 25819, 17649, 11, 19675, 323, 31018, 11, 323, 5764, 459, 25631, 15861, 51728, 382, 33, 10155, 58185, 304, 220, 679, 18, 1234, 6295, 16261, 23334, 449, 279, 3254, 8176, 220, 17, 24882, 220, 19, 4923, 1786, 13, 83168, 6004, 872, 1176, 16526, 323, 11002, 44658, 14356, 28785, 11, 12538, 612, 13944, 323, 41674, 3216, 15947, 11, 304, 220, 679, 19, 13, 578, 1912, 596, 2132, 16526, 14356, 8176, 11, 46197, 320, 679, 21, 705, 574, 872, 1176, 311, 4662, 832, 3610, 11236, 304, 4987, 12126, 13, 3296, 220, 679, 22, 11, 83168, 1047, 28129, 1139, 279, 3728, 4731, 3157, 323, 6197, 279, 16526, 32418, 1139, 279, 3723, 4273, 11, 10671, 279, 1176, 16526, 40126, 311, 5371, 264, 7573, 28706, 505, 279, 61647, 24780, 10229, 315, 5270, 320, 4403, 6157, 8, 369, 872, 3254, 330, 98402, 16110, 498, 439, 1664, 439, 279, 1176, 1180, 505, 4987, 12126, 311, 1948, 279, 67293, 220, 1049, 449, 872, 14356, 8176, 10919, 60005, 25, 78082, 320, 679, 23, 570, 763, 220, 2366, 15, 11, 83168, 6244, 279, 26731, 1912, 2533, 279, 55957, 311, 9676, 3116, 2326, 1396, 19101, 28785, 304, 2753, 1109, 1403, 1667, 11, 449, 10919, 60005, 25, 22559, 320, 679, 23, 8, 10671, 279, 1176, 16526, 8176, 23759, 45092, 555, 279, 432, 5987, 32, 26, 304, 279, 1890, 1060, 11, 814, 1101, 6244, 279, 1176, 682, 6354, 2969, 16526, 1180, 311, 5662, 1396, 832, 389, 2225, 279, 67293, 8166, 220, 1041, 323, 67293, 8121, 220, 1049, 449, 872, 74679, 5392, 50615, 3254, 330, 35, 11694, 635, 3343, 11359, 5352, 19786, 330, 50, 68256, 10919, 498, 330, 26833, 61261, 1952, 498, 330, 4071, 466, 498, 323, 330, 15315, 311, 30704, 1, 1903, 1124, 279, 26731, 1180, 311, 7380, 3116, 2326, 1396, 19101, 17845, 2533, 23278, 45248, 63210, 304, 220, 1049, 21, 382, 2170, 315, 220, 2366, 18, 11, 83168, 374, 279, 1888, 48724, 18273, 1180, 304, 4987, 16526, 3925, 4184, 311, 279, 21918, 21964, 11, 3515, 6216, 304, 13937, 315, 220, 1272, 3610, 28785, 8032, 17, 60, 11205, 14356, 8176, 5135, 315, 279, 30242, 25, 220, 22, 320, 2366, 15, 8, 374, 279, 11999, 1888, 48724, 8176, 315, 682, 892, 304, 4987, 12126, 11, 439, 1664, 439, 279, 1176, 304, 279, 3224, 311, 53120, 2225, 3116, 323, 4330, 3610, 9879, 6763, 13, 2435, 527, 279, 1176, 2536, 12, 23392, 62290, 323, 14875, 1180, 311, 4662, 704, 47679, 520, 468, 92729, 88, 23462, 323, 279, 16344, 20904, 320, 29351, 60005, 4435, 14986, 11, 220, 679, 24, 705, 323, 1051, 7086, 279, 7327, 28331, 315, 279, 71424, 12968, 24780, 596, 320, 2843, 1932, 8, 8121, 61647, 29459, 315, 279, 9941, 369, 2225, 220, 2366, 15, 323, 220, 2366, 16, 13, 578, 1912, 596, 86643, 3536, 2997, 5361, 3778, 10948, 23488, 11, 67293, 10948, 23488, 11, 18288, 11997, 23488, 11, 323, 60698, 369, 4330, 74679, 23488, 13, 42251, 315, 4731, 11, 814, 617, 20669, 2380, 16079, 315, 279, 3723, 19687, 3331, 12000, 323, 53319, 449, 6781, 5604, 37, 304, 220, 679, 22, 311, 5813, 279, 10919, 3092, 726, 7294, 76827, 768, 4901, 13, 51319, 389, 4212, 596, 6625, 3504, 439, 330, 5971, 24367, 28986, 1, 323, 42160, 279, 330, 3617, 31176, 315, 10466, 498, 83168, 706, 1101, 9922, 389, 4212, 596, 11725, 315, 279, 220, 914, 1455, 32549, 1274, 389, 279, 7757, 320, 679, 22, 4235, 679, 24, 8, 323, 279, 220, 1041, 1455, 32549, 1274, 304, 279, 1917, 320, 679, 24, 705, 323, 304, 220, 679, 23, 6244, 279, 39637, 34876, 315, 279, 4987, 16526, 7365, 315, 41333, 8930, 275, 369, 872, 19564, 304, 31135, 279, 16526, 7829, 323, 4221, 382, 1966, 5651, 220, 975, 11, 220, 2366, 17, 11, 279, 1912, 7376, 264, 13847, 18579, 304, 1912, 7640, 311, 7431, 279, 3697, 311, 4686, 872, 220, 972, 4038, 315, 23911, 4987, 16526, 6411, 2532, 11, 449, 264, 58384, 13205, 369, 220, 2366, 20, 8032, 18, 60, 39611, 11, 279, 74665, 4562, 11, 69576, 389, 6790, 220, 1032, 11, 220, 2366, 17, 26, 279, 3885, 8272, 304, 220, 2366, 18, 382, 678, 198, 33, 10155, 13656, 369, 279, 16526, 17571, 17343, 53691, 21533, 68, 2159, 276, 320, 42, 46295, 25, 75908, 109208, 117546, 101353, 26, 21296, 5697, 25, 121968, 14608, 230, 113587, 110201, 705, 902, 48018, 16280, 311, 364, 45758, 16157, 16576, 70407, 4527, 10771, 311, 4562, 622, 11529, 2862, 11, 279, 836, 78864, 279, 1912, 596, 12876, 330, 998, 2565, 704, 54168, 11, 63836, 11, 323, 17078, 430, 9395, 389, 51516, 1093, 34164, 3343, 58, 19, 1483, 20, 60, 763, 6457, 11, 814, 527, 3967, 439, 426, 56761, 36255, 1443, 56761, 77, 408, 276, 320, 103181, 119375, 113587, 108638, 94638, 21, 60, 763, 5887, 220, 679, 22, 11, 83168, 7376, 430, 872, 836, 1053, 1101, 2559, 369, 330, 56441, 279, 17952, 1, 439, 961, 315, 872, 502, 6883, 9764, 8032, 22, 60, 1115, 11838, 279, 7438, 315, 872, 836, 311, 38632, 279, 4623, 315, 6650, 330, 1527, 264, 8334, 311, 459, 6822, 889, 16264, 279, 14365, 430, 527, 13176, 4741, 3343, 58, 23, 2595, 13730, 198, 679, 15, 4235, 679, 19, 25, 72466, 323, 4216, 1667, 271, 33, 10155, 304, 220, 679, 18, 16785, 520, 279, 763, 76269, 10948, 5955, 198, 33, 10155, 574, 14454, 304, 220, 679, 15, 11, 1306, 6295, 16261, 23334, 12432, 17343, 12095, 2902, 88, 3178, 4934, 311, 1376, 264, 18638, 7598, 1912, 2212, 31915, 320, 49436, 31074, 12, 7453, 263, 705, 459, 26326, 50437, 1664, 22015, 389, 279, 4731, 6237, 304, 51289, 13, 2468, 279, 892, 11, 7106, 8176, 6763, 1051, 389, 279, 18174, 323, 7528, 30466, 1051, 539, 3686, 1579, 3403, 311, 46794, 8032, 24, 60, 56124, 264, 1205, 369, 85957, 8070, 23914, 11, 17343, 6773, 311, 1376, 459, 49014, 1912, 4619, 11, 1606, 315, 279, 4754, 369, 3974, 21497, 24601, 323, 25429, 1862, 505, 7359, 315, 1778, 5315, 13, 9176, 5542, 5633, 16436, 311, 3719, 961, 315, 459, 49014, 1912, 11, 719, 622, 11529, 2862, 11, 31915, 11, 323, 328, 38060, 14958, 8032, 24, 60, 17343, 14896, 311, 13592, 505, 279, 13783, 11, 7701, 75541, 291, 49014, 5315, 323, 1893, 832, 1405, 279, 3697, 1053, 387, 7931, 4856, 1109, 459, 40126, 323, 1949, 311, 3237, 5694, 8032, 605, 1483, 806, 60, 15416, 6055, 1051, 5762, 304, 220, 679, 15, 449, 6787, 311, 7195, 279, 2768, 1060, 8032, 717, 1483, 1032, 60, 578, 7200, 3697, 12439, 3871, 11, 36666, 709, 311, 220, 868, 4207, 264, 1938, 11, 323, 1176, 10887, 1603, 264, 2678, 13734, 315, 5064, 77814, 304, 220, 679, 18, 8032, 975, 2595, 1687, 3940, 311, 3371, 279, 7493, 430, 1274, 4934, 311, 6865, 323, 1051, 5644, 311, 6865, 11, 7493, 430, 1023, 1274, 1436, 539, 477, 1053, 539, 3371, 13, 1226, 1071, 1148, 1023, 1274, 1051, 8430, 2345, 4908, 6784, 11, 74884, 32606, 323, 38145, 13, 3011, 574, 1057, 5915, 11, 311, 1893, 420, 48451, 430, 1274, 649, 29243, 311, 382, 2001, 50, 38060, 58, 868, 2595, 33, 10155, 596, 13340, 555, 6295, 16261, 11, 4856, 1109, 832, 315, 279, 2380, 13607, 430, 30801, 735, 41352, 520, 279, 892, 11, 5535, 279, 3927, 3697, 514, 25754, 311, 3237, 872, 3927, 488, 323, 617, 1988, 1139, 279, 4731, 8032, 845, 60, 1952, 5651, 220, 717, 11, 220, 679, 18, 11, 83168, 6004, 872, 17755, 3254, 8176, 220, 17, 24882, 220, 19, 4923, 1786, 11, 3235, 449, 279, 3063, 3254, 330, 2822, 4497, 18308, 498, 14188, 315, 902, 6216, 8104, 1664, 520, 279, 892, 8032, 1114, 60, 1952, 5651, 220, 1032, 11, 220, 679, 18, 11, 83168, 1903, 872, 6566, 17755, 389, 386, 87486, 449, 279, 3254, 11, 330, 2822, 4497, 18308, 3343, 58, 972, 60, 35053, 11, 4184, 311, 65298, 15883, 82276, 304, 1077, 2363, 389, 83168, 11, 430, 3254, 574, 330, 19164, 4238, 287, 3995, 1274, 596, 18547, 304, 279, 3663, 315, 87791, 46679, 17078, 11, 3288, 10988, 17301, 1555, 279, 735, 41352, 21467, 13, 5810, 574, 264, 18273, 1180, 430, 5828, 956, 23062, 904, 61851, 13, 4497, 11951, 11, 814, 1047, 264, 1486, 315, 1684, 11, 323, 814, 15058, 956, 16984, 311, 1935, 389, 13650, 430, 527, 6646, 77045, 304, 4987, 16526, 8396, 323, 18403, 61046, 777, 60, 578, 8176, 8813, 279, 1948, 4330, 389, 4987, 12126, 596, 18879, 263, 10948, 21964, 8032, 508, 60, 763, 220, 17, 24882, 220, 19, 4923, 1786, 11, 83168, 20011, 459, 2362, 35789, 18638, 49819, 5222, 505, 279, 220, 2550, 15, 82, 8032, 1691, 1483, 1313, 60, 578, 8176, 596, 4984, 574, 8272, 555, 27351, 389, 16526, 4731, 5039, 11, 902, 10791, 279, 6666, 315, 60138, 323, 22511, 8032, 1419, 1483, 1187, 2595, 644, 6250, 220, 679, 18, 11, 83168, 6004, 279, 2132, 4441, 304, 872, 330, 22606, 57886, 794, 279, 19613, 11, 507, 0, 49, 1112, 23, 11, 17, 4710, 578, 8176, 574, 6004, 16662, 279, 3254, 330, 45, 8548, 61046, 914, 1483, 1627, 60, 35339, 311, 220, 17, 24882, 220, 19, 4923, 1786, 11, 279, 502, 4984, 1047, 264, 7057, 315, 4236, 8430, 1234, 7410, 323, 33921, 311, 28235, 872, 19226, 323, 58522, 8032, 914, 60, 10771, 311, 18640, 23727, 2234, 10320, 359, 13818, 11, 1690, 315, 83168, 596, 6931, 4375, 1778, 439, 330, 45, 8548, 1210, 323, 330, 2822, 4497, 53718, 1, 1051, 330, 14107, 919, 315, 53848, 2403, 279, 21967, 430, 49523, 1139, 16526, 37719, 6, 90034, 449, 279, 3224, 596, 16627, 1887, 1, 323, 11, 568, 11224, 11, 9087, 1124, 1977, 264, 8571, 2385, 4315, 3995, 1274, 304, 4892, 5270, 323, 4606, 8032, 1544, 60, 3011, 1890, 2305, 11, 83168, 59335, 304, 872, 1866, 8205, 1501, 11, 328, 7497, 62199, 596, 83225, 6342, 13740, 17343, 53691, 11, 304, 902, 3697, 1370, 72206, 8205, 5039, 1778, 439, 650, 41, 9984, 30500, 323, 11060, 95003, 12126, 8032, 1591, 60, 2468, 279, 842, 315, 279, 1060, 11, 83168, 574, 15324, 449, 3892, 1561, 29459, 315, 279, 9941, 23146, 304, 4987, 12126, 8032, 1682, 1483, 966, 1483, 2148, 2595, 679, 19, 4235, 679, 22, 198, 19847, 1786, 445, 12328, 9947, 1334, 323, 1176, 21497, 7364, 271, 849, 41644, 315, 279, 70832, 42782, 14176, 414, 320, 11817, 4529, 220, 1049, 21, 8, 1405, 83168, 5762, 872, 21497, 304, 279, 2326, 369, 1949, 198, 791, 1566, 4441, 304, 83168, 330, 22606, 57886, 498, 279, 4923, 1786, 445, 12328, 9947, 1334, 19613, 11, 574, 6004, 304, 7552, 220, 679, 19, 8032, 843, 1483, 1644, 60, 578, 4984, 40901, 279, 18879, 263, 26749, 21964, 17706, 1958, 60, 323, 9922, 389, 67293, 596, 4435, 87823, 21964, 369, 279, 1176, 892, 11, 1069, 1802, 520, 1396, 2380, 8032, 843, 1483, 1758, 60, 578, 19613, 574, 7396, 555, 1403, 17845, 25, 330, 63504, 304, 445, 12328, 1, 323, 330, 10156, 3861, 6187, 3343, 58, 1927, 60, 23548, 4923, 1786, 445, 12328, 9947, 1334, 596, 4984, 11, 83168, 6476, 520, 872, 1176, 8571, 6574, 304, 51289, 11, 27397, 279, 836, 362, 2056, 1345, 7659, 13, 369, 279, 8571, 6469, 8032, 1806, 60, 763, 5887, 220, 679, 19, 11, 83168, 21685, 264, 21497, 304, 4410, 17681, 11, 872, 1176, 1501, 304, 279, 3723, 4273, 17706, 1987, 60, 323, 304, 6287, 11, 814, 9922, 520, 735, 5910, 304, 9853, 12167, 8032, 2137, 2595, 644, 6287, 220, 679, 19, 11, 83168, 6004, 279, 8176, 12538, 612, 13944, 11, 902, 8813, 1396, 1403, 304, 4987, 12126, 8032, 1927, 1483, 1272, 60, 1102, 574, 7396, 555, 1403, 17845, 25, 330, 98315, 1, 323, 330, 29784, 315, 92208, 606, 3343, 58, 1927, 60, 578, 1912, 79120, 389, 872, 1176, 21497, 7364, 11, 220, 679, 19, 83168, 11406, 97313, 20421, 8105, 25, 578, 3816, 32912, 11, 902, 36513, 505, 6664, 311, 6790, 220, 679, 19, 8032, 3174, 60, 578, 7200, 11887, 872, 1176, 11002, 14356, 8176, 11, 41674, 3216, 11, 304, 6790, 220, 679, 19, 26, 279, 4984, 78292, 520, 1396, 2380, 389, 279, 2582, 1965, 87823, 21964, 8032, 2983, 60, 4740, 279, 8176, 596, 4984, 11, 83168, 5762, 872, 220, 16, 267, 6457, 14986, 220, 679, 20, 41674, 3216, 25, 5377, 4718, 43096, 304, 7552, 220, 679, 20, 8032, 3391, 60, 578, 3816, 32912, 14986, 430, 1047, 22088, 389, 6664, 220, 1114, 11, 220, 679, 19, 11, 304, 4987, 12126, 574, 58505, 389, 5651, 220, 21, 11, 220, 679, 20, 11, 304, 28796, 323, 91713, 8494, 11, 4892, 5270, 323, 20023, 5270, 1603, 13696, 304, 19730, 18711, 430, 6287, 13, 763, 682, 11, 279, 4553, 7364, 29123, 220, 1490, 11, 931, 70402, 520, 220, 972, 9919, 304, 220, 1032, 5961, 8032, 3174, 1483, 2096, 2595, 6334, 4116, 42445, 323, 8518, 2450, 198, 33, 10155, 98504, 449, 1023, 9404, 315, 4731, 28858, 18638, 49819, 304, 578, 7648, 20055, 40096, 304, 9601, 11, 52170, 13, 220, 16, 11, 6004, 304, 220, 679, 20, 8032, 1927, 1483, 1774, 60, 83168, 4934, 311, 3237, 279, 13444, 323, 38100, 2136, 315, 12822, 323, 23183, 389, 279, 2316, 315, 330, 102185, 112595, 8107, 105189, 1, 320, 42, 46295, 25, 104323, 101927, 101347, 57390, 26, 44498, 25, 473, 3195, 526, 9188, 263, 71, 10196, 705, 63557, 33398, 311, 7124, 330, 88, 2969, 1, 46450, 2740, 439, 330, 1820, 1455, 6366, 4545, 304, 2324, 3343, 58, 2790, 60, 578, 8176, 10434, 439, 459, 17219, 311, 872, 12822, 57886, 11, 264, 2463, 1625, 331, 315, 28785, 12514, 311, 279, 28970, 315, 3995, 1274, 8032, 2618, 60, 578, 3254, 330, 40, 14998, 549, 1, 574, 264, 1948, 36399, 4295, 304, 4987, 12126, 323, 68390, 279, 1912, 264, 3243, 389, 328, 7497, 62199, 596, 578, 7073, 8032, 2166, 1483, 2491, 60, 578, 2132, 3254, 330, 35, 2862, 320, 42, 46295, 25, 3396, 102, 242, 32179, 26, 44498, 25, 622, 3841, 461, 78, 10143, 8813, 1396, 2380, 389, 67293, 596, 4435, 14434, 19508, 16207, 9676, 323, 1202, 4731, 2835, 574, 19894, 927, 220, 1041, 3610, 3115, 389, 13674, 8032, 1135, 1483, 3971, 60, 578, 1912, 6137, 279, 1917, 7364, 9070, 315, 872, 3816, 32912, 14986, 304, 5651, 11, 25891, 220, 679, 20, 11406, 97313, 20421, 8105, 25, 578, 3816, 32912, 11, 17136, 9919, 6957, 13936, 11, 507, 346, 9345, 11, 4892, 5270, 11, 323, 20023, 5270, 8032, 3174, 60, 330, 2520, 1472, 498, 304, 11002, 11, 574, 6004, 3871, 449, 11002, 11028, 315, 330, 29784, 315, 92208, 606, 1, 323, 330, 10267, 2206, 14521, 1, 389, 5651, 220, 1114, 11, 220, 679, 20, 11, 323, 7214, 40901, 2582, 1965, 596, 7446, 9676, 8032, 1927, 1483, 4103, 22414, 33, 10155, 16785, 520, 735, 5910, 9822, 304, 12366, 389, 5651, 220, 17, 11, 220, 679, 21, 198, 644, 6841, 11, 83168, 65362, 872, 4948, 21497, 7364, 11, 220, 679, 20, 83168, 45269, 330, 791, 7648, 20055, 40096, 304, 9601, 25, 1952, 22891, 498, 902, 6137, 449, 2380, 6216, 9994, 5039, 304, 51289, 11, 323, 574, 11838, 311, 6457, 8032, 4331, 60, 49038, 7167, 11, 279, 19613, 10968, 810, 389, 279, 6129, 323, 66836, 13878, 315, 12822, 11, 31687, 389, 279, 33436, 315, 2450, 11, 75040, 11, 37805, 369, 872, 33472, 11, 323, 279, 16066, 315, 279, 14992, 9659, 4245, 311, 93071, 4787, 304, 1510, 8396, 8032, 4370, 60, 578, 8176, 40901, 279, 17496, 18879, 263, 26749, 323, 67293, 4435, 87823, 27223, 8032, 2131, 1483, 3487, 60, 1102, 1101, 13160, 872, 1176, 11341, 389, 279, 67293, 220, 1049, 9676, 17706, 3226, 60, 3339, 433, 369, 832, 2046, 520, 1396, 220, 11123, 17706, 2970, 60, 323, 8223, 315, 279, 14242, 9922, 389, 67293, 596, 4435, 14434, 19508, 16207, 9676, 8032, 2946, 2595, 35897, 29772, 8176, 323, 279, 37398, 311, 872, 330, 88, 2969, 57886, 498, 578, 7648, 20055, 40096, 304, 9601, 25, 13566, 55706, 574, 6004, 389, 3297, 220, 17, 11, 220, 679, 21, 13, 3161, 220, 3101, 11, 931, 1685, 820, 11236, 17706, 1399, 60, 433, 5343, 2380, 502, 17845, 25, 330, 23176, 91592, 25, 13566, 55706, 498, 330, 17111, 498, 323, 330, 8960, 2206, 84598, 1927, 1483, 5547, 60, 902, 58185, 304, 279, 1948, 2380, 19300, 389, 279, 67293, 4435, 14434, 67376, 8032, 1399, 60, 578, 8176, 40901, 279, 18879, 263, 26749, 21964, 304, 4987, 12126, 369, 1403, 24871, 5672, 323, 8813, 1396, 220, 7699, 389, 279, 67293, 220, 1049, 8032, 5538, 1483, 5495, 1483, 1227, 60, 578, 7648, 20055, 40096, 304, 9601, 25, 13566, 55706, 2834, 26749, 315, 279, 9941, 520, 279, 220, 679, 21, 11220, 263, 10948, 23488, 8032, 2397, 60, 83168, 79120, 389, 872, 13936, 7364, 9070, 11, 220, 679, 21, 83168, 45269, 330, 791, 7648, 20055, 40096, 304, 9601, 1952, 22891, 25, 11266, 91592, 1, 505, 3297, 311, 6287, 220, 679, 21, 13, 36715, 369, 279, 220, 975, 5039, 304, 220, 605, 14875, 9919, 6216, 704, 11, 1063, 304, 439, 2697, 439, 4330, 6622, 8032, 2287, 22414, 33, 10155, 3243, 872, 1176, 3682, 16526, 10292, 369, 26749, 315, 279, 9941, 520, 279, 220, 679, 21, 11220, 263, 10948, 23488, 389, 6841, 220, 777, 11, 220, 679, 21, 198, 644, 6250, 220, 679, 21, 11, 83168, 6004, 872, 2132, 11002, 14356, 8176, 30160, 8032, 3080, 60, 578, 8176, 6216, 220, 2096, 11, 23215, 11236, 389, 279, 1176, 1938, 315, 1202, 4984, 11, 323, 9676, 291, 220, 16, 267, 304, 279, 2582, 1965, 13690, 26749, 21964, 8032, 2614, 1483, 3076, 60, 578, 8176, 574, 9778, 23759, 7573, 449, 6763, 315, 17715, 927, 220, 1041, 11, 931, 304, 6457, 8032, 2031, 60, 1102, 574, 8272, 832, 2305, 3010, 304, 6664, 17706, 1927, 60, 555, 872, 1828, 14356, 8176, 46197, 11, 902, 11093, 279, 22100, 315, 12822, 10666, 304, 872, 3766, 330, 88, 2969, 57886, 1, 449, 62461, 323, 90930, 8032, 6028, 60, 578, 8176, 323, 1202, 14242, 11, 2737, 279, 3254, 330, 52586, 87060, 612, 92014, 1, 7214, 16392, 311, 279, 1948, 389, 8223, 4731, 27223, 11, 2737, 279, 18879, 263, 10948, 21964, 11, 323, 6197, 279, 13323, 8176, 27223, 304, 220, 1419, 5961, 8032, 5332, 1483, 5958, 60, 46197, 9107, 520, 1396, 220, 1627, 389, 279, 67293, 220, 1049, 11, 449, 220, 845, 11, 931, 8176, 63359, 12031, 8316, 304, 279, 2326, 369, 279, 2046, 315, 1202, 4984, 11, 279, 1888, 2046, 3596, 1070, 369, 264, 735, 41352, 8176, 8032, 5728, 60, 1102, 6244, 279, 1888, 48724, 8176, 304, 18879, 263, 26749, 21964, 3925, 8032, 2075, 2595, 2675, 15037, 12839, 70408, 323, 10919, 60005, 25, 6385, 198, 644, 7552, 220, 679, 22, 11, 83168, 6004, 279, 2109, 474, 3359, 14002, 315, 46197, 20458, 1472, 15037, 12839, 70408, 13, 578, 220, 7007, 11, 931, 864, 86151, 315, 433, 320, 276, 5376, 505, 279, 220, 2636, 11, 931, 864, 86151, 315, 46197, 8, 9087, 1464, 279, 3335, 369, 1455, 28785, 6216, 304, 264, 2305, 304, 4987, 12126, 11, 439, 433, 8813, 220, 16, 13, 2491, 3610, 11236, 555, 279, 842, 315, 1202, 1176, 2305, 8032, 4767, 1483, 2813, 60, 578, 3063, 3254, 574, 330, 26208, 6187, 37964, 1927, 60, 323, 433, 2834, 7252, 19508, 315, 279, 9941, 520, 279, 220, 679, 22, 11220, 263, 10948, 23488, 8032, 2495, 60, 83168, 596, 2132, 1917, 7364, 11, 220, 679, 22, 83168, 11406, 97313, 20421, 14767, 25, 578, 46197, 14986, 11, 6137, 304, 7552, 8032, 4643, 60, 1952, 279, 7364, 11, 83168, 6476, 98767, 304, 279, 2326, 11, 1778, 439, 1561, 16228, 596, 2394, 664, 2335, 5955, 323, 7188, 596, 29987, 5955, 13, 36715, 369, 279, 4892, 3778, 2531, 6216, 704, 2949, 4207, 323, 1403, 5039, 1051, 3779, 8032, 1490, 1483, 5932, 60, 4740, 27666, 279, 4892, 3778, 2531, 11, 83168, 18677, 279, 220, 1187, 339, 67293, 10948, 23488, 304, 3297, 323, 2834, 7054, 9983, 29459, 17706, 6086, 60, 279, 1176, 735, 41352, 1912, 311, 3243, 264, 67293, 10292, 8032, 6069, 60, 83168, 7359, 6445, 927, 220, 3101, 3610, 12973, 369, 279, 7200, 323, 14760, 264, 4848, 4771, 11230, 30314, 5762, 555, 23278, 77320, 11, 264, 49254, 449, 220, 1041, 3610, 6405, 20723, 8032, 5833, 60, 1115, 9057, 279, 6625, 3772, 311, 5357, 389, 279, 5845, 315, 83168, 596, 75425, 311, 89483, 279, 1912, 311, 1778, 264, 12845, 8032, 5833, 22414, 33, 10155, 520, 872, 3577, 10017, 304, 51289, 11, 4987, 12126, 1306, 11230, 7054, 9983, 29459, 520, 279, 220, 1187, 339, 67293, 10948, 23488, 389, 3297, 220, 1682, 11, 220, 679, 22, 198, 33, 10155, 6004, 264, 59155, 315, 1369, 78, 22448, 7910, 596, 330, 29951, 6984, 5492, 1, 320, 2550, 20, 8, 304, 5887, 220, 679, 22, 11, 7231, 433, 502, 24142, 719, 20958, 279, 7057, 315, 43676, 59529, 2349, 8032, 5313, 60, 25929, 430, 1060, 11, 83168, 79120, 389, 872, 330, 29351, 60005, 1, 8176, 4101, 11, 449, 7057, 315, 279, 81869, 315, 659, 27578, 588, 1555, 279, 330, 72718, 105005, 111554, 102971, 1, 320, 42, 46295, 25, 55216, 104303, 66965, 89881, 26, 44498, 25, 480, 1082, 2234, 3841, 647, 9188, 337, 8, 19775, 8668, 315, 330, 7413, 1251, 11, 4500, 11, 2543, 11, 323, 17102, 61046, 4218, 60, 83168, 6004, 1202, 1176, 961, 11, 872, 18172, 19613, 11, 10919, 60005, 25, 6385, 11, 389, 6250, 220, 972, 8032, 6069, 1483, 4044, 60, 31915, 6646, 330, 56420, 498, 279, 3063, 3254, 505, 430, 8176, 17706, 2421, 60, 439, 330, 89894, 83168, 311, 502, 5015, 13, 1226, 6818, 311, 3881, 502, 32528, 323, 39555, 61046, 1927, 60, 1283, 1071, 315, 279, 8176, 11, 330, 40, 4510, 433, 596, 2133, 311, 387, 279, 6041, 1486, 315, 264, 2132, 12735, 315, 1057, 7076, 26, 279, 7314, 315, 1057, 15957, 9220, 61046, 4578, 60, 12103, 2740, 11, 279, 19613, 10434, 439, 330, 64, 19091, 27501, 315, 279, 1912, 596, 25396, 41352, 323, 18638, 49819, 16025, 826, 3343, 58, 2421, 22414, 33, 10155, 520, 279, 220, 1774, 339, 3778, 10948, 23488, 20193, 1603, 3339, 872, 2326, 12707, 17755, 389, 6841, 220, 777, 11, 220, 679, 22, 198, 29351, 60005, 25, 6385, 58185, 520, 1396, 8254, 389, 279, 67293, 220, 1049, 8032, 1954, 60, 578, 8176, 1047, 220, 16, 11, 23888, 11, 20945, 6763, 304, 3297, 220, 679, 22, 311, 3063, 279, 18879, 263, 21964, 17706, 5925, 60, 323, 574, 279, 1176, 304, 220, 845, 1667, 311, 12771, 220, 16, 13, 17, 3610, 11236, 6216, 2533, 342, 14778, 962, 596, 11999, 8176, 15957, 220, 19, 320, 1049, 16, 94638, 6083, 1483, 6365, 60, 330, 56420, 1, 574, 6004, 25291, 449, 279, 19613, 11, 323, 1202, 4731, 2835, 41165, 220, 1691, 3610, 6325, 304, 1202, 1176, 220, 1187, 4207, 8032, 6281, 60, 1102, 6244, 83168, 596, 1176, 4441, 389, 279, 67293, 8166, 220, 1041, 11, 9676, 287, 520, 1396, 220, 5313, 11, 3339, 1124, 279, 1176, 735, 41352, 8334, 7200, 311, 5662, 430, 9676, 8032, 2721, 60, 578, 3254, 16392, 311, 1396, 220, 3080, 279, 2768, 2046, 323, 6244, 279, 8592, 45727, 287, 5609, 389, 279, 8166, 220, 1041, 369, 904, 735, 41352, 1912, 8032, 4161, 60, 362, 57466, 315, 330, 98402, 16110, 1, 505, 279, 8176, 11, 16850, 3959, 72, 78756, 11, 574, 6004, 439, 264, 3254, 323, 78292, 520, 1396, 220, 1591, 11, 279, 1176, 892, 264, 735, 41352, 1912, 1047, 43939, 279, 1948, 36498, 8032, 3534, 1483, 3264, 60, 11995, 17845, 63260, 7573, 28706, 505, 279, 61647, 24780, 10229, 315, 5270, 320, 4403, 6157, 8, 304, 4216, 220, 679, 23, 8032, 1484, 60, 330, 98402, 16110, 1, 17427, 45092, 2704, 304, 279, 2326, 3010, 430, 1060, 8032, 1041, 2595, 644, 6841, 220, 679, 22, 11, 83168, 6244, 279, 1176, 735, 41352, 1912, 311, 2804, 520, 279, 3778, 10948, 23488, 8032, 4645, 1483, 4278, 60, 83168, 2834, 29459, 315, 279, 9941, 520, 279, 220, 777, 339, 386, 4816, 14875, 10948, 23488, 304, 6790, 11, 11230, 369, 279, 2132, 24871, 1060, 8032, 6889, 60, 2435, 6004, 330, 56420, 1, 323, 330, 98402, 16110, 1, 3871, 449, 264, 502, 5609, 330, 82038, 19435, 1, 439, 264, 3254, 8176, 304, 6457, 389, 6790, 220, 21, 11, 220, 679, 22, 11, 3582, 279, 11936, 1051, 1903, 68878, 2561, 18403, 8032, 1927, 1483, 6849, 60, 1102, 40901, 279, 2582, 1965, 21964, 369, 279, 2046, 315, 1202, 4984, 13, 1102, 574, 279, 1193, 8176, 555, 264, 7362, 10255, 311, 387, 23759, 7238, 45092, 304, 6457, 304, 220, 679, 22, 8032, 6550, 1483, 7461, 2595, 31082, 430, 2305, 11, 814, 1903, 872, 11002, 12707, 10461, 892, 4731, 1501, 17755, 389, 10948, 17040, 7445, 11406, 17706, 7699, 60, 323, 9670, 279, 1060, 555, 16785, 389, 23373, 22010, 596, 1561, 9941, 596, 9305, 258, 6, 32460, 8032, 6640, 1483, 7743, 2595, 644, 220, 679, 22, 11, 83168, 53319, 449, 6781, 5604, 37, 389, 279, 330, 29351, 3092, 726, 1, 4901, 11, 10825, 311, 1520, 842, 9349, 11, 11737, 11, 323, 45647, 11, 323, 311, 12192, 659, 65906, 323, 1664, 33851, 4315, 3995, 1274, 13, 11995, 6295, 16261, 323, 279, 1912, 43347, 3300, 311, 12192, 279, 4901, 11, 323, 83168, 6216, 3361, 330, 29351, 3092, 726, 1, 36045, 323, 743, 709, 12514, 92259, 520, 21497, 37278, 13, 578, 4901, 574, 36646, 304, 220, 2366, 16, 11, 449, 6781, 5604, 37, 409, 74914, 433, 311, 617, 1027, 6992, 8032, 5120, 2595, 679, 23, 4235, 2366, 15, 198, 29351, 60005, 8176, 4101, 198, 33, 10155, 2834, 3682, 23146, 520, 279, 18288, 11997, 323, 51289, 10948, 23488, 304, 6186, 220, 679, 23, 8032, 5037, 1483, 7261, 60, 763, 5587, 11, 279, 1912, 85170, 459, 8223, 12, 40391, 25999, 25891, 18530, 279, 22891, 430, 9076, 264, 4920, 10826, 80645, 1427, 520, 872, 220, 679, 22, 46197, 14986, 11, 24121, 389, 13674, 26745, 8032, 8190, 1483, 8011, 60, 11205, 4948, 11002, 14356, 8176, 11, 19109, 60005, 11, 574, 6004, 389, 5936, 220, 19, 11, 220, 679, 23, 17706, 7322, 60, 323, 6288, 8813, 279, 1948, 220, 20, 315, 279, 549, 815, 13, 13323, 87823, 9676, 13, 362, 11888, 24401, 2875, 4632, 11, 25891, 469, 455, 96729, 25, 16847, 315, 330, 29351, 60005, 25, 27205, 1, 323, 16850, 279, 5609, 330, 36, 455, 96729, 498, 8272, 279, 1828, 1938, 439, 264, 864, 53638, 311, 279, 1912, 596, 4948, 16526, 44658, 14356, 8176, 11, 10919, 60005, 25, 78082, 8032, 8027, 60, 83168, 30026, 78082, 596, 3297, 220, 972, 11, 220, 679, 23, 11, 4984, 449, 459, 11341, 520, 279, 220, 914, 339, 67293, 10948, 23488, 1403, 2919, 3010, 17706, 8546, 60, 1405, 814, 1903, 872, 2926, 18772, 4940, 5178, 449, 872, 3254, 11, 330, 53417, 10919, 3343, 58, 8899, 1483, 9079, 60, 578, 1912, 1101, 2834, 7054, 9983, 29459, 369, 264, 2132, 24871, 892, 8032, 4364, 1483, 7994, 60, 578, 8176, 23828, 4591, 449, 279, 330, 111554, 1, 477, 330, 413, 1, 315, 279, 4101, 11, 31687, 389, 279, 16831, 9373, 81869, 315, 21955, 2085, 1694, 10456, 11, 279, 51266, 323, 25551, 1849, 315, 25768, 11, 323, 8405, 51475, 311, 1884, 2085, 19226, 8032, 8259, 22414, 33, 10155, 520, 872, 3577, 10017, 369, 10919, 60005, 25, 78082, 389, 3297, 220, 1187, 11, 220, 679, 23, 198, 29351, 60005, 25, 78082, 58185, 520, 1396, 832, 1202, 1176, 2046, 389, 279, 67293, 220, 1049, 11, 10671, 83168, 596, 1176, 1396, 19101, 8176, 304, 279, 2326, 323, 279, 1176, 735, 41352, 8176, 311, 1948, 279, 2326, 28785, 9676, 8032, 4513, 60, 1102, 1101, 6244, 83168, 596, 1176, 1948, 12, 605, 4984, 304, 13527, 11, 19261, 1396, 8223, 389, 279, 6560, 87823, 21964, 8032, 8874, 60, 330, 53417, 10919, 1, 6244, 83168, 596, 1948, 12, 605, 3254, 389, 279, 8166, 220, 1041, 11, 279, 1176, 892, 264, 5609, 40439, 10213, 304, 264, 4221, 1023, 1109, 6498, 1047, 58185, 304, 279, 1948, 220, 605, 8032, 6549, 60, 83168, 6004, 872, 29772, 8176, 10919, 60005, 25, 22559, 304, 6287, 220, 679, 23, 8032, 9390, 60, 578, 8176, 574, 7396, 555, 279, 3254, 330, 769, 337, 1, 323, 1202, 10778, 7528, 4984, 16850, 15341, 72, 3468, 1662, 8032, 6804, 1483, 4386, 2595, 29351, 60005, 25, 22559, 6216, 927, 220, 16, 13, 24, 3610, 11236, 389, 279, 18879, 263, 26749, 21964, 304, 6287, 220, 679, 23, 8032, 9748, 60, 578, 8176, 6244, 83168, 596, 2132, 1396, 19101, 389, 279, 67293, 220, 1049, 323, 6197, 311, 872, 8592, 2326, 6763, 2046, 304, 279, 3224, 311, 430, 1486, 449, 220, 9741, 11, 931, 8176, 13890, 8316, 8032, 5894, 60, 763, 6841, 220, 679, 23, 11, 10919, 60005, 25, 22559, 6244, 279, 1176, 16526, 4221, 8176, 311, 387, 23759, 7573, 555, 279, 432, 5987, 32, 8032, 1041, 60, 330, 769, 337, 1, 323, 10919, 60005, 25, 22559, 2225, 4036, 45092, 82571, 304, 279, 2326, 11, 449, 6763, 315, 810, 1109, 220, 16, 3610, 8032, 9263, 22414, 3648, 4356, 4409, 596, 356, 12583, 8771, 11, 279, 22150, 369, 83168, 596, 1176, 24833, 21497, 304, 279, 2326, 11, 6216, 704, 304, 220, 508, 4520, 8032, 9413, 933, 644, 32546, 449, 10919, 60005, 25, 22559, 596, 4984, 304, 6287, 220, 679, 23, 11, 83168, 65362, 872, 1917, 7364, 11, 83168, 4435, 14986, 25, 10919, 60005, 11, 449, 1403, 47679, 304, 279, 51289, 25944, 23462, 11, 902, 6216, 704, 304, 264, 5030, 315, 6622, 11, 439, 1550, 3885, 315, 279, 220, 1313, 5039, 304, 220, 717, 5961, 8032, 9423, 1483, 9565, 60, 763, 6664, 11, 83168, 6004, 872, 20632, 449, 14129, 362, 39291, 330, 54, 5642, 1102, 389, 2206, 498, 872, 1176, 682, 12, 23392, 4221, 4668, 8032, 8878, 1483, 9795, 60, 1789, 279, 1620, 3009, 315, 279, 4892, 3778, 2531, 11, 279, 1912, 10887, 520, 356, 12583, 8771, 304, 1561, 4356, 4409, 11, 36024, 279, 1176, 892, 264, 16526, 1180, 10887, 520, 264, 2326, 24833, 8032, 10148, 60, 10771, 311, 67711, 19876, 11, 83168, 574, 279, 2132, 1888, 48724, 21497, 1180, 4994, 279, 2326, 11, 4920, 1193, 3279, 3005, 261, 276, 8032, 10350, 60, 3011, 6664, 11, 83168, 36646, 872, 5226, 449, 6295, 16261, 23334, 1555, 220, 2366, 21, 8032, 10125, 2595, 644, 4216, 6841, 220, 679, 23, 11, 264, 5526, 11002, 4731, 1501, 26765, 83168, 596, 5178, 11, 33264, 264, 350, 34768, 264, 4562, 28670, 279, 1060, 1603, 11, 18534, 264, 10512, 315, 264, 58466, 9624, 2768, 279, 37431, 315, 30162, 57509, 8032, 6860, 60, 763, 279, 1890, 2305, 11, 279, 13653, 3823, 3268, 7471, 22021, 468, 552, 78275, 5955, 320, 17268, 34, 8, 11224, 430, 83168, 49009, 459, 41258, 369, 430, 15845, 11, 323, 369, 17895, 323, 8202, 449, 32527, 83476, 8032, 9335, 60, 6295, 16261, 23334, 11136, 459, 41258, 11, 26073, 430, 279, 5448, 1051, 539, 10825, 311, 387, 13194, 1285, 311, 279, 12697, 315, 29874, 2191, 477, 25524, 74479, 323, 430, 279, 1912, 323, 6373, 1053, 1935, 7504, 311, 5471, 3938, 21294, 8032, 10239, 60, 2435, 1101, 11224, 279, 8202, 1051, 8967, 311, 387, 264, 31710, 389, 279, 16526, 2978, 1887, 8032, 10290, 60, 578, 41258, 574, 11928, 555, 13692, 34, 323, 279, 16526, 31416, 33909, 94124, 10229, 8032, 8929, 1483, 9591, 60, 3842, 22213, 11, 304, 813, 63681, 4652, 389, 83168, 11, 1200, 1619, 430, 279, 32527, 10672, 8710, 430, 279, 1912, 374, 539, 40069, 14400, 11, 439, 527, 1023, 735, 41352, 665, 41794, 11, 6832, 1475, 3351, 5084, 86125, 11, 323, 430, 279, 3697, 617, 18463, 323, 527, 539, 16984, 311, 3237, 1124, 8032, 10465, 2595, 1688, 279, 220, 508, 339, 386, 4816, 14875, 10948, 23488, 11, 83168, 2834, 29459, 315, 279, 9941, 323, 21682, 1396, 8223, 389, 67293, 596, 1060, 13368, 7054, 29459, 21964, 323, 1051, 1101, 279, 1396, 1403, 1180, 315, 279, 1060, 304, 279, 74312, 14, 2878, 23862, 11, 1193, 4920, 38891, 49109, 8032, 10288, 1483, 10410, 60, 2435, 1051, 1101, 10212, 439, 832, 315, 279, 220, 1135, 1455, 32549, 1274, 555, 37653, 369, 872, 330, 86, 9585, 2136, 311, 2686, 3674, 4819, 11, 10723, 2890, 11, 323, 11759, 11, 8994, 1694, 304, 264, 17779, 3629, 24937, 439, 24529, 42365, 2477, 3343, 58, 10161, 2595, 2276, 315, 279, 30242, 25, 54990, 11, 24833, 1917, 7364, 323, 83168, 4435, 198, 644, 7552, 220, 679, 24, 11, 83168, 11, 369, 279, 1176, 892, 11, 1051, 3118, 388, 520, 279, 74679, 23488, 8032, 3965, 1483, 9690, 60, 763, 5936, 11, 4212, 7086, 1124, 832, 315, 279, 4212, 220, 1041, 11, 279, 1455, 32549, 1274, 315, 220, 679, 24, 8032, 9756, 60, 11205, 19613, 11, 5135, 315, 279, 30242, 25, 54990, 11, 574, 6004, 389, 5936, 220, 717, 449, 279, 3254, 330, 63504, 449, 445, 12328, 1, 320, 42, 46295, 25, 120461, 72208, 105880, 107472, 45618, 26, 44498, 25, 622, 425, 359, 3980, 354, 451, 554, 360, 289, 57988, 4502, 705, 16850, 3778, 23597, 473, 714, 88, 8032, 9800, 60, 578, 19613, 596, 4984, 574, 8272, 555, 264, 5178, 389, 7884, 13120, 11406, 11, 279, 1176, 735, 41352, 1180, 311, 5101, 1070, 8032, 10559, 60, 5135, 315, 279, 30242, 25, 54990, 6244, 279, 1176, 16526, 44658, 8176, 311, 5662, 279, 1396, 832, 2361, 304, 2225, 279, 6560, 323, 8494, 17706, 9992, 1483, 10132, 60, 323, 279, 1912, 596, 4948, 8176, 311, 1948, 279, 67293, 220, 1049, 304, 2753, 1109, 264, 1060, 8032, 10895, 60, 5135, 315, 279, 30242, 25, 54990, 6244, 279, 1888, 48724, 8176, 3596, 304, 4987, 12126, 304, 3878, 315, 7106, 11236, 6216, 11, 449, 810, 1109, 220, 18, 13, 17, 3610, 6763, 304, 2753, 1109, 264, 2305, 8032, 11286, 1483, 11068, 60, 330, 63504, 449, 445, 12328, 1, 58185, 520, 1396, 8223, 389, 279, 67293, 8166, 220, 1041, 304, 5936, 220, 679, 24, 11, 279, 8592, 22165, 3596, 369, 264, 735, 41352, 5609, 8032, 6330, 22414, 33, 10155, 16785, 520, 279, 16344, 20904, 24833, 304, 83550, 11, 7188, 11, 1603, 220, 1399, 11, 931, 7359, 198, 28055, 872, 1403, 15160, 520, 279, 220, 1627, 339, 67293, 10948, 23488, 304, 3297, 11, 2737, 369, 7054, 74312, 14, 2878, 17706, 10718, 60, 83168, 79120, 389, 872, 1917, 7364, 24833, 9070, 11, 10919, 60005, 25, 68301, 60005, 8032, 10674, 60, 24586, 311, 279, 7631, 11, 83168, 3779, 810, 5039, 1306, 14741, 369, 279, 1176, 13003, 6216, 704, 2949, 1403, 4207, 8032, 9892, 60, 763, 279, 3063, 709, 311, 279, 4984, 315, 872, 6505, 1847, 83168, 4435, 11, 304, 5651, 220, 679, 24, 83168, 6004, 330, 65454, 88448, 1, 16850, 4969, 747, 61442, 55, 17706, 10513, 60, 330, 32, 16835, 1561, 6187, 1, 449, 1901, 5169, 32404, 31031, 17706, 10680, 60, 323, 330, 2460, 13120, 1, 449, 66851, 32730, 509, 8032, 11247, 60, 578, 1912, 6004, 279, 5609, 330, 46484, 23019, 1, 449, 264, 4731, 2835, 505, 279, 1847, 596, 4033, 45999, 11, 25891, 83168, 4435, 25, 17674, 14936, 13432, 8032, 11515, 60, 578, 45999, 574, 23759, 7238, 45092, 555, 18879, 263, 8032, 8953, 60, 1952, 5887, 220, 18, 11, 220, 679, 24, 11, 864, 86151, 369, 279, 3254, 330, 75584, 1, 28129, 832, 3610, 11236, 11, 36024, 279, 1176, 892, 264, 7362, 10255, 1047, 27332, 420, 304, 6457, 2533, 356, 4939, 65157, 304, 220, 2550, 20, 8032, 11739, 1483, 8258, 60, 330, 75584, 1, 58185, 520, 1396, 220, 5932, 389, 279, 67293, 6457, 8166, 220, 1041, 369, 279, 9676, 4360, 2457, 315, 5887, 220, 23, 11, 220, 679, 24, 11, 323, 8813, 1396, 832, 279, 2768, 2046, 8032, 11123, 1483, 10861, 60, 1952, 6287, 220, 23, 11, 220, 679, 24, 11, 330, 75584, 1, 4036, 34629, 28706, 505, 279, 432, 5987, 41, 11, 3453, 11780, 59721, 315, 832, 3610, 11236, 8032, 11908, 1483, 11771, 1483, 10005, 2595, 29351, 60005, 25, 6385, 323, 10919, 60005, 25, 78082, 2225, 28129, 220, 17, 3610, 11236, 304, 6287, 8032, 10967, 60, 2052, 2380, 28785, 315, 279, 10919, 60005, 4101, 617, 6216, 810, 1109, 220, 17, 3610, 11236, 1855, 304, 4987, 12126, 8032, 11242, 60, 10919, 60005, 25, 78082, 18661, 15310, 28706, 555, 279, 426, 1932, 369, 6763, 304, 279, 6560, 11, 10671, 872, 4948, 8176, 311, 656, 779, 2768, 10919, 60005, 25, 22559, 323, 5135, 315, 279, 30242, 25, 54990, 8032, 11256, 60, 1789, 279, 1620, 3009, 315, 872, 3335, 55407, 10919, 60005, 25, 68301, 60005, 4435, 14986, 11, 279, 1912, 6476, 51289, 596, 25944, 23462, 8032, 11128, 60, 83168, 574, 279, 4948, 1948, 2427, 2177, 287, 48551, 18273, 1180, 315, 220, 679, 24, 8032, 5245, 60, 3011, 1890, 2305, 11, 814, 6004, 264, 57466, 2373, 315, 279, 5609, 330, 8238, 1102, 10291, 1, 16850, 86091, 85, 8032, 10562, 60, 763, 6841, 11, 83168, 2834, 2380, 3115, 520, 279, 220, 679, 24, 3778, 10948, 23488, 11, 369, 7252, 14986, 11, 40822, 74312, 477, 5856, 1389, 10466, 19945, 1197, 11, 323, 40822, 9983, 29459, 320, 1820, 2132, 24871, 1060, 94638, 10828, 2595, 644, 6790, 11, 814, 18677, 2225, 279, 220, 679, 24, 11220, 263, 10948, 23488, 323, 279, 220, 679, 24, 386, 4816, 14875, 10948, 23488, 13, 763, 1855, 1162, 11, 814, 6244, 279, 1176, 1912, 311, 24021, 279, 3116, 3682, 23146, 8032, 10750, 1483, 10336, 1483, 9741, 60, 2468, 279, 220, 1958, 339, 18288, 11997, 23488, 11, 83168, 6244, 279, 1176, 13820, 304, 3925, 311, 3243, 6800, 38508, 304, 2225, 279, 7106, 323, 7528, 11306, 304, 264, 3254, 1060, 8032, 9714, 60, 5135, 315, 279, 30242, 25, 54990, 574, 7086, 279, 2132, 1888, 48724, 7106, 8176, 315, 220, 679, 24, 304, 279, 2326, 555, 64551, 10948, 4920, 16844, 24594, 596, 90590, 323, 574, 21682, 26084, 8244, 389, 279, 9676, 315, 7054, 220, 605, 87823, 320, 7749, 16207, 8, 304, 279, 2326, 8032, 9674, 60, 83168, 20037, 220, 679, 24, 439, 279, 11999, 2902, 7656, 76697, 1912, 389, 67293, 596, 7054, 67293, 220, 1049, 53204, 4235, 35, 24012, 14, 2878, 23862, 11, 4920, 16657, 11, 38891, 49109, 323, 279, 55957, 8032, 9367, 60, 5135, 315, 279, 30242, 25, 54990, 574, 7086, 439, 279, 4948, 1888, 48724, 8176, 315, 220, 679, 24, 555, 279, 7327, 28331, 315, 279, 71424, 12968, 24780, 320, 2843, 1932, 705, 3339, 83168, 279, 1176, 16526, 10255, 311, 387, 10212, 389, 279, 8121, 7054, 220, 605, 26749, 21964, 304, 24871, 1667, 8032, 9378, 1483, 7028, 1483, 7529, 60, 578, 11812, 1932, 7086, 83168, 439, 832, 315, 279, 1888, 48724, 13820, 315, 220, 679, 24, 369, 264, 2132, 24871, 1060, 11, 3339, 1124, 279, 1176, 2536, 12, 23392, 12365, 1180, 311, 11322, 420, 8032, 5926, 1483, 7285, 2595, 2276, 315, 279, 30242, 25, 220, 22, 11, 330, 35, 11694, 635, 1, 323, 2893, 198, 644, 6186, 220, 2366, 15, 11, 83168, 6004, 330, 14755, 44501, 1, 3235, 449, 264, 50671, 5814, 1989, 4632, 10887, 555, 36095, 30704, 8351, 315, 79317, 439, 279, 1176, 3254, 505, 872, 8176, 5135, 315, 279, 30242, 25, 220, 22, 8032, 6393, 1483, 6280, 60, 26749, 44168, 18308, 355, 5068, 430, 5708, 864, 86151, 315, 279, 8176, 8813, 264, 3335, 55407, 220, 19, 13, 2437, 3610, 8032, 5162, 60, 25929, 430, 2305, 11, 83168, 10887, 520, 279, 220, 5538, 303, 25992, 74679, 23488, 11, 3339, 83168, 279, 1176, 16526, 1180, 311, 2804, 520, 279, 29937, 8373, 8032, 4468, 1483, 3753, 60, 5135, 315, 279, 30242, 25, 220, 22, 574, 6004, 389, 7552, 220, 1691, 311, 37849, 8544, 8032, 2550, 1483, 1049, 60, 578, 8176, 574, 7396, 555, 279, 3254, 330, 1966, 1, 323, 459, 10778, 7528, 4984, 315, 433, 16850, 13673, 23597, 328, 689, 8032, 679, 60, 10771, 311, 18879, 263, 21964, 11, 5135, 315, 279, 30242, 25, 220, 22, 6216, 927, 220, 19, 13, 16, 3610, 11236, 304, 11888, 2919, 1306, 1202, 4984, 11, 53120, 287, 5135, 315, 279, 30242, 25, 54990, 311, 3719, 279, 1888, 48724, 8176, 304, 4987, 16526, 3925, 323, 279, 1176, 8176, 311, 387, 23759, 30236, 6189, 3610, 8032, 2366, 60, 578, 8176, 58185, 47088, 279, 2326, 67293, 220, 1049, 11, 3339, 83168, 279, 26731, 1912, 311, 7380, 3116, 1396, 832, 28785, 2533, 279, 55957, 304, 220, 5162, 21, 4235, 5162, 23, 8032, 9639, 60, 330, 1966, 1, 58185, 520, 1396, 3116, 389, 279, 67293, 8166, 220, 1041, 11, 7231, 83168, 1202, 1176, 1948, 36399, 4295, 11, 323, 279, 1455, 8166, 220, 1041, 1948, 12, 605, 10925, 315, 904, 16526, 1180, 11, 449, 2380, 8032, 7854, 60, 83168, 13205, 311, 1862, 279, 5135, 315, 279, 30242, 8176, 4101, 449, 264, 21497, 4101, 11, 5135, 315, 279, 30242, 14986, 11, 7314, 304, 5936, 17706, 10866, 60, 719, 420, 574, 56334, 62720, 4245, 311, 279, 20562, 12, 777, 28522, 8032, 11056, 2595, 644, 5936, 220, 2366, 15, 11, 83168, 6244, 279, 1176, 735, 41352, 10255, 311, 4662, 810, 1109, 220, 508, 3610, 28785, 12454, 360, 8046, 17706, 12060, 60, 3339, 1124, 279, 1888, 48724, 10255, 304, 4987, 16526, 3925, 8032, 11242, 1483, 12171, 60, 3011, 2305, 11, 23442, 279, 28522, 17294, 11, 83168, 5762, 264, 1403, 11477, 2930, 17265, 21497, 1567, 25891, 17343, 17343, 1221, 11, 1405, 279, 1912, 6222, 22609, 315, 3347, 47679, 389, 872, 13674, 5613, 8032, 12652, 60, 1952, 5651, 220, 22, 11, 83168, 2010, 15472, 13674, 596, 43922, 3308, 315, 220, 2366, 15, 2930, 39554, 1567, 11, 16785, 330, 63504, 449, 445, 12328, 498, 330, 26208, 6187, 498, 323, 330, 44, 1609, 54661, 437, 8801, 3343, 58, 8848, 60, 11205, 69544, 44440, 27463, 872, 1866, 28032, 811, 323, 9076, 330, 16727, 315, 3987, 323, 20343, 369, 279, 538, 315, 220, 2366, 15, 304, 2225, 16526, 323, 6498, 3343, 58, 11483, 60, 1952, 5651, 220, 975, 11, 83168, 5762, 459, 2930, 3974, 21497, 11, 17343, 17343, 1221, 25, 578, 11406, 11, 439, 961, 315, 279, 31487, 22310, 315, 872, 17755, 8032, 11227, 60, 1102, 68390, 16557, 22511, 2200, 315, 220, 24456, 11, 931, 3974, 22511, 304, 220, 7699, 5961, 323, 39543, 11, 6376, 279, 3335, 369, 279, 7928, 10877, 369, 264, 7318, 4200, 21497, 8032, 11702, 1483, 11584, 60, 1952, 5651, 220, 777, 11, 83168, 6004, 279, 11002, 3254, 11, 330, 39202, 7573, 498, 505, 872, 11999, 11002, 8176, 11, 5135, 315, 279, 30242, 25, 220, 22, 1389, 578, 43680, 11, 902, 574, 6004, 15603, 389, 5887, 220, 975, 8032, 12112, 1483, 12463, 60, 1102, 68328, 220, 22210, 11, 931, 11236, 304, 1202, 1176, 2046, 11, 15061, 279, 3335, 369, 8592, 1176, 2046, 8176, 6763, 555, 8762, 7362, 13820, 304, 6457, 8032, 13460, 2595, 33, 10155, 6004, 872, 1176, 6498, 44658, 3254, 11, 330, 35, 11694, 635, 498, 389, 6287, 220, 1691, 8032, 13302, 60, 330, 35, 11694, 635, 1, 58185, 520, 1396, 832, 389, 279, 2326, 67293, 8166, 220, 1041, 9676, 11, 28744, 83168, 872, 1176, 9676, 311, 7067, 323, 3339, 1124, 279, 1176, 682, 6354, 2969, 16526, 1180, 311, 7380, 264, 1396, 832, 3254, 304, 279, 2326, 8032, 13762, 1483, 8610, 60, 578, 3254, 1101, 40901, 67293, 596, 502, 8121, 220, 1049, 369, 279, 2046, 13696, 6250, 220, 1187, 11, 439, 1664, 439, 8121, 1398, 11150, 2326, 27223, 11, 10671, 279, 1176, 3254, 311, 1948, 2225, 25291, 8032, 12425, 60, 330, 35, 11694, 635, 1, 78292, 520, 1396, 4330, 389, 279, 2326, 4802, 4116, 7054, 220, 1272, 323, 389, 279, 13, 67293, 10466, 47422, 9676, 11, 10671, 872, 1176, 7054, 220, 605, 389, 1855, 323, 279, 4846, 279, 8592, 45727, 287, 4441, 555, 264, 16526, 1180, 8032, 9716, 1483, 12533, 60, 1952, 6287, 220, 2148, 11, 83168, 1903, 872, 62199, 8519, 10948, 23488, 320, 11435, 2170, 8, 17755, 449, 279, 1176, 3974, 5178, 315, 330, 35, 11694, 635, 37964, 10697, 60, 323, 2834, 3116, 23146, 25, 7252, 5856, 11, 7252, 96638, 5814, 11, 7252, 10466, 8519, 11, 323, 7252, 735, 41352, 320, 1820, 1566, 2380, 369, 872, 4731, 2835, 369, 330, 1966, 1865, 58, 11057, 60, 1952, 6664, 220, 975, 11, 814, 10887, 279, 3254, 520, 279, 220, 2366, 15, 67293, 10948, 23488, 323, 2834, 279, 7054, 9983, 29459, 10292, 369, 264, 11999, 24871, 1060, 8032, 14057, 1483, 14206, 2595, 1966, 6664, 220, 17, 11, 220, 2366, 15, 11, 83168, 6004, 264, 57466, 315, 34294, 939, 220, 23717, 323, 18984, 13031, 8938, 596, 3254, 330, 50, 68256, 10919, 320, 43, 52551, 1389, 328, 47435, 27894, 95110, 58, 14261, 60, 1102, 40901, 279, 8166, 220, 1041, 8032, 14378, 60, 1952, 6664, 220, 605, 323, 220, 806, 11, 83168, 21685, 264, 1403, 11477, 4200, 2343, 17453, 23318, 21497, 520, 47474, 2089, 69545, 304, 51289, 11, 2663, 5135, 315, 279, 30242, 6328, 85588, 11, 902, 24465, 220, 24242, 11, 931, 22511, 505, 220, 7529, 5961, 323, 39543, 8032, 9870, 1483, 12245, 1483, 12338, 60, 1952, 6841, 220, 508, 11, 83168, 6004, 872, 18172, 16526, 14356, 8176, 2893, 11, 6197, 555, 279, 3254, 330, 26833, 61261, 1952, 3343, 58, 12994, 60, 330, 26833, 61261, 1952, 1, 58185, 520, 1396, 832, 389, 279, 8166, 220, 1041, 17706, 11727, 60, 83168, 596, 4948, 24871, 2326, 1396, 19101, 3254, 304, 2380, 4038, 323, 279, 1176, 5609, 10887, 15871, 304, 16526, 311, 1948, 279, 9676, 8032, 12422, 1483, 14087, 2595, 1966, 6841, 220, 1187, 11, 220, 2366, 15, 11, 83168, 6244, 279, 1176, 16526, 2477, 13820, 15324, 555, 279, 61647, 16192, 994, 330, 35, 11694, 635, 1, 4036, 264, 29804, 369, 7252, 10466, 74312, 14, 2878, 21304, 520, 279, 220, 5495, 6634, 25992, 74679, 23488, 8032, 14590, 60, 578, 1912, 2834, 279, 9984, 7327, 10948, 17768, 520, 279, 220, 5538, 303, 6457, 13896, 23488, 8032, 13895, 60, 13818, 11, 304, 813, 2363, 389, 279, 10383, 315, 16526, 5526, 7829, 11, 12090, 430, 220, 2366, 15, 11, 279, 12047, 1060, 304, 1690, 1274, 596, 6439, 11, 574, 264, 67479, 832, 369, 16526, 7829, 11, 449, 94137, 635, 11230, 279, 16192, 17768, 369, 7252, 25586, 323, 83168, 17437, 2380, 1396, 19101, 13280, 389, 279, 67293, 8121, 220, 1049, 8032, 14815, 2595, 2366, 16, 4235, 29844, 198, 42513, 466, 498, 330, 15315, 311, 30704, 1, 323, 38091, 271, 33, 10155, 16785, 330, 4071, 466, 1, 520, 279, 220, 2491, 339, 3778, 10948, 23488, 389, 6841, 220, 1691, 11, 220, 2366, 16, 58, 8273, 933, 1966, 5587, 220, 19, 11, 220, 2366, 16, 11, 279, 11812, 1932, 7086, 83168, 1202, 8121, 61647, 29459, 315, 279, 9941, 369, 220, 2366, 15, 11, 279, 1176, 14875, 323, 1176, 2536, 12, 23392, 12365, 1180, 311, 1948, 279, 23862, 8032, 13341, 60, 83168, 25366, 2380, 19300, 304, 279, 8121, 26749, 16207, 21964, 315, 220, 2366, 15, 11, 449, 5135, 315, 279, 30242, 25, 220, 22, 520, 1396, 832, 11, 2893, 320, 16939, 36422, 14398, 8, 520, 1396, 1403, 11, 323, 5135, 315, 279, 30242, 25, 220, 22, 4235, 791, 43680, 520, 1396, 8223, 8032, 12754, 60, 1952, 279, 13945, 11887, 8121, 26749, 2052, 15392, 21964, 11, 5135, 315, 279, 30242, 25, 220, 22, 11922, 1176, 2035, 323, 2893, 320, 16939, 36422, 14398, 8, 11922, 11999, 8032, 14052, 60, 1952, 5587, 220, 975, 11, 220, 2366, 16, 11, 83168, 10887, 330, 35, 11694, 635, 1, 520, 279, 220, 5495, 6634, 25992, 74679, 23488, 11, 10671, 279, 1176, 16526, 29311, 311, 2804, 11, 3582, 814, 1550, 539, 3243, 279, 10292, 8032, 13719, 60, 1952, 5936, 220, 16, 11, 83168, 6004, 330, 52587, 4470, 498, 279, 1176, 3254, 505, 872, 1243, 5352, 5065, 11002, 29772, 8176, 11, 83168, 11, 279, 7252, 8032, 13078, 60, 83168, 5762, 264, 1403, 11477, 2930, 17265, 1567, 389, 872, 13674, 5613, 7314, 5936, 220, 1114, 11, 25891, 17343, 17343, 1221, 220, 1691, 11, 323, 43087, 2380, 315, 872, 3766, 304, 29145, 47679, 8032, 14205, 2595, 1966, 3297, 220, 1691, 11, 83168, 6004, 872, 2132, 6498, 44658, 3254, 11, 330, 4071, 466, 3343, 58, 14125, 60, 1102, 58185, 520, 1396, 832, 389, 279, 8166, 220, 1041, 22416, 404, 11999, 1396, 832, 304, 11888, 4038, 2345, 28936, 1124, 279, 81610, 1180, 311, 11322, 3116, 9676, 2442, 32542, 2533, 23278, 45248, 63210, 304, 220, 1049, 21, 11, 323, 279, 26731, 1912, 2533, 279, 13972, 220, 20, 304, 220, 4468, 15, 8032, 14185, 60, 11205, 1828, 6498, 44658, 3254, 11, 330, 15315, 311, 30704, 498, 574, 6004, 389, 5887, 220, 24, 8032, 14735, 60, 1102, 6244, 83168, 596, 37477, 1396, 19101, 389, 279, 14434, 40200, 9676, 11, 33459, 872, 3335, 439, 279, 1912, 449, 279, 1455, 1396, 19101, 10925, 389, 279, 23862, 8032, 5154, 60, 1952, 6250, 220, 1187, 11, 220, 2366, 16, 11, 279, 7200, 6004, 279, 3254, 330, 5159, 29849, 1, 449, 24062, 1387, 8032, 13860, 60, 578, 3254, 58185, 520, 1396, 832, 389, 279, 8166, 220, 1041, 11, 3339, 433, 279, 1176, 20632, 1990, 1403, 5315, 311, 17755, 520, 1396, 832, 8032, 12326, 60, 578, 7200, 5762, 459, 2930, 21497, 11, 25891, 18628, 311, 30704, 389, 22891, 11, 389, 6664, 220, 1187, 11, 220, 2366, 16, 11, 304, 51289, 8032, 14022, 60, 1952, 6841, 220, 1419, 11, 330, 4071, 466, 1, 15662, 264, 74679, 29804, 369, 7252, 10466, 74312, 14, 2878, 21304, 520, 279, 220, 1227, 339, 25992, 74679, 23488, 8032, 12375, 1483, 3192, 60, 28232, 6841, 220, 1544, 323, 6790, 220, 17, 11, 83168, 5762, 872, 1176, 3974, 24601, 1603, 459, 304, 29145, 10877, 2533, 1603, 279, 28522, 13, 578, 7200, 6476, 3116, 6216, 9994, 5039, 520, 2100, 23550, 23462, 304, 9853, 12167, 439, 264, 42271, 315, 872, 18628, 311, 30704, 389, 22891, 21497, 4101, 8032, 4146, 1483, 15574, 1483, 15966, 2595, 1966, 6186, 220, 868, 11, 220, 2366, 17, 11, 264, 44682, 3566, 998, 263, 3196, 389, 83168, 11, 25891, 220, 22, 37, 988, 25, 921, 587, 6292, 11, 574, 6004, 13, 578, 20303, 68328, 220, 868, 3610, 6325, 31550, 304, 1202, 1176, 1403, 2919, 315, 18539, 323, 6244, 279, 8592, 23318, 291, 2316, 3596, 11887, 555, 5000, 998, 263, 8032, 15537, 1483, 11387, 60, 578, 7200, 5762, 2380, 7347, 37789, 4107, 47679, 520, 51289, 25944, 23462, 389, 5587, 220, 605, 11, 220, 717, 323, 220, 1032, 22416, 7928, 4731, 66237, 15848, 60, 12054, 555, 279, 4987, 16526, 3109, 2533, 279, 28522, 17294, 1051, 27070, 81902, 264, 2860, 10877, 315, 220, 1774, 11, 931, 1274, 8032, 15602, 1483, 14274, 60, 1952, 5936, 220, 18, 11, 83168, 10887, 330, 4071, 466, 1, 520, 279, 220, 1227, 339, 25992, 74679, 23488, 17706, 3192, 60, 3582, 279, 5609, 1550, 539, 3243, 279, 10292, 369, 902, 433, 574, 39048, 8032, 15666, 60, 1952, 5936, 220, 23, 11, 279, 7200, 15662, 8254, 60698, 520, 279, 220, 2366, 17, 67293, 10948, 23488, 58, 12815, 60, 323, 2834, 2380, 11, 3339, 1124, 279, 1455, 5392, 50615, 323, 279, 1455, 12, 675, 21632, 1912, 304, 279, 1501, 596, 3925, 8032, 14374, 1483, 15999, 2595, 1966, 5651, 220, 605, 11, 220, 2366, 17, 11, 83168, 6004, 872, 2380, 12, 6620, 84108, 8176, 38091, 8032, 16567, 1483, 16332, 1483, 16955, 60, 1952, 5651, 220, 975, 11, 2391, 872, 43641, 22310, 47674, 11, 279, 7200, 7376, 264, 13643, 25288, 315, 1912, 7640, 311, 5357, 389, 13839, 7224, 323, 1023, 79645, 8032, 10914, 1483, 15828, 60, 10320, 1395, 13332, 11, 902, 25241, 6295, 16261, 17706, 15741, 60, 65876, 304, 17876, 12518, 430, 83168, 574, 14188, 834, 7198, 287, 6463, 2133, 389, 81286, 11, 719, 1053, 387, 22815, 4726, 287, 872, 3927, 31133, 449, 279, 2440, 596, 2539, 1862, 1418, 2103, 24435, 304, 1912, 7640, 11, 2737, 279, 39970, 315, 6588, 83168, 8032, 15451, 1483, 16590, 60, 578, 10672, 9057, 10320, 1395, 13332, 596, 5708, 311, 18174, 19019, 11, 13239, 304, 264, 18979, 304, 3157, 907, 315, 2326, 3, 16, 13, 22, 7239, 8032, 14417, 60, 1952, 6287, 220, 1187, 11, 67293, 14756, 5068, 430, 83168, 1053, 387, 16785, 304, 19111, 276, 389, 6664, 220, 868, 304, 264, 8935, 21497, 304, 1862, 315, 279, 3363, 596, 9045, 311, 617, 264, 4435, 1398, 3571, 304, 220, 9639, 15, 11, 24435, 1234, 279, 24206, 14968, 311, 15936, 8032, 16660, 1483, 16367, 2595, 86426, 2532, 323, 5226, 43901, 198, 644, 34781, 315, 279, 17480, 315, 279, 6664, 220, 2366, 17, 8935, 21497, 11, 10320, 1395, 13332, 596, 5708, 7729, 12504, 311, 3770, 1202, 4113, 67992, 23442, 14691, 3157, 33422, 922, 279, 25127, 315, 279, 14827, 23911, 6411, 88724, 479, 315, 279, 7200, 596, 3697, 8032, 16949, 60, 9636, 4987, 16526, 7016, 11, 682, 3025, 97397, 25000, 2011, 4686, 1990, 220, 972, 323, 220, 1691, 4038, 315, 6411, 2532, 11, 6118, 555, 4325, 220, 1591, 8032, 16660, 60, 37653, 5513, 5068, 279, 21497, 439, 264, 2450, 719, 1101, 16717, 430, 1070, 1051, 912, 4726, 21497, 13003, 13847, 13, 1102, 574, 13240, 430, 422, 279, 7200, 3697, 8308, 872, 2532, 11, 10320, 1395, 13332, 1053, 9229, 7154, 2326, 3, 605, 7239, 927, 5899, 1667, 11, 449, 279, 4814, 311, 279, 4987, 16526, 8752, 520, 7154, 2326, 3, 2137, 7239, 8032, 17267, 2595, 644, 6664, 220, 2366, 17, 11, 6295, 16261, 11007, 430, 39611, 11, 279, 7200, 596, 24417, 4562, 11, 20330, 220, 1682, 11, 1047, 50682, 813, 88724, 479, 711, 29278, 1715, 8032, 11209, 60, 578, 1023, 3697, 13205, 311, 88724, 3010, 11, 449, 279, 1912, 9293, 311, 35740, 635, 304, 220, 2366, 20, 2768, 32643, 8032, 15282, 1483, 16544, 60, 2468, 279, 842, 315, 6664, 220, 2366, 17, 11, 83168, 15662, 4330, 60698, 369, 279, 220, 2366, 17, 386, 39200, 23488, 11, 449, 279, 7200, 3697, 12588, 8223, 4726, 60698, 439, 13839, 13820, 8032, 16085, 60, 1952, 6841, 220, 868, 11, 83168, 15662, 2380, 60698, 520, 279, 220, 2397, 339, 25992, 74679, 23488, 11, 2737, 264, 29804, 369, 7252, 10948, 8519, 369, 330, 29174, 311, 15936, 3343, 58, 17058, 60, 330, 5159, 29849, 1, 574, 39048, 369, 7252, 10466, 74312, 14, 2878, 21304, 11, 3339, 83168, 279, 1193, 1180, 311, 387, 39048, 2380, 1667, 304, 264, 2872, 304, 420, 5699, 2533, 1202, 17219, 304, 220, 679, 16, 8032, 17058, 60, 578, 7200, 574, 1101, 39048, 369, 26749, 315, 279, 9941, 439, 15109, 13820, 389, 24062, 1387, 596, 10948, 315, 279, 328, 65733, 8032, 17058, 60, 39611, 69576, 439, 459, 4642, 14523, 27202, 389, 6790, 220, 1032, 8032, 15935, 60, 1952, 7552, 220, 1627, 11, 220, 2366, 18, 11, 6295, 16261, 7376, 430, 622, 11529, 2862, 1047, 11472, 36935, 315, 279, 51101, 1133, 315, 813, 6411, 2532, 8032, 17361, 2595, 28055, 872, 13847, 25768, 11, 279, 7200, 596, 83168, 364, 29174, 311, 15936, 6, 304, 19111, 276, 21497, 4632, 574, 6004, 389, 7552, 220, 16, 11, 220, 2366, 18, 8032, 17897, 60, 12220, 264, 8577, 311, 18157, 11, 31915, 3309, 469, 11673, 430, 83168, 330, 14724, 2586, 3871, 1578, 994, 584, 6381, 1057, 6411, 2532, 11, 323, 584, 690, 1427, 369, 502, 80526, 552, 1990, 603, 311, 3810, 264, 2132, 10474, 61046, 15287, 60, 10320, 1395, 21892, 17343, 12095, 2902, 88, 3178, 11224, 389, 5587, 220, 868, 11, 220, 2366, 18, 11, 430, 872, 47637, 2643, 539, 12446, 304, 220, 2366, 20, 2533, 433, 574, 2653, 311, 2218, 264, 3230, 2457, 11, 323, 430, 814, 1047, 539, 14407, 872, 5226, 43901, 3686, 8032, 17212, 60, 622, 11529, 2862, 69576, 439, 459, 4642, 14523, 27202, 389, 5936, 220, 972, 8032, 13754, 60, 1952, 3297, 220, 717, 11, 83168, 6004, 264, 45999, 330, 791, 29935, 1, 369, 279, 4987, 16526, 11625, 4101, 46168, 919, 8032, 17335, 60, 2057, 83623, 872, 56766, 22310, 11, 279, 1912, 6004, 279, 5609, 330, 18293, 9220, 1, 389, 5651, 220, 24, 8032, 16443, 2595, 1966, 6250, 220, 508, 11, 220, 2366, 18, 11, 10320, 1395, 11007, 1555, 264, 3577, 4984, 430, 83168, 1053, 14195, 872, 14079, 17517, 13, 578, 3697, 690, 1879, 1521, 20038, 95659, 11, 13126, 872, 6411, 2532, 11, 1306, 264, 4580, 11175, 449, 6295, 16261, 10948, 11, 6573, 7922, 872, 15507, 311, 3938, 7224, 6041, 505, 220, 2366, 20, 60525, 13, 10320, 1395, 13605, 872, 50919, 369, 12899, 83168, 6, 1912, 79645, 323, 43347, 15375, 402, 4776, 1862, 311, 18885, 872, 15603, 10383, 11, 23391, 279, 1912, 596, 50106, 1524, 1306, 27666, 872, 6411, 2532, 8032, 17313, 1483, 17168, 60, 1952, 6250, 220, 1313, 11, 220, 2366, 18, 11, 328, 38060, 69576, 439, 264, 3674, 12128, 8032, 16780, 60, 31915, 323, 650, 69576, 389, 6790, 220, 806, 11, 220, 2366, 18, 11, 8272, 555, 11641, 258, 323, 50432, 74, 1982, 389, 6790, 220, 717, 8032, 17408, 60, 1952, 5651, 220, 717, 11, 220, 2366, 19, 11, 39611, 6244, 279, 1176, 83168, 4562, 311, 4686, 813, 23911, 6411, 2532, 323, 574, 19073, 57191, 8032, 18163, 2595, 9470, 5050, 198, 644, 27256, 2436, 198, 13242, 25, 220, 1544, 6622, 13, 15, 25, 1544, 3214, 35623, 2561, 732, 34, 198, 33, 10155, 4562, 39611, 320, 3133, 8, 16785, 68362, 44662, 596, 330, 352, 12, 2319, 1, 44453, 2391, 872, 1176, 21497, 520, 468, 92729, 88, 23462, 389, 5651, 220, 16, 11, 220, 679, 24, 198, 33, 10155, 617, 22628, 1369, 78, 22448, 7910, 323, 30857, 17706, 17690, 60, 39322, 11, 81500, 336, 11, 59817, 4410, 11, 42865, 11, 3962, 84171, 11, 25972, 393, 952, 11, 323, 48399, 439, 18273, 12979, 811, 8032, 15531, 60, 2435, 617, 1101, 22628, 16657, 439, 459, 10383, 11, 5605, 814, 330, 70, 4361, 709, 10307, 6946, 315, 11406, 38505, 3343, 58, 3101, 60, 12220, 872, 21497, 520, 468, 92729, 88, 23462, 304, 7295, 11, 39611, 7318, 35491, 311, 16657, 555, 6522, 279, 13734, 304, 264, 2373, 315, 68362, 44662, 596, 330, 352, 12, 2319, 1, 44453, 8032, 3101, 60, 330, 91638, 26634, 2405, 458, 1, 902, 574, 6004, 994, 83168, 596, 18638, 49819, 7434, 574, 520, 1202, 2673, 11, 21935, 68089, 311, 279, 13820, 889, 28160, 1124, 11, 2737, 279, 4987, 16526, 1912, 11266, 1609, 5234, 11, 19455, 11419, 11, 6295, 22235, 11, 7121, 51825, 11, 323, 3885, 8032, 12405, 2595, 35897, 220, 679, 21, 8176, 46197, 574, 14948, 555, 32565, 1036, 473, 24201, 596, 5108, 315, 4325, 11775, 11, 4829, 1122, 8032, 13121, 60, 11205, 5609, 330, 52586, 87060, 612, 92014, 1, 17637, 80474, 94206, 596, 14636, 3165, 4845, 62531, 589, 3497, 64, 11, 323, 1202, 4731, 2835, 4519, 9302, 15407, 311, 58463, 7957, 2999, 3271, 596, 578, 445, 2908, 369, 358, 7063, 355, 11, 21286, 1430, 3320, 361, 29952, 596, 63641, 449, 279, 15128, 315, 358, 7063, 355, 11, 323, 3320, 361, 29952, 596, 578, 15128, 315, 279, 64264, 43145, 8032, 13236, 60, 22395, 279, 32465, 323, 1023, 8336, 430, 617, 14948, 872, 4375, 527, 1884, 555, 5340, 22227, 15356, 587, 10830, 11, 71138, 5724, 735, 13, 2009, 4673, 258, 11, 22770, 50432, 11, 10058, 83853, 323, 94206, 8032, 12166, 60, 578, 10919, 60005, 4101, 574, 28160, 555, 9939, 718, 5659, 76, 596, 578, 5277, 315, 90422, 17706, 15741, 1483, 13364, 60, 323, 872, 220, 679, 23, 5609, 330, 44638, 14355, 1, 505, 10919, 60005, 25, 78082, 574, 14948, 555, 7957, 432, 13, 32362, 88, 596, 51342, 32745, 279, 15852, 14355, 8032, 12879, 2595, 62698, 950, 1742, 198, 12834, 872, 54529, 11, 83168, 617, 46728, 18638, 7598, 439, 872, 18273, 2385, 11, 14090, 4245, 311, 279, 10383, 315, 31915, 323, 328, 38060, 596, 4092, 439, 26326, 436, 28921, 26, 58, 14777, 60, 2391, 4216, 21728, 311, 279, 2326, 11, 279, 1912, 4036, 76579, 505, 3778, 436, 28921, 8032, 14498, 60, 17343, 12095, 2902, 88, 3178, 8767, 26579, 430, 735, 41352, 439, 264, 4459, 27741, 505, 3776, 4731, 17706, 15500, 60, 323, 3229, 29016, 328, 13, 21293, 11224, 11, 330, 33, 10155, 596, 16448, 23354, 304, 279, 2326, 11105, 279, 42271, 315, 279, 5627, 430, 735, 41352, 5865, 439, 961, 315, 264, 3728, 432, 49339, 14135, 61046, 12226, 60, 350, 11606, 13, 5657, 323, 13566, 90284, 13818, 315, 650, 10745, 25660, 279, 3839, 330, 2729, 299, 25, 6385, 1, 505, 10919, 60005, 25, 6385, 439, 279, 1888, 3187, 315, 279, 1912, 596, 8830, 315, 2362, 35789, 18638, 7598, 11, 449, 436, 2690, 14948, 555, 34349, 423, 323, 97197, 582, 323, 110292, 4341, 56759, 505, 279, 220, 2550, 15, 82, 311, 1893, 264, 11670, 18638, 7598, 5222, 8032, 15134, 22414, 74627, 292, 16110, 1, 320, 38595, 362, 39291, 57466, 8, 320, 679, 22, 340, 13242, 25, 220, 1187, 6622, 13, 15, 25, 1187, 198, 791, 57466, 2373, 315, 330, 98402, 16110, 1, 4519, 99117, 43998, 323, 23709, 9567, 927, 18638, 7598, 62684, 8032, 13384, 933, 1, 63504, 3161, 445, 12328, 1, 320, 679, 24, 340, 13242, 25, 220, 966, 6622, 13, 15, 25, 966, 198, 35897, 3254, 330, 63504, 3161, 445, 12328, 1, 374, 7633, 439, 459, 85773, 25396, 41352, 5609, 449, 86605, 30777, 8032, 15231, 933, 1360, 22442, 5737, 1521, 3626, 30, 3580, 3772, 1520, 627, 791, 4984, 315, 330, 52586, 87060, 612, 92014, 1, 304, 220, 679, 21, 49858, 83168, 596, 9320, 505, 264, 18638, 7598, 311, 264, 2477, 1912, 13, 5657, 323, 13818, 10555, 430, 279, 5609, 27741, 505, 15612, 43341, 11, 1239, 6885, 35794, 11, 323, 4647, 78030, 65812, 719, 12462, 369, 264, 330, 2308, 61652, 11679, 42914, 1, 4856, 1109, 279, 330, 34057, 4908, 16975, 315, 1202, 34453, 3343, 58, 15134, 60, 578, 1912, 1101, 6137, 52913, 8776, 16526, 5540, 1139, 872, 4731, 13, 1789, 3187, 11, 872, 3254, 330, 769, 337, 1, 320, 679, 23, 8, 4519, 459, 1008, 2808, 505, 393, 598, 13915, 11, 264, 16526, 1376, 315, 2040, 780, 48929, 11, 323, 26480, 737, 31767, 315, 279, 10578, 315, 16526, 503, 526, 8890, 47389, 8032, 16104, 2595, 8142, 83168, 33095, 20282, 304, 18638, 7598, 11, 872, 5222, 706, 85957, 13, 2435, 1176, 98504, 449, 432, 49339, 11, 7091, 323, 34997, 18638, 7598, 389, 12538, 612, 13944, 304, 220, 679, 19, 26, 58, 15189, 1483, 15340, 60, 99117, 304, 872, 578, 7648, 20055, 40096, 304, 9601, 8176, 4101, 26, 58, 4370, 60, 4647, 78030, 65812, 323, 35148, 3838, 389, 46197, 323, 1472, 15037, 12839, 70408, 26, 58, 16718, 1483, 17592, 1483, 6028, 60, 3938, 22253, 323, 20023, 2477, 304, 872, 10919, 60005, 8176, 4101, 26, 58, 4218, 60, 6435, 1773, 685, 5041, 7819, 17706, 16874, 60, 94197, 7477, 11, 94700, 2477, 17706, 9588, 60, 69392, 11, 23709, 11, 2477, 7091, 11, 323, 18638, 2477, 304, 872, 5135, 315, 279, 30242, 8176, 4101, 26, 58, 14423, 60, 323, 54969, 304, 872, 3254, 330, 35, 11694, 635, 3343, 58, 15805, 1483, 15726, 60, 578, 7200, 3697, 617, 36131, 2204, 36744, 389, 13839, 14242, 11, 1778, 439, 36182, 13836, 389, 650, 596, 330, 626, 11750, 1, 323, 36612, 432, 49339, 389, 11641, 258, 596, 330, 77591, 3343, 58, 16718, 2595, 43, 11160, 950, 22100, 198, 12834, 872, 18488, 11, 83168, 617, 11846, 430, 11890, 872, 1866, 7493, 374, 279, 1888, 1648, 369, 279, 14992, 9659, 311, 29243, 311, 872, 4731, 8032, 16723, 60, 24838, 1690, 315, 872, 1866, 24142, 17706, 15257, 1483, 17470, 60, 279, 1912, 35434, 20789, 2324, 11704, 1778, 439, 51978, 323, 75040, 304, 872, 990, 323, 2543, 1124, 1139, 2555, 30673, 323, 810, 71128, 13, 31915, 11224, 430, 83168, 16696, 311, 5766, 264, 69423, 477, 312, 18375, 26673, 16630, 304, 872, 11936, 330, 28753, 430, 596, 539, 279, 1648, 430, 584, 1390, 311, 9041, 1057, 1984, 2564, 1226, 2351, 9405, 449, 2204, 6439, 11, 719, 499, 4250, 5268, 1063, 2574, 13, 2100, 584, 3463, 430, 3021, 11, 279, 1972, 7438, 315, 433, 11, 8638, 449, 21955, 13520, 323, 25694, 1063, 11245, 552, 323, 1063, 52087, 552, 430, 584, 617, 505, 279, 1633, 1212, 61046, 13817, 60, 3277, 4691, 422, 433, 374, 5107, 311, 3350, 922, 2574, 1093, 10723, 2890, 11, 328, 38060, 16846, 3638, 1687, 2733, 430, 1274, 889, 617, 279, 5452, 311, 3137, 922, 1884, 2574, 2216, 1288, 3137, 810, 11, 1606, 814, 2019, 18710, 374, 2555, 1405, 499, 733, 311, 279, 8952, 323, 499, 2351, 29704, 11, 719, 499, 649, 956, 2216, 1440, 3156, 279, 10896, 13739, 311, 499, 2564, 4497, 323, 810, 11, 358, 1781, 13820, 477, 40501, 889, 617, 264, 7899, 1288, 3137, 922, 1521, 5435, 323, 4546, 433, 709, 311, 279, 7479, 8032, 13817, 2595, 90130, 36131, 304, 83168, 596, 2624, 5814, 2134, 505, 24919, 330, 1820, 35665, 323, 74884, 32606, 315, 2978, 44041, 12822, 1, 311, 330, 41815, 1093, 3021, 11, 27607, 11, 4814, 11, 4648, 11, 323, 810, 61046, 16884, 60, 23591, 83168, 11936, 11, 1778, 439, 330, 2822, 4497, 18308, 1, 323, 330, 45, 8548, 1, 505, 872, 2978, 57886, 11, 1051, 7633, 555, 350, 15924, 61338, 439, 27762, 555, 4443, 11704, 449, 4987, 12126, 596, 33956, 5603, 311, 6873, 323, 2663, 369, 2349, 311, 279, 16627, 1887, 323, 59529, 17078, 8032, 18196, 60, 578, 3697, 6, 11704, 449, 4987, 16526, 12822, 7829, 1101, 14948, 279, 11936, 330, 35, 2862, 1, 323, 330, 53443, 94613, 1, 320, 42, 46295, 25, 5251, 109, 223, 108307, 26, 44498, 25, 14659, 7270, 6043, 8, 505, 872, 12822, 57886, 13, 4314, 11936, 5905, 1803, 1697, 66949, 323, 70433, 7231, 709, 24364, 12135, 11, 11103, 11, 2911, 11, 6300, 14740, 11, 10632, 11, 323, 3674, 2324, 304, 279, 3663, 315, 7100, 27129, 323, 59529, 220, 3385, 1418, 13176, 72106, 505, 279, 3772, 323, 9191, 22540, 8032, 18196, 60, 578, 1912, 596, 2440, 42160, 578, 7648, 20055, 40096, 304, 9601, 25, 13566, 55706, 11, 279, 17102, 311, 872, 12822, 57886, 11, 330, 64, 3361, 8176, 430, 15785, 279, 17102, 315, 279, 25706, 11879, 315, 279, 4101, 11, 8649, 279, 1566, 7493, 3309, 555, 3995, 1274, 889, 11, 8994, 459, 36218, 323, 62945, 8903, 320, 791, 7648, 20055, 40096, 304, 9601, 52170, 13, 220, 16, 8, 3136, 311, 22531, 4741, 320, 791, 7648, 20055, 40096, 304, 9601, 52170, 13, 220, 17, 570, 37964, 10568, 60, 46197, 10968, 389, 10723, 2890, 11, 63836, 315, 279, 735, 41352, 330, 307, 337, 1, 6237, 11, 323, 24944, 264, 8954, 74477, 1984, 8032, 16707, 60, 578, 10919, 60005, 4101, 11784, 502, 22100, 9002, 12822, 7829, 304, 4987, 12126, 11, 2737, 279, 28361, 315, 3021, 11, 6784, 315, 72643, 11, 323, 81869, 315, 659, 27578, 588, 8032, 17079, 60, 10771, 311, 65298, 15883, 82276, 11, 83168, 596, 220, 2366, 15, 330, 447, 277, 39457, 8176, 1, 2893, 330, 7562, 4440, 279, 1912, 596, 5108, 311, 3878, 449, 264, 15187, 502, 8903, 323, 6209, 1862, 369, 872, 24475, 2133, 1555, 279, 1890, 96710, 22510, 323, 27924, 3343, 58, 8765, 2595, 33, 10155, 596, 24142, 617, 1101, 20669, 13650, 4994, 12822, 7829, 13, 578, 5609, 330, 6219, 358, 41856, 1, 505, 46197, 29440, 59529, 1469, 19682, 7119, 10223, 279, 2704, 41608, 26, 279, 69435, 330, 1687, 2351, 682, 12875, 323, 49910, 611, 584, 3719, 12875, 1606, 584, 2351, 19021, 1, 9922, 311, 5905, 4987, 16526, 20214, 315, 11930, 4033, 13106, 10320, 526, 12, 1146, 564, 11, 889, 64854, 264, 69524, 1887, 369, 279, 3224, 323, 889, 18307, 7633, 5578, 1274, 439, 330, 81134, 323, 49910, 3343, 83168, 6004, 279, 5609, 23442, 279, 220, 679, 21, 4987, 16526, 5054, 26681, 430, 19543, 304, 279, 64754, 315, 4872, 5657, 4323, 359, 2902, 9188, 8032, 18196, 60, 31915, 323, 328, 38060, 596, 4443, 28970, 449, 10723, 2890, 617, 14948, 1063, 315, 872, 4731, 8032, 13817, 1483, 15134, 60, 330, 2688, 11450, 1, 505, 220, 679, 22, 596, 1472, 15037, 12839, 70408, 374, 459, 7294, 48587, 479, 56664, 11, 43676, 330, 543, 279, 1234, 81134, 304, 279, 1917, 1, 311, 2567, 11039, 17706, 17153, 60, 323, 330, 26208, 6187, 1, 39377, 279, 12697, 315, 279, 50313, 337, 79549, 31926, 8032, 16596, 60, 10139, 380, 12149, 30411, 37475, 83168, 304, 74615, 369, 330, 82, 23635, 58, 287, 60, 27136, 922, 13650, 814, 82577, 3062, 11, 1524, 304, 264, 15692, 8396, 3343, 58, 17014, 60, 33600, 4987, 16526, 4872, 17781, 96660, 3502, 1071, 25, 330, 4959, 315, 279, 8254, 3697, 68341, 304, 264, 1648, 430, 374, 837, 311, 5678, 323, 279, 2324, 568, 6944, 311, 3974, 13, 11205, 62684, 323, 24142, 74809, 15481, 24743, 11, 4221, 11, 7829, 11, 323, 14673, 61046, 17609, 2595, 72603, 198, 6334, 4652, 25, 41333, 5536, 315, 83168, 198, 2520, 27927, 10401, 315, 420, 8712, 11, 1518, 16526, 32418, 382, 33, 10155, 16785, 520, 279, 12126, 7424, 35206, 73169, 47784, 304, 12366, 389, 6664, 220, 975, 11, 220, 679, 23, 198, 1966, 5936, 220, 1682, 11, 220, 679, 24, 11, 4212, 14756, 7086, 83168, 832, 315, 279, 220, 1041, 1455, 32549, 1274, 315, 279, 1060, 11, 55402, 1124, 279, 330, 3617, 31176, 315, 10466, 3343, 58, 18633, 60, 67293, 11145, 8211, 73822, 86288, 1098, 84, 6497, 7863, 279, 1912, 596, 10383, 311, 430, 315, 279, 55957, 8032, 17887, 60, 386, 7532, 2956, 11145, 73046, 38208, 53977, 10555, 430, 330, 37241, 83168, 15058, 956, 279, 1176, 311, 1825, 279, 14365, 311, 735, 9483, 454, 15603, 11, 814, 1051, 279, 1176, 311, 3719, 21391, 13, 2435, 1541, 956, 1120, 14638, 311, 3995, 1274, 719, 1101, 311, 279, 220, 1135, 82, 323, 220, 1399, 82, 4325, 38462, 61046, 13679, 60, 578, 1176, 2536, 12, 23392, 12365, 10255, 311, 1304, 279, 8121, 29459, 21964, 304, 220, 679, 23, 11, 83168, 574, 279, 2132, 1888, 48724, 13820, 15603, 4028, 5361, 3772, 15771, 11, 2132, 1193, 311, 42865, 8032, 16546, 1483, 17590, 1483, 16522, 60, 763, 220, 2366, 15, 11, 83168, 6244, 279, 1176, 2536, 38702, 944, 323, 2536, 12, 23392, 12365, 10255, 311, 387, 7086, 11812, 1932, 596, 8121, 61647, 29459, 315, 279, 9941, 8032, 13341, 60, 763, 4987, 12126, 11, 83168, 41853, 369, 220, 3174, 13, 24, 3346, 315, 8176, 6763, 304, 279, 1176, 4376, 315, 220, 679, 24, 11, 709, 505, 872, 3157, 4430, 315, 220, 914, 13, 18, 3346, 279, 3766, 1060, 8032, 17451, 2595, 644, 220, 2366, 17, 11, 1472, 3458, 13818, 7633, 83168, 439, 3515, 41963, 63600, 279, 16526, 32418, 11, 14393, 279, 3728, 14800, 315, 16526, 7829, 439, 13750, 439, 41118, 1550, 304, 279, 3766, 13515, 323, 449, 279, 8333, 315, 10383, 430, 279, 16192, 17768, 27875, 4987, 16526, 4632, 94137, 635, 1047, 304, 220, 2366, 15, 8032, 12901, 60, 4987, 12126, 596, 8792, 6201, 11, 279, 8715, 315, 12126, 11, 1766, 304, 220, 2366, 16, 430, 83168, 11, 2737, 264, 330, 83473, 2515, 1, 430, 5343, 7319, 32083, 311, 4987, 12126, 26, 7319, 2802, 304, 16526, 7829, 11, 9698, 11, 323, 4007, 315, 279, 16526, 4221, 26, 323, 3779, 13489, 2326, 3, 20, 7239, 824, 1060, 311, 4987, 12126, 596, 8752, 11, 264, 6650, 315, 922, 220, 15, 13, 20, 3346, 8032, 18061, 60, 362, 220, 679, 23, 4007, 8710, 430, 11, 389, 5578, 11, 220, 4728, 11, 931, 48512, 824, 1060, 1047, 12263, 4987, 12126, 927, 279, 3347, 3116, 1667, 369, 83168, 14228, 8125, 8032, 17678, 2595, 54, 32929, 11054, 83168, 439, 6164, 1524, 4315, 1023, 7701, 32549, 735, 41352, 5315, 1778, 439, 20666, 6, 24367, 11, 7445, 31870, 11, 1398, 78, 11, 94666, 11, 323, 5348, 64349, 58, 19746, 60, 323, 5296, 430, 83168, 596, 2450, 5039, 279, 12939, 315, 264, 3831, 11, 4642, 8571, 2385, 304, 279, 4325, 315, 3674, 3772, 11, 1405, 8571, 55927, 649, 387, 439, 3062, 439, 18273, 4367, 311, 264, 5609, 596, 2450, 8032, 18634, 60, 578, 1912, 706, 1101, 39575, 5694, 520, 279, 52301, 315, 279, 2626, 3185, 315, 279, 735, 41352, 5064, 555, 34118, 2753, 58096, 17517, 449, 872, 6373, 2883, 311, 35608, 872, 32692, 4113, 488, 323, 28697, 8032, 8652, 1483, 18113, 60, 3161, 420, 26627, 5603, 311, 7076, 6373, 11, 83168, 3549, 12401, 20405, 311, 279, 4987, 16526, 12822, 323, 15253, 3927, 488, 323, 54348, 4315, 872, 10877, 8032, 10465, 2595, 35, 10567, 316, 2826, 198, 644, 813, 220, 2550, 15, 9071, 25046, 7572, 11, 5054, 28568, 15466, 452, 9188, 8040, 279, 7434, 315, 264, 2132, 955, 315, 2410, 2204, 505, 8776, 59021, 2191, 8032, 16482, 60, 452, 9188, 6267, 11, 330, 2028, 2132, 13189, 315, 2410, 1389, 902, 13980, 994, 832, 3224, 5334, 1023, 5961, 311, 1390, 1148, 433, 6944, 1389, 2643, 387, 2663, 1080, 57212, 535, 477, 8579, 2410, 304, 13168, 449, 279, 2653, 477, 3290, 2410, 315, 22106, 3885, 311, 656, 1148, 433, 6944, 61046, 17228, 60, 59250, 1778, 439, 386, 8039, 3489, 434, 569, 617, 9435, 279, 7434, 315, 8579, 2410, 311, 83168, 323, 872, 10383, 389, 16924, 62340, 323, 6625, 4398, 8032, 18384, 60, 1472, 3458, 13818, 323, 386, 8039, 3489, 434, 569, 682, 1373, 279, 11667, 315, 8579, 2410, 439, 2737, 7829, 11, 5054, 2819, 11, 323, 7362, 4947, 11, 902, 17208, 311, 83168, 596, 5845, 311, 387, 1080, 57212, 535, 304, 872, 5603, 311, 31135, 872, 1984, 315, 26348, 11, 26586, 11, 323, 28118, 2324, 596, 97737, 4669, 872, 7353, 14638, 389, 279, 6625, 6566, 8032, 18384, 22414, 33, 10155, 323, 4900, 13142, 38180, 520, 279, 5929, 4783, 389, 3297, 220, 2148, 11, 220, 2366, 17, 627, 33, 10155, 574, 18719, 311, 2686, 279, 3723, 19687, 3331, 12000, 304, 1561, 4356, 304, 6250, 220, 679, 23, 8032, 17306, 1483, 18349, 60, 3011, 1890, 1060, 814, 10887, 304, 12366, 1603, 459, 10877, 315, 220, 3443, 11, 2737, 4900, 17781, 96660, 3502, 323, 1023, 7510, 11, 520, 279, 220, 679, 23, 12126, 7424, 35206, 73169, 47784, 11, 264, 30048, 32689, 279, 11919, 4398, 1990, 9822, 323, 4987, 12126, 8032, 18520, 60, 3011, 1060, 11, 83168, 6244, 279, 39637, 34876, 315, 279, 7365, 315, 41333, 8930, 275, 13, 18185, 13042, 60082, 36342, 1694, 2728, 311, 34876, 449, 927, 220, 868, 1667, 315, 26501, 11, 17781, 15324, 279, 1912, 11, 4330, 1667, 1139, 872, 7076, 11, 369, 872, 19564, 304, 31135, 16526, 7829, 323, 4221, 15603, 8032, 17112, 1483, 19192, 60, 763, 6250, 220, 679, 24, 11, 83168, 1051, 9932, 555, 17781, 1418, 38787, 15174, 369, 279, 2262, 19647, 11, 369, 3515, 96734, 18699, 2626, 4211, 1555, 2167, 10758, 449, 7359, 8032, 6843, 60, 763, 220, 2366, 15, 11, 83168, 4036, 279, 7957, 362, 13, 13000, 44555, 17768, 304, 18324, 315, 872, 19310, 19564, 311, 279, 20862, 315, 2326, 16222, 61148, 4398, 11, 279, 39637, 5954, 461, 288, 311, 5371, 279, 10292, 8032, 18277, 1483, 18509, 60, 763, 5887, 220, 2366, 16, 11, 814, 1051, 21489, 9984, 42855, 38139, 2303, 369, 12781, 2672, 811, 323, 21023, 555, 4900, 17781, 13, 763, 872, 3560, 439, 665, 3415, 1065, 11, 814, 1520, 311, 330, 19223, 17985, 389, 3728, 84029, 11, 1778, 439, 22556, 4500, 11, 311, 1057, 3938, 22540, 323, 311, 20259, 279, 7140, 596, 34616, 2410, 4028, 279, 1917, 37964, 18199, 60, 323, 5101, 520, 6625, 4455, 1778, 439, 279, 220, 4767, 339, 3723, 19687, 3331, 12000, 8032, 15951, 60, 1952, 3297, 220, 2148, 11, 220, 2366, 17, 11, 83168, 12263, 2326, 4900, 13142, 38180, 520, 279, 5929, 4783, 311, 4358, 279, 3293, 10205, 304, 7294, 12, 68540, 12491, 17073, 323, 21934, 8032, 12676, 2595, 37, 2255, 198, 11439, 311, 23727, 2234, 10320, 359, 13818, 11, 83168, 596, 10205, 574, 72849, 555, 264, 2294, 5376, 304, 4731, 2835, 15840, 323, 15652, 389, 13674, 323, 279, 5108, 315, 459, 49014, 32447, 11, 2737, 22480, 438, 3876, 315, 2536, 38827, 950, 3956, 11, 3953, 11, 323, 18884, 17422, 11, 439, 1664, 439, 459, 14800, 315, 2930, 4731, 75425, 8032, 18044, 60, 578, 1912, 706, 264, 3544, 11, 7701, 17057, 11, 2930, 4029, 315, 7359, 3967, 439, 6395, 19708, 320, 2654, 25745, 38366, 386, 732, 13, 369, 30160, 705, 902, 48018, 279, 1912, 596, 24142, 323, 3674, 3772, 8158, 1139, 1023, 15823, 323, 9248, 48801, 19564, 315, 83168, 596, 3697, 13, 1666, 315, 220, 2366, 15, 11, 1063, 220, 1272, 3610, 6395, 19708, 3697, 18447, 311, 279, 7200, 596, 13674, 5613, 11, 449, 810, 1109, 220, 966, 3610, 2768, 279, 4033, 83168, 6405, 323, 14318, 9815, 8032, 5833, 60, 578, 8571, 4029, 8779, 7068, 83168, 596, 1396, 19101, 9676, 33407, 4669, 47672, 21343, 389, 17265, 15771, 17706, 18775, 60, 439, 1664, 439, 41802, 311, 4668, 83168, 596, 4731, 389, 9063, 17789, 323, 12707, 8032, 19057, 60, 4427, 6395, 19708, 3697, 1253, 1524, 53120, 279, 1912, 5196, 304, 10383, 4315, 83168, 7359, 8032, 19929, 22414, 76887, 520, 279, 83168, 1917, 7364, 21497, 364, 29351, 60005, 6, 304, 9853, 12167, 389, 6250, 220, 21, 11, 220, 679, 23, 627, 33, 10155, 16681, 291, 323, 17045, 449, 872, 20723, 505, 872, 30758, 2919, 4669, 3674, 3772, 17706, 14648, 60, 439, 1664, 439, 4669, 83168, 29849, 11, 459, 25631, 51728, 16239, 279, 3697, 3309, 1555, 4731, 6946, 11, 6505, 3953, 11, 6603, 11, 2875, 12631, 11, 323, 810, 430, 6835, 7359, 3130, 311, 46820, 553, 8032, 18650, 60, 13818, 12090, 430, 6395, 19708, 527, 15107, 311, 83168, 2533, 279, 3697, 527, 3970, 439, 1234, 81134, 11, 71373, 505, 279, 16526, 47967, 323, 264, 12309, 9099, 16526, 16924, 2883, 11, 902, 6276, 3995, 7359, 311, 10765, 449, 1124, 8032, 17662, 60, 83168, 596, 24142, 6604, 311, 3674, 2819, 11, 323, 7359, 6013, 555, 4560, 311, 7417, 279, 1917, 13, 1666, 264, 1121, 11, 279, 75425, 15870, 83487, 55280, 389, 48801, 11384, 323, 41589, 55096, 32360, 4819, 1778, 439, 34267, 58187, 11, 19739, 21934, 11, 2911, 596, 3268, 11, 10182, 2349, 11, 323, 279, 20562, 12, 777, 28522, 8032, 18017, 1483, 18265, 1483, 12935, 60, 37957, 505, 6395, 19708, 311, 83168, 22223, 279, 1912, 596, 6299, 323, 24142, 26, 83168, 706, 34373, 3738, 16526, 4339, 430, 5222, 1093, 3778, 19739, 1776, 1759, 505, 872, 11936, 323, 9670, 20632, 449, 264, 11002, 17276, 994, 16526, 6395, 19708, 3697, 25660, 813, 6325, 14560, 8032, 18322, 2595, 3976, 4987, 16526, 3229, 55551, 82279, 12336, 11, 279, 5133, 1990, 83168, 323, 6395, 19708, 374, 330, 64, 27848, 9473, 1990, 13820, 323, 872, 7359, 1, 430, 374, 922, 810, 1109, 16632, 330, 729, 1711, 279, 7200, 596, 9036, 2826, 498, 719, 1101, 330, 428, 2518, 279, 7200, 596, 1984, 315, 98042, 1139, 279, 1917, 3343, 12336, 1200, 1572, 430, 83168, 323, 6395, 19708, 527, 330, 64, 7891, 315, 2349, 304, 78258, 79500, 11, 539, 1120, 315, 1803, 1697, 2349, 3343, 58, 10898, 60, 578, 7200, 3697, 5694, 7655, 323, 617, 1317, 26579, 872, 7359, 6, 3560, 304, 872, 2450, 8032, 19166, 60, 10771, 311, 21077, 32602, 11, 330, 33, 10155, 81658, 264, 4545, 315, 1803, 1697, 18475, 13, 6395, 19708, 11105, 264, 364, 5065, 315, 4325, 6, 369, 279, 3995, 11, 304, 902, 13042, 5788, 323, 10383, 527, 3728, 323, 23222, 11, 323, 1405, 279, 12822, 527, 31205, 323, 40418, 17045, 61046, 19867, 2595, 11663, 66724, 198, 3812, 11073, 1392, 198, 6334, 4652, 25, 41333, 5536, 315, 83168, 17036, 4060, 11073, 1392, 198, 33, 10155, 53319, 449, 393, 13722, 7314, 304, 220, 679, 20, 11, 15453, 439, 393, 13722, 12126, 596, 6883, 96662, 1603, 24050, 311, 3728, 96662, 304, 220, 679, 23, 11, 323, 22923, 279, 57466, 315, 393, 13722, 596, 330, 38062, 258, 1, 323, 330, 59837, 3612, 1, 1584, 15603, 8032, 13897, 1483, 19162, 1483, 18781, 60, 763, 220, 679, 24, 11, 83168, 8667, 449, 435, 10746, 311, 19507, 1202, 10775, 2332, 686, 8032, 19230, 60, 83168, 706, 1101, 10434, 439, 3728, 6883, 96662, 369, 24294, 38784, 6, 36122, 17706, 12910, 60, 323, 60940, 37792, 6, 220, 679, 24, 43772, 39773, 279, 330, 20577, 285, 1037, 37964, 18695, 60, 323, 35784, 10633, 2849, 9249, 39773, 11, 279, 330, 45, 78309, 3343, 58, 16481, 60, 83168, 6244, 3728, 96662, 315, 279, 9249, 8761, 22019, 4101, 31922, 469, 311, 12192, 1268, 9249, 11731, 649, 1520, 12896, 10182, 2349, 8032, 20062, 60, 763, 220, 2366, 15, 11, 83168, 53319, 449, 18907, 38784, 17706, 19081, 60, 28965, 264, 7347, 83168, 57689, 2373, 315, 279, 20238, 328, 508, 10, 323, 20238, 426, 29088, 10, 8032, 20422, 60, 1666, 279, 1176, 8762, 2477, 1912, 3596, 311, 51696, 449, 423, 2521, 11, 83168, 10775, 291, 665, 41794, 505, 13818, 12201, 6, 5075, 7424, 543, 220, 679, 24, 4526, 520, 872, 21497, 520, 800, 1037, 409, 9822, 8032, 15515, 60, 578, 7200, 6244, 3728, 6883, 96662, 369, 12140, 650, 3159, 783, 304, 5936, 220, 2366, 16, 8032, 19631, 2595, 30690, 32329, 18237, 198, 6334, 4652, 25, 7302, 32329, 18237, 315, 83168, 198, 33, 10155, 527, 3967, 369, 872, 58738, 45036, 79645, 13, 26778, 3697, 315, 279, 7200, 617, 1027, 304, 55015, 1139, 41385, 25968, 19424, 11, 1778, 439, 279, 6781, 5604, 37, 16958, 1105, 10349, 323, 279, 7997, 49687, 10349, 11, 304, 68023, 315, 279, 1404, 323, 11900, 315, 872, 24910, 8032, 19695, 1483, 18252, 1483, 20077, 60, 2435, 617, 1101, 4036, 23146, 369, 872, 24910, 11, 449, 832, 4562, 12588, 264, 90500, 315, 279, 17979, 17768, 369, 24910, 311, 279, 19071, 17706, 19498, 60, 323, 83168, 439, 264, 4459, 12588, 264, 6781, 5604, 37, 31016, 556, 17768, 369, 872, 10919, 3092, 726, 4901, 13, 2435, 3629, 33009, 38171, 11, 449, 872, 36380, 425, 3010, 1694, 1903, 586, 555, 279, 11351, 814, 1862, 323, 279, 3772, 8032, 19695, 1483, 19615, 60, 578, 7200, 596, 9045, 617, 27762, 872, 7359, 311, 1101, 16988, 304, 5370, 48801, 323, 38748, 7640, 11, 323, 389, 13402, 1524, 2489, 872, 24910, 8032, 20698, 2595, 14945, 337, 3536, 198, 6334, 4652, 25, 1796, 315, 23146, 323, 60698, 4036, 555, 83168, 198, 33, 10155, 617, 4036, 12387, 23146, 6957, 872, 7076, 13, 2435, 617, 20503, 332, 3210, 2834, 279, 67293, 10948, 17768, 369, 7054, 9983, 29459, 2533, 220, 679, 22, 26, 527, 279, 1193, 735, 41352, 1912, 311, 3243, 7054, 74312, 14, 2878, 11, 520, 279, 220, 679, 24, 67293, 10948, 23488, 26, 58, 19838, 60, 323, 527, 279, 1455, 12, 675, 21632, 1912, 304, 18772, 4940, 3925, 439, 315, 220, 2366, 17, 11, 449, 220, 717, 15160, 8244, 8032, 18572, 60, 83168, 527, 1101, 279, 1193, 735, 41352, 1912, 311, 3243, 40822, 74312, 477, 5856, 1389, 10466, 19945, 1197, 323, 40822, 9983, 29459, 520, 279, 3778, 10948, 23488, 17706, 3443, 60, 323, 6244, 279, 1176, 14875, 1180, 304, 279, 1501, 596, 3925, 311, 3243, 29459, 315, 279, 9941, 304, 220, 2366, 16, 8032, 10841, 60, 2435, 527, 279, 1176, 16526, 2477, 1180, 311, 5371, 264, 74679, 17768, 29804, 17706, 16496, 60, 323, 279, 1176, 16526, 10255, 311, 387, 39048, 369, 264, 5567, 17768, 8032, 13074, 60, 3161, 220, 966, 23146, 8244, 11, 2737, 264, 3335, 3116, 24871, 15160, 369, 29459, 315, 279, 9941, 320, 39563, 705, 83168, 527, 279, 1455, 12, 675, 21632, 7362, 10255, 304, 279, 3925, 315, 279, 6457, 7573, 11997, 23488, 13, 2435, 617, 1101, 4036, 264, 2860, 315, 220, 1135, 54544, 520, 279, 386, 39200, 23488, 8032, 7507, 60, 128009, 128006, 78191, 128007, 271], text: \r\n\r\n```",
        "created_at": "2024-10-01T07:26:47Z"
      },
      {
        "author": "CatherineSue",
        "body": "Did a bit deep dive, when in streaming and for the first few chunks with empty generation, it executes this line: https://github.com/vllm-project/vllm/blob/main/vllm/sequence.py#L531. It returns the cached_token_ids, and with `num_new_tokens=0`, it just returns the prompt_token_ids. Not sure if this is the right behavior tho.",
        "created_at": "2024-10-01T08:09:39Z"
      },
      {
        "author": "CatherineSue",
        "body": "@njhill I tried to change https://github.com/vllm-project/vllm/blob/main/vllm/sequence.py#L531 to:\r\n```\r\n        if num_new_tokens == 0:\r\n            return []\r\n\r\n        return self.data._cached_all_token_ids[-num_new_tokens:]\r\n```\r\nIt fixed both logprobs and the completion_tokens usage info issue I mentioned.\r\n\r\nBut I don't think it is the correct fix. Since the logic there seems tricky. Kindly ask for your input. Thanks!",
        "created_at": "2024-10-02T03:14:43Z"
      },
      {
        "author": "njhill",
        "body": "@CatherineSue sorry for the delay, I will look at this today.",
        "created_at": "2024-10-02T17:23:13Z"
      },
      {
        "author": "njhill",
        "body": "> @njhill I tried to change https://github.com/vllm-project/vllm/blob/main/vllm/sequence.py#L531 to:\r\n> \r\n> ```\r\n>         if num_new_tokens == 0:\r\n>             return []\r\n> \r\n>         return self.data._cached_all_token_ids[-num_new_tokens:]\r\n> ```\r\n> \r\n> It fixed both logprobs and the completion_tokens usage info issue I mentioned.\r\n> \r\n> But I don't think it is the correct fix. Since the logic there seems tricky. Kindly ask for your input. Thanks!\r\n\r\n@CatherineSue actually this is the correct fix I think! This is why negative indexing is a bit precarious.\r\n\r\nWould you like to open another PR with this? It would be great to have a unit test for this case too.\r\n\r\nThanks for finding the bug.",
        "created_at": "2024-10-02T21:11:20Z"
      },
      {
        "author": "CatherineSue",
        "body": "@njhill thank you for verifying. Opened a PR. ",
        "created_at": "2024-10-03T02:51:50Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4019": {
    "issue_number": 4019,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4019",
    "title": "[Bug][ROCm]: Process killed during tensor-parallel inference",
    "body": "### Your current environment\n\nI'm using a docker container built from `Dockerfile.rocm` with MI250x GPUs.\r\n\r\n```text\r\nPyTorch version: 2.1.1+git011de5c\r\nIs debug build: False\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: 6.0.32830-d62f6a171\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 17.0.0 (https://github.com/RadeonOpenCompute/llvm-project roc-6.0.0 23483 7208e8d15fbf218deb74483ea8c549c67ca4985e)\r\nCMake version: version 3.29.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.18 (main, Sep 11 2023, 13:41:44)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.19.0-45-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 10.1.243\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: AMD Instinct MI250X/MI250NoGCNArchNameOnOldPyTorch\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: 6.0.32830\r\nMIOpen runtime version: 3.0.0\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   48 bits physical, 48 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              1\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      25\r\nModel:                           1\r\nModel name:                      AMD EPYC 7713 64-Core Processor\r\nStepping:                        1\r\nFrequency boost:                 enabled\r\nCPU MHz:                         1500.000\r\nCPU max MHz:                     3720.7029\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        3992.21\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-63\r\nNUMA node1 CPU(s):               64-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.4.1\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.4\r\n[pip3] torch==2.1.1+git011de5c\r\n[pip3] torchvision==0.16.1+fdea156\r\n[pip3] triton==2.1.0\r\n[conda] No relevant packagesROCM Version: 6.0.32830-d62f6a171\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\n\n### üêõ Describe the bug\n\nGot this error when running `python benchmarks/benchmark_throughput.py --model meta-llama/Llama-2-7b-hf -tp 2 --dataset ShareGPT_Vicuna_unfiltered/resolve/main/ShareGPT_V3_unfiltered_cleaned_split.json`. I tried multiple times, but always got the same error. I didn't get this error when using a single GPU.\r\n\r\n```\r\nProcessed prompts:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                                                  | 799/1000 [04:55<00:17, 11.58it/s]\r\n(RayWorkerVllm pid=12470) /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/Indexing.hip:1294: indexSelectLargeIndex: Device-side assertion `srcIndex < srcSelectDimSize' failed.\r\n(RayWorkerVllm pid=12470) /var/lib/jenkins/pytorch/aten/src/ATen/native/hip/Indexing.hip:1294: indexSelectLargeIndex: Device-side assertion `srcIndex < srcSelectDimSize' failed.\r\n```\r\n\r\n```\r\n(RayWorkerVllm pid=12470) :0:rocdevice.cpp            :2726: 1292301652939 us: [pid:12470 tid:0x7eeb82ffd700] Callback: Queue 0x7eebf4200000 aborting with error : HSA_STATUS_ERROR_EXCEPTION: An HSAIL operation resulted in a hardware exception. code: 0x1016\r\n(RayWorkerVllm pid=12470) *** SIGABRT received at time=1712868846 on cpu 107 ***\r\n(RayWorkerVllm pid=12470) PC: @     0x7f1d4cad800b  (unknown)  raise\r\n(RayWorkerVllm pid=12470)     @     0x7f1d4cdf5420  (unknown)  (unknown)\r\n(RayWorkerVllm pid=12470) [2024-04-11 20:54:06,580 E 12470 16787] logging.cc:361: *** SIGABRT received at time=1712868846 on cpu 107 ***\r\n(RayWorkerVllm pid=12470) [2024-04-11 20:54:06,580 E 12470 16787] logging.cc:361: PC: @     0x7f1d4cad800b  (unknown)  raise\r\n(RayWorkerVllm pid=12470) [2024-04-11 20:54:06,580 E 12470 16787] logging.cc:361:     @     0x7f1d4cdf5420  (unknown)  (unknown)\r\n(RayWorkerVllm pid=12470) Fatal Python error: Aborted\r\n(RayWorkerVllm pid=12470) \r\n```",
    "state": "closed",
    "labels": [
      "bug",
      "rocm"
    ],
    "created_at": "2024-04-11T20:58:32Z",
    "closed_at": "2024-09-04T14:07:11Z",
    "author": "WoosukKwon",
    "comments_count": 3,
    "comments": [
      {
        "author": "hongxiayang",
        "body": "@WoosukKwon This is a known issue when using non-eager mode. Please use `--enforce-eager` to run the benchmarking for now for tp > 1.",
        "created_at": "2024-04-12T14:51:16Z"
      },
      {
        "author": "sunway513",
        "body": "From @WoosukKwon , `--enforce-eager` doesn't use hipgraph, it's functional but impact the perf. ",
        "created_at": "2024-04-25T16:49:08Z"
      },
      {
        "author": "hongxiayang",
        "body": "Closing this issue as the graph mode has been supported after ROCm 6.1 upgrade in current main branch.",
        "created_at": "2024-09-04T14:07:11Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5067": {
    "issue_number": 5067,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5067",
    "title": "[Bug]: The VRAM usage of calculating log_probs is not considered in profile run",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.9.1-arch1-1-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Ti SUPER\r\nNvidia driver version: 550.78\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               24\r\nOn-line CPU(s) list:                  0-23\r\nVendor ID:                            GenuineIntel\r\nModel name:                           13th Gen Intel(R) Core(TM) i7-13700K\r\nCPU family:                           6\r\nModel:                                183\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             1\r\nCPU max MHz:                          5400.0000\r\nCPU min MHz:                          800.0000\r\nBogoMIPS:                             6837.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect user_shstk avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            640 KiB (16 instances)\r\nL1i cache:                            768 KiB (16 instances)\r\nL2 cache:                             24 MiB (10 instances)\r\nL3 cache:                             30 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-23\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Mitigation; Clear Register File\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: 7.0 7.5 8.0 8.6 8.9 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \t0-23\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nI encountered an unexpected `CUDA out of memory` error while adding a new feature for LoRA into vLLM. After experimenting with different settings, I discovered that the bug only appears when `prompt_logprobs` in `SamplingParams` is set to a non-zero value and a long prompt length is used, as mentioned in #1532. I then tried to locate the bug and found the following (some unimportant tracebacks are omitted):\r\n\r\n```\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/workspace/vllm/worker/model_runner.py\", line 721, in execute_model\r\n[rank0]:     output = self.model.sample(\r\n[rank0]:   File \"/workspace/vllm/model_executor/models/llama.py\", line 381, in sample\r\n[rank0]:     next_tokens = self.sampler(logits, sampling_metadata)\r\n[rank0]:   File \"/workspace/vllm/model_executor/layers/sampler.py\", line 72, in forward\r\n[rank0]:     logits = _apply_penalties(logits, sampling_tensors.prompt_tokens,\r\n[rank0]:   File \"/workspace/vllm/model_executor/layers/sampler.py\", line 208, in _apply_penalties\r\n[rank0]:     output_bin_counts, output_mask = _get_bin_counts_and_mask(\r\n[rank0]:   File \"/workspace/vllm/model_executor/layers/sampler.py\", line 143, in _get_bin_counts_and_mask\r\n[rank0]:     bin_counts = torch.zeros((num_seqs, vocab_size + 1),\r\n[rank0]: torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.41 GiB. GPU\r\n```\r\n\r\nThe bug is caused by calculations in `_get_bin_counts_and_mask` during the sampling phase. When `prompt_logprobs` is enabled, the log probabilities of all tokens in the prompt (up to 8192 for Llama 3, which I am using) are calculated, leading to a memory usage of up to\r\n\r\n$$num\\ tokens \\times vocab\\ size \\times 4 \\text{Bytes} = 8192 \\times 128256 \\times 4 \\text{Bytes} = 7.8 \\text{GiB}$$\r\n\r\nHowever, this memory usage is not predicted in `profile_run()`, where the sampling parameters are set as:\r\n```python\r\n# Enable top-k sampling to reflect the accurate memory usage.\r\nsampling_params = SamplingParams(top_p=0.99, top_k=self.vocab_size - 1)\r\n```\r\nThis only considers the calculation of log probabilities for up to 256 tokens (the maximum batched sequence count). ",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-05-27T07:36:26Z",
    "closed_at": "2024-11-25T02:06:03Z",
    "author": "Conless",
    "comments_count": 11,
    "comments": [
      {
        "author": "Conless",
        "body": "A trivial way to solve this is to add a limitation $log\\ prob\\ tokens < max\\ seqs$ in the following function in `scheduler.py`:\r\n```py\r\ndef can_schedule(self, *, num_new_tokens: int, num_new_seqs: int):\r\n        assert num_new_tokens != 0\r\n        assert num_new_seqs != 0\r\n        return (self.num_batched_tokens + num_new_tokens <= self.token_budget\r\n                and self.num_curr_seqs + num_new_seqs <= self.max_num_seqs)\r\n```\r\nIf this solution is acceptable, I can submit a pull request later.",
        "created_at": "2024-05-27T07:40:41Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "Agree this is an issue that needs to be fixed.\r\n\r\nI don't quite see how `log_prob_tokens < max_seqs` is the right solution though ... isn't this a bit too course-grained?",
        "created_at": "2024-05-27T17:12:35Z"
      },
      {
        "author": "Conless",
        "body": "@robertgshaw2-neuralmagic I agree with you. Another solution I came up with, which is more fine-grained, is to add a new argument `max_num_logprobs` to `EngineArgs` (defaulting to the value of `max_num_seqs`). However, I'm concerned that this argument might rarely be used.\r\n\r\nWhat do you think about it?",
        "created_at": "2024-05-28T01:35:58Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "> @robertgshaw2-neuralmagic I agree with you. Another solution I came up with, which is more fine-grained, is to add a new argument `max_num_logprobs` to `EngineArgs` (defaulting to the value of `max_num_seqs`). However, I'm concerned that this argument might rarely be used.\r\n> \r\n> What do you think about it?\r\n\r\nI think we should have some user controlled `max_num_logprobs` with a sensible default. Let me ask the rest of the group\r\n\r\nThen we will need to:\r\n- update profiling logic to take this into account\r\n- update scheduler logic to take this into account",
        "created_at": "2024-05-28T01:40:01Z"
      },
      {
        "author": "Conless",
        "body": "@robertgshaw2-neuralmagic Thank you for considering this.\r\n\r\n> Then we will need to:\r\n> \r\n> * update profiling logic to take this into account\r\n> * update scheduler logic to take this into account\r\n\r\nThis modification should not be too difficult. Given the concise structure of the current code, it seems feasible to implement by adding the logic to the generation of prompts in `profile_run` and to `can_schedule` in the scheduler.\r\n",
        "created_at": "2024-05-28T02:08:00Z"
      },
      {
        "author": "rangehow",
        "body": "Just ran into this problem : (   ",
        "created_at": "2024-06-04T08:55:15Z"
      },
      {
        "author": "mgoin",
        "body": "@Conless would you be willing to tackle the support for this? I agree it would be a nice improvement - I have run into this when performing LLM evaluations on MMLU, this requires a lot of logprobs.",
        "created_at": "2024-06-04T23:28:13Z"
      },
      {
        "author": "Conless",
        "body": "@mgoin No problem, I'd be delighted to tackle it.",
        "created_at": "2024-06-05T09:28:59Z"
      },
      {
        "author": "likaixin2000",
        "body": "Had the same problem :( Are there any quick fixes to this?",
        "created_at": "2024-06-07T08:41:27Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-26T02:00:31Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-25T02:06:02Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6449": {
    "issue_number": 6449,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6449",
    "title": "[Bug]: Seed issue with Pipeline Parallel",
    "body": "### Your current environment\n\nv0.5.1\n\n### üêõ Describe the bug\n\nOpenAI API specifies that you can provide a seed: https://platform.openai.com/docs/api-reference/chat/create#chat-create-seed\r\n\r\nThis allows reproducibility for example with non-zero temperature parameter.\r\n\r\nCurrently, any state information is stored/advanced on the driver process only. We need to extend this to the worker actually doing the sampling.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-15T18:35:56Z",
    "closed_at": "2024-07-30T17:40:09Z",
    "author": "andoorve",
    "comments_count": 9,
    "comments": [
      {
        "author": "binxuan",
        "body": "+ 1, got ```openai.InternalServerError: Internal Server Error``` when I specified seed in the completion request\r\n",
        "created_at": "2024-07-16T23:35:35Z"
      },
      {
        "author": "andoorve",
        "body": "@binxuan do you have any other debug information? That shouldn't be expected",
        "created_at": "2024-07-17T02:39:20Z"
      },
      {
        "author": "binxuan",
        "body": "Below is my error trace, not sure if this is the root cause.\r\n \r\n> Traceback (most recent call last):\r\n  File \"python/ray/_raylet.pyx\", line 919, in ray._raylet.prepare_args_internal\r\n  File \"/home/binxuan/.conda/envs/vllm_dist/lib/python3.10/site-packages/ray/_private/serialization.py\", line 519, in serialize\r\n    return self._serialize_to_msgpack(value)\r\n  File \"/home/binxuan/.conda/envs/vllm_dist/lib/python3.10/site-packages/ray/_private/serialization.py\", line 497, in _serialize_to_msgpack\r\n    pickle5_serialized_object = self._serialize_to_pickle5(\r\n  File \"/home/binxuan/.conda/envs/vllm_dist/lib/python3.10/site-packages/ray/_private/serialization.py\", line 444, in _serialize_to_pickle5\r\n    raise e\r\n  File \"/home/binxuan/.conda/envs/vllm_dist/lib/python3.10/site-packages/ray/_private/serialization.py\", line 439, in _serialize_to_pickle5\r\n    inband = pickle.dumps(\r\n  File \"/home/binxuan/.conda/envs/vllm_dist/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py\", line 1479, in dumps\r\n    cp.dump(obj)\r\n  File \"/home/binxuan/.conda/envs/vllm_dist/lib/python3.10/site-packages/ray/cloudpickle/cloudpickle.py\", line 1245, in dump\r\n    return super().dump(obj)\r\nTypeError: cannot pickle 'torch._C.Generator' object",
        "created_at": "2024-07-18T02:46:05Z"
      },
      {
        "author": "andoorve",
        "body": "@njhill any comments? Did you run into the above?",
        "created_at": "2024-07-18T03:10:46Z"
      },
      {
        "author": "njhill",
        "body": "@andoorve yes this is the known issue, I should hopefully have time to fix it tomorrow.",
        "created_at": "2024-07-18T03:44:55Z"
      },
      {
        "author": "sekh77",
        "body": "Hi @njhill - Hope you have had some chance to fix this issue. Is it available now in the latest version (0.5.2)?",
        "created_at": "2024-07-21T13:51:42Z"
      },
      {
        "author": "njhill",
        "body": "@sekh77 it's not in 0.5.2. I am working on it right now and it should be ready today.",
        "created_at": "2024-07-22T16:24:25Z"
      },
      {
        "author": "sekh77",
        "body": "@njhill  - Ok, thanks. Will this also be available for people who are on 0.5.1?",
        "created_at": "2024-07-22T18:10:33Z"
      },
      {
        "author": "njhill",
        "body": "@sekh77 yes, it will be available to everyone! You'll just have to upgrade to the latest version (hopefully will be in a new release in the next day or two).",
        "created_at": "2024-07-22T18:43:06Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5736": {
    "issue_number": 5736,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5736",
    "title": "[Bug]: which torchvision version required",
    "body": "/vllm_2$ python examples/phi3v_example.py\r\nWARNING 06-21 14:53:06 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For multi-node inference, please install Ray with `pip install ray`.\r\n/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\r\n  warnings.warn(\r\nINFO 06-21 14:53:08 llm_engine.py:164] Initializing an LLM engine (v0.5.0.post1) with config: model='microsoft/Phi-3-vision-128k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-vision-128k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8128, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/Phi-3-vision-128k-instruct)\r\nWARNING 06-21 14:53:10 cpu_executor.py:116] CUDA graph is not supported on CPU, fallback to the eager mode.\r\nWARNING 06-21 14:53:10 cpu_executor.py:143] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\nINFO 06-21 14:53:10 selector.py:113] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\nINFO 06-21 14:53:10 selector.py:64] Using Torch SDPA backend.\r\nINFO 06-21 14:53:11 selector.py:113] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\nINFO 06-21 14:53:11 selector.py:64] Using Torch SDPA backend.\r\nINFO 06-21 14:53:12 weight_utils.py:218] Using model weights format ['*.safetensors']\r\nINFO 06-21 14:53:14 cpu_executor.py:72] # CPU blocks: 682\r\nProcessed prompts:   0%|                                                                                    | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s][rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/examples/phi3v_example.py\", line 58, in <module>\r\n[rank0]:     run_phi3v()\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/examples/phi3v_example.py\", line 31, in run_phi3v\r\n[rank0]:     outputs = llm.generate(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/utils.py\", line 727, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/entrypoints/llm.py\", line 304, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/entrypoints/llm.py\", line 556, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/engine/llm_engine.py\", line 806, in step\r\n[rank0]:     output = self.model_executor.execute_model(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/executor/cpu_executor.py\", line 78, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/worker/cpu_worker.py\", line 302, in execute_model\r\n[rank0]:     output = self.model_runner.execute_model(seq_group_metadata_list,\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/worker/cpu_model_runner.py\", line 337, in execute_model\r\n[rank0]:     ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/worker/cpu_model_runner.py\", line 287, in prepare_input_tensors\r\n[rank0]:     ) = self._prepare_prompt(seq_group_metadata_list)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/worker/cpu_model_runner.py\", line 132, in _prepare_prompt\r\n[rank0]:     mm_kwargs = self.multi_modal_input_processor(mm_data)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/multimodal/registry.py\", line 142, in process_input\r\n[rank0]:     .process_input(data, model_config, vlm_config)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/multimodal/base.py\", line 126, in process_input\r\n[rank0]:     return processor(data, model_config, vlm_config)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/multimodal/image.py\", line 109, in _default_input_processor\r\n[rank0]:     image_processor = self._get_hf_image_processor(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/multimodal/image.py\", line 97, in _get_hf_image_processor\r\n[rank0]:     return cached_get_image_processor(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/vllm-0.5.0.post1+cpu-py3.10-linux-x86_64.egg/vllm/transformers_utils/image_processor.py\", line 21, in get_image_processor\r\n[rank0]:     processor: BaseImageProcessor = AutoImageProcessor.from_pretrained(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py\", line 398, in from_pretrained\r\n[rank0]:     image_processor_class = get_class_from_dynamic_module(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\", line 501, in get_class_from_dynamic_module\r\n[rank0]:     final_module = get_cached_module_file(\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\", line 326, in get_cached_module_file\r\n[rank0]:     modules_needed = check_imports(resolved_module_file)\r\n[rank0]:   File \"/home/sapidblue/SapidBlue/invoice_data_extraction/vllm_2/venv/lib/python3.10/site-packages/transformers/dynamic_module_utils.py\", line 181, in check_imports\r\n[rank0]:     raise ImportError(\r\n[rank0]: ImportError: This modeling file requires the following packages that were not found in your environment: torchvision. Run `pip install torchvision`\r\nProcessed prompts:   0%|                                                                                    | 0/1 [00:01<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-21T09:26:04Z",
    "closed_at": "2024-06-24T04:11:54Z",
    "author": "tusharraskar",
    "comments_count": 12,
    "comments": [
      {
        "author": "tusharraskar",
        "body": "I am using cpu\r\n",
        "created_at": "2024-06-21T09:27:09Z"
      },
      {
        "author": "DarkLight1337",
        "body": "The latest version should be fine. By the way, Phi-3V is not well-supported on CPU yet.",
        "created_at": "2024-06-21T11:55:55Z"
      },
      {
        "author": "tusharraskar",
        "body": "for cpu required torch == 2.3.1+cpu and torchvision==0.18.1  required torch == 2.3.1\r\n\r\ntorchvision==0.18.1 not working",
        "created_at": "2024-06-21T12:03:06Z"
      },
      {
        "author": "DarkLight1337",
        "body": "@Isotr0py can you provide more insight on this?",
        "created_at": "2024-06-21T12:21:42Z"
      },
      {
        "author": "Isotr0py",
        "body": "@tusharraskar `torchvision==0.18.1` should work fine. Maybe you can try to reinstall `torch` and `torchvision`:\r\n```bash\r\npip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\r\n```",
        "created_at": "2024-06-21T12:33:02Z"
      },
      {
        "author": "DarkLight1337",
        "body": "The CI failure [here](https://buildkite.com/vllm/ci-aws/builds/2193#019039d9-928a-4056-b293-9ae18af44dc6) may be related to this issue. It seems that `torchvision` cannot be installed correctly alongside PyTorch for CPU.",
        "created_at": "2024-06-21T14:21:48Z"
      },
      {
        "author": "Isotr0py",
        "body": "@DarkLight1337 We only added `torchvision` to `requirements-test.txt` and `.buildkite/run-cpu-test.sh` didn't install package from it.  So `torchvision` wouldn't be installed by default in intel-test.",
        "created_at": "2024-06-21T14:54:50Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> @DarkLight1337 We only added `torchvision` to `requirements-test.txt` and `.buildkite/run-cpu-test.sh` didn't install package from it. So `torchvision` wouldn't be installed by default in intel-test.\r\n\r\nHmm ok, let me update that then.",
        "created_at": "2024-06-21T14:59:20Z"
      },
      {
        "author": "tusharraskar",
        "body": "\r\n> @tusharraskar `torchvision==0.18.1` should work fine. Maybe you can try to reinstall `torch` and `torchvision`:\r\n> \r\n> ```shell\r\n> pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\r\n> ```\r\n\r\n\r\nNow it's working, but the model is using only one core, which makes the process slow. I also added a screenshot.\r\n![Screenshot from 2024-06-21 20-50-27](https://github.com/vllm-project/vllm/assets/117747869/d77b3a4f-6bac-4e8c-a51d-59838330a299)\r\n\r\n\r\n",
        "created_at": "2024-06-21T15:19:43Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> > @tusharraskar `torchvision==0.18.1` should work fine. Maybe you can try to reinstall `torch` and `torchvision`:\r\n> > ```shell\r\n> > pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cpu\r\n> > ```\r\n> \r\n> Now it's working, but the model is using only one core, which makes the process slow. I also added a screenshot\r\n\r\n> The latest version should be fine. By the way, Phi-3V is not well-supported on CPU yet.\r\n\r\nThat is exactly what I meant by this.\r\n",
        "created_at": "2024-06-21T15:24:21Z"
      },
      {
        "author": "DarkLight1337",
        "body": "You can check #4194 for the progress on CPU support.",
        "created_at": "2024-06-21T15:38:31Z"
      },
      {
        "author": "Isotr0py",
        "body": "@tusharraskar I think adding `dtype=\"bfloat16\"` or `dtype=\"float\"` to `LLM` should be able to use multi-core. ",
        "created_at": "2024-06-21T15:43:56Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/10693": {
    "issue_number": 10693,
    "issue_url": "https://github.com/vllm-project/vllm/issues/10693",
    "title": "[Bug]: MambaCacheManager Can Possibly Run Out of Free Slots",
    "body": "### Your current environment\r\n\r\n_No response_\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### üêõ Describe the bug\r\n\r\n\r\nIn the current implementation of `MambaCacheManager._assign_seq_id_to_cache_index`, if `cur_id` is not amongst the finished requests, it will try to pop a `free_cache_index`.\r\n- However, it seems there might be an edge case where the `_assign_seq_id_to_cache_index` tries to aggressively pop free indices before `_release_finished_requests` has a change to return them\r\n\r\nWe have some private experiments involving mamba that we reuse the above `MambaCacheManager` implementation, but we have  observed errors like below\r\n\r\n```\r\n  File \"/net/storage149/mnt/md0/nmg/miniconda3/envs/vllm-mamba/lib/python3.10/site-packages/vllm/model_executor/models/jamba.py\", line 441, in forward\r\n    ) = self.mamba_cache.current_run_tensors(input_ids, attn_metadata,\r\n  File \"/net/storage149/mnt/md0/nmg/miniconda3/envs/vllm-mamba/lib/python3.10/site-packages/vllm/model_executor/models/mamba_cache.py\", line 54, in current_run_tensors\r\n    state_indices = self._prepare_current_run_mamba_cache(\r\n  File \"/net/storage149/mnt/md0/nmg/miniconda3/envs/vllm-mamba/lib/python3.10/site-packages/vllm/model_executor/models/mamba_cache.py\", line 144, in _prepare_current_run_mamba_cache\r\n    return [\r\n  File \"/net/storage149/mnt/md0/nmg/miniconda3/envs/vllm-mamba/lib/python3.10/site-packages/vllm/model_executor/models/mamba_cache.py\", line 145, in <listcomp>\r\n    self._assign_seq_id_to_cache_index(req_id, seq_id,\r\n  File \"/net/storage149/mnt/md0/nmg/miniconda3/envs/vllm-mamba/lib/python3.10/site-packages/vllm/model_executor/models/mamba_cache.py\", line 119, in _assign_seq_id_to_cache_index\r\n    destination_index = self.free_cache_indices.pop()\r\nIndexError: pop from empty list\r\n```\r\n\r\nwhich suggests the issue being diagnosed above. \r\n\r\nWe have made sure that we initialize `MambaCacheManager` will have `max_batch_size` equal to `scheduler_config.max_num_seqs`, which we have set it 10 times as large as our batch_size. We use around 8 scheduler steps.\r\n\r\nQuestion: But how can we be sure that the cache occupancy will never exceed `max_batch_size`?\r\n\r\nCC: @nelsonspbr \r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-11-27T03:43:28Z",
    "closed_at": "2024-11-27T19:02:29Z",
    "author": "fabianlim",
    "comments_count": 5,
    "comments": [
      {
        "author": "mzusman",
        "body": "Hi @fabianlim , \r\nThank you for finding out this bug, we haven't had the chance to test mamba-like models with multistep scheduler setup,\r\nSince the ssm cache of mamba models in vLLM are managed inside the modeling file, It can cause some problems with the release and allocation of cache slots of incoming/finished requests, especially upon introduction of new features like multistep scheduling.\r\nPlease take a look at #10705 I just opened that fixes this bug. \r\nThanks!",
        "created_at": "2024-11-27T10:36:17Z"
      },
      {
        "author": "fabianlim",
        "body": "thank you very much! we will try it as soon as we get a chance! @mzusman ",
        "created_at": "2024-11-27T11:38:29Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "+1, we are still seeing this bug, any recommendation setup we can use to work around?",
        "created_at": "2025-02-11T08:14:12Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "I don't think multi-step is enabled here:\n```\n[Replica 0] INFO 02-11 06:17:34 llm_engine.py:232] Initializing a V0 LLM engine (v20250207a0) with config: model='/models/jamba-1.5-mini', speculative_config=None, tokenizer='/models/jamba-1.5-mini', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=262144, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/models/jamba-1.5-mini, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":64}, use_cached_outputs=False,\n```",
        "created_at": "2025-02-11T08:17:54Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "create another issue https://github.com/vllm-project/vllm/issues/13129",
        "created_at": "2025-02-12T06:05:30Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6015": {
    "issue_number": 6015,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6015",
    "title": "embedings  error                    python -m vllm.entrypoints.openai.api_server --trust-remote-code --model gte_Qwen2-7B-instruct --seed 48 --max-model-len 1000 --tensor-parallel-size 2 --gpu-memory-utilization 1 --dtype float16 ",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nERROR 07-01 08:12:10 async_llm_engine.py:52] Engine background task failed\r\nERROR 07-01 08:12:10 async_llm_engine.py:52] Traceback (most recent call last):\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     return_value = task.result()\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     return fut.result()\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     request_outputs = await self.engine.step_async()\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     output = await self.model_executor.execute_model_async(\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 166, in execute_model_async\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 149, in _driver_execute_model_async\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     return await self.driver_exec_model(execute_model_req)\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 280, in execute_model\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 735, in execute_model\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     ) = self.prepare_input_tensors(seq_group_metadata_list)\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 682, in prepare_input_tensors\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     sampling_metadata = SamplingMetadata.prepare(\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/sampling_metadata.py\", line 116, in prepare\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     ) = _prepare_seq_groups(seq_group_metadata_list, seq_lens, query_lens,\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]   File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/sampling_metadata.py\", line 208, in _prepare_seq_groups\r\nERROR 07-01 08:12:10 async_llm_engine.py:52]     if sampling_params.seed is not None:\r\nERROR 07-01 08:12:10 async_llm_engine.py:52] AttributeError: 'NoneType' object has no attribute 'seed'\r\nException in callback functools.partial(<function _log_task_completion at 0x7f40f9a39630>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f40b87a3160>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7f40f9a39630>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f40b87a3160>>)>\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/opt/conda/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    return fut.result()\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 166, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 149, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 280, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 735, in execute_model\r\n    ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 682, in prepare_input_tensors\r\n    sampling_metadata = SamplingMetadata.prepare(\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/sampling_metadata.py\", line 116, in prepare\r\n    ) = _prepare_seq_groups(seq_group_metadata_list, seq_lens, query_lens,\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/sampling_metadata.py\", line 208, in _prepare_seq_groups\r\n    if sampling_params.seed is not None:\r\nAttributeError: 'NoneType' object has no attribute 'seed'\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 54, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\nINFO 07-01 08:12:10 async_llm_engine.py:167] Aborted request cmpl-13a5e1f614ab4afe99ca9ccc99097603-0.\r\nINFO:     192.168.30.254:63180 - \"POST /v1/embeddings HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 399, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/opt/conda/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 70, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/opt/conda/lib/python3.10/site-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/opt/conda/lib/python3.10/site-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/opt/conda/lib/python3.10/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 132, in create_embedding\r\n    generator = await openai_serving_embedding.create_embedding(\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_embedding.py\", line 124, in create_embedding\r\n    async for i, res in result_generator:\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/utils.py\", line 250, in consumer\r\n    raise e\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/utils.py\", line 241, in consumer\r\n    raise item\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/utils.py\", line 225, in producer\r\n    async for item in iterator:\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 747, in encode\r\n    async for output in self._process_request(\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 780, in _process_request\r\n    raise e\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 776, in _process_request\r\n    async for request_output in stream:\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 89, in __anext__\r\n    raise result\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/opt/conda/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    return fut.result()\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 166, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 149, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/opt/conda/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/worker.py\", line 280, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 735, in execute_model\r\n    ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 682, in prepare_input_tensors\r\n    sampling_metadata = SamplingMetadata.prepare(\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/sampling_metadata.py\", line 116, in prepare\r\n    ) = _prepare_seq_groups(seq_group_metadata_list, seq_lens, query_lens,\r\n  File \"/opt/conda/lib/python3.10/site-packages/vllm/model_executor/sampling_metadata.py\", line 208, in _prepare_seq_groups\r\n    if sampling_params.seed is not None:\r\nAttributeError: 'NoneType' object has no attribute 'seed'",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-07-01T08:15:44Z",
    "closed_at": "2024-11-15T04:23:12Z",
    "author": "2679326161or",
    "comments_count": 6,
    "comments": [
      {
        "author": "youkaichao",
        "body": "please provide more details, following the issue template to report your environment, and show how you use vllm.",
        "created_at": "2024-07-06T01:40:51Z"
      },
      {
        "author": "Junyi-99",
        "body": "same error here.\r\n\r\nI triggered this exception by adding an OpenAI-API-compatible embedding model in Dify.\r\n\r\n---\r\n\r\nI was using the generation model `Llama-3-8b` instead of an embedding model.\r\n\r\nthe problem solved when I switch to an embedding model.",
        "created_at": "2024-07-08T03:03:38Z"
      },
      {
        "author": "Junyi-99",
        "body": "@2679326161or If you want to use an embedding model, try: https://huggingface.co/intfloat/e5-mistral-7b-instruct. \r\n\r\nThe models, such as Llama-3-8b, Mistral-7B-Instruct-v0.3, are generation models rather than an embedding model",
        "created_at": "2024-07-08T07:25:03Z"
      },
      {
        "author": "LJLQ",
        "body": "Êàë‰πüÈÅáÂà∞‰∫ÜÂêåÊ†∑ÁöÑÈóÆÈ¢òÔºåÂΩìËØ∑Ê±ÇembeddingÁöÑÊó∂ÂÄôÊúçÂä°Â∞±Êä•Èîô‰∫ÜÔºåÂêéÈù¢Âç≥‰ΩøÊòØchatËØ∑Ê±Ç‰πüÊó†Ê≥ïÊ≠£Â∏∏ËøîÂõû",
        "created_at": "2024-07-09T06:57:48Z"
      },
      {
        "author": "QwertyJack",
        "body": "Related to #7502 and fixed by #7504.",
        "created_at": "2024-08-14T06:51:35Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-11-14T02:01:11Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7742": {
    "issue_number": 7742,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7742",
    "title": "[Bug]: Requesting Prompt Logprobs with an MLP Speculator Crashes the Server",
    "body": "### Your current environment\n\nUsing the latest vLLM off of `main`.\n\n### üêõ Describe the bug\n\nWhen running the online server with a model with an MLP speculator, sending a request that request prompt logprobs causes the server to crash with an `AssertionError`.\r\n\r\nStacktrace:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 125, in generate\r\n    async for request_output in results_generator:\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 1054, in generate\r\n    async for output in await self.add_request(\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 114, in generator\r\n    raise result\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 920, in run_engine_loop\r\n    result = task.result()\r\n             ^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 863, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 332, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/executor/gpu_executor.py\", line 170, in execute_model_async\r\n    output = await make_async(self.driver_worker.execute_model\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.11/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 387, in execute_model\r\n    return self._run_no_spec(execute_model_req,\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib64/python3.11/contextlib.py\", line 81, in inner\r\n    return func(*args, **kwds)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 481, in _run_no_spec\r\n    self.previous_hidden_states.update(\r\n  File \"/workspace/my-vllm/lib64/python3.11/site-packages/vllm/sequence.py\", line 1199, in update\r\n    assert len(seq_group_metadata_list) == len(hidden_states)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n```\r\n\r\n### To Reproduce\r\n\r\nRun a server with an MLP speculator, eg. one of IBM's granite models:\r\n```\r\nvllm serve ibm-granite/granite-3b-code-instruct --speculative-model ibm-granite/granite-3b-code-instruct-accelerator --use-v2-block-manager --enforce-eager\r\n```\r\n\r\nSend an `echo` request with logprobs requested for the prompt tokens:\r\n```\r\ncurl http://localhost:8000/v1/completions \\\r\n  -H \"Content-Type: application/json\" \\\r\n  -d '{\r\n      \"model\": \"ibm-granite/granite-3b-code-instruct\",\r\n      \"prompt\": \"Hello World\",\r\n      \"echo\": 1,\r\n      \"logprobs\": 1,\r\n      \"temperature\": 0\r\n  }'\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-21T15:47:39Z",
    "closed_at": "2024-09-25T00:29:57Z",
    "author": "tjohnson31415",
    "comments_count": 1,
    "comments": [
      {
        "author": "tjohnson31415",
        "body": "I wanted to create an issue to describe the crash and the reproduction, but I am also investigating a fix for this and will push up a PR soon.",
        "created_at": "2024-08-21T15:48:17Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8369": {
    "issue_number": 8369,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8369",
    "title": "[Bug]: Crash after few multi image calls",
    "body": "### Your current environment\r\n\r\nEnvironment was set up by pulling the main branch and building the Dockerfile. Hardware was 4xA100 with an Azure Instance (Standard NC96ads A100 v4). Server image is: ubuntu-hpc (2204)\r\n\r\nStartup:\r\npython3 -m vllm.entrypoints.openai.api_server --port=8000 --host=0.0.0.0 --chat-template=\"/docker_share/models/internVL2-template.jinja\" --model=\"/fine_tunes/internvl2_76b_hermes2_llama3_70b_dynamic_res_2nd_finetune\" --tensor-parallel-size=4 --max-model-len=8192 --trust_remote_code --enforce-eager --max-lora-rank 128 --limit-mm-per-prompt image=4\r\n\r\n### üêõ Describe the bug\r\n\r\nI have build from source with the current main branch to use online multi image inference with internVL2 76B (finetuned). First few inferences work with no issue. After like 10 calls the server crashes with following stack trace\r\n\r\nThe issue occurs when callen multithreaded and single threaded.\r\nSomehow the bug doesnt happen when i remove  --max-lora-rank 128 and set --max-model-len=6000\r\n\r\n<details>\r\n<summary>Stack trace</summary>\r\n\r\n```text\r\nERROR 09-11 05:24:13 async_llm_engine.py:63] Engine background task failed\r\nERROR 09-11 05:24:13 async_llm_engine.py:63] Traceback (most recent call last):\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     return_value = task.result()\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]                    ^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     result = task.result()\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]              ^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     outputs = await self.model_executor.execute_model_async(\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     return await self.driver_exec_model(execute_model_req)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     inputs = self.prepare_input(execute_model_req)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 291, in prepare_input\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     return self._get_driver_input_and_broadcast(execute_model_req)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 253, in _get_driver_input_and_broadcast\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     self.model_runner.prepare_model_input(\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     model_input = self._prepare_model_input_tensors(\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1038, in _prepare_model_input_tensors\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     builder.add_seq_group(seq_group_metadata)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 664, in add_seq_group\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     per_seq_group_fn(inter_data, seq_group_metadata)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 636, in _compute_multi_modal_input\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     mm_kwargs = self.multi_modal_input_mapper(mm_data)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 125, in map_input\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     input_dict = plugin.map_input(model_config, data_value)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/base.py\", line 265, in map_input\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     return mapper(InputContext(model_config), data)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/internvl.py\", line 279, in input_mapper_for_internvl\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]     data = torch.stack(data)\r\nERROR 09-11 05:24:13 async_llm_engine.py:63]            ^^^^^^^^^^^^^^^^^\r\nERROR 09-11 05:24:13 async_llm_engine.py:63] RuntimeError: stack expects each tensor to be equal size, but got [7, 3, 448, 448] at entry 0 and [13, 3, 448, 448] at entry 1\r\nException in callback functools.partial(<function _log_task_completion at 0x7f2af3d3e2a0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f2af054a8a0>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7f2af3d3e2a0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f2af054a8a0>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\n    result = task.result()\r\n             ^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\n    outputs = await self.model_executor.execute_model_async(\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\n    inputs = self.prepare_input(execute_model_req)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 291, in prepare_input\r\n    return self._get_driver_input_and_broadcast(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 253, in _get_driver_input_and_broadcast\r\n    self.model_runner.prepare_model_input(\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\n    model_input = self._prepare_model_input_tensors(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1038, in _prepare_model_input_tensors\r\n    builder.add_seq_group(seq_group_metadata)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 664, in add_seq_group\r\n    per_seq_group_fn(inter_data, seq_group_metadata)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 636, in _compute_multi_modal_input\r\n    mm_kwargs = self.multi_modal_input_mapper(mm_data)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 125, in map_input\r\n    input_dict = plugin.map_input(model_config, data_value)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/base.py\", line 265, in map_input\r\n    return mapper(InputContext(model_config), data)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/internvl.py\", line 279, in input_mapper_for_internvl\r\n    data = torch.stack(data)\r\n           ^^^^^^^^^^^^^^^^^\r\nRuntimeError: stack expects each tensor to be equal size, but got [7, 3, 448, 448] at entry 0 and [13, 3, 448, 448] at entry 1\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 65, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nERROR 09-11 05:24:13 client.py:266] Got Unhealthy response from RPC Server\r\nERROR 09-11 05:24:13 client.py:412] AsyncEngineDeadError('Background loop is stopped.')\r\nERROR 09-11 05:24:13 client.py:412] Traceback (most recent call last):\r\nERROR 09-11 05:24:13 client.py:412]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 409, in generate\r\nERROR 09-11 05:24:13 client.py:412]     await self.check_health(socket=socket)\r\nERROR 09-11 05:24:13 client.py:412]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 429, in check_health\r\nERROR 09-11 05:24:13 client.py:412]     await self._send_one_way_rpc_request(\r\nERROR 09-11 05:24:13 client.py:412]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 267, in _send_one_way_rpc_request\r\nERROR 09-11 05:24:13 client.py:412]     raise response\r\nERROR 09-11 05:24:13 client.py:412] vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop is stopped.\r\nERROR 09-11 05:24:13 client.py:266] Got Unhealthy response from RPC Server\r\nERROR 09-11 05:24:13 client.py:412] AsyncEngineDeadError('Background loop is stopped.')\r\nERROR 09-11 05:24:13 client.py:412] Traceback (most recent call last):\r\nERROR 09-11 05:24:13 client.py:412]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 409, in generate\r\nERROR 09-11 05:24:13 client.py:412]     await self.check_health(socket=socket)\r\nERROR 09-11 05:24:13 client.py:412]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 429, in check_health\r\nERROR 09-11 05:24:13 client.py:412]     await self._send_one_way_rpc_request(\r\nERROR 09-11 05:24:13 client.py:412]   File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/client.py\", line 267, in _send_one_way_rpc_request\r\nERROR 09-11 05:24:13 client.py:412]     raise response\r\nERROR 09-11 05:24:13 client.py:412] vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop is stopped.\r\nCRITICAL 09-11 05:24:13 launcher.py:82] AsyncLLMEngine has failed, terminating server process\r\nINFO:     10.151.92.18:51372 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nCRITICAL 09-11 05:24:13 launcher.py:82] AsyncLLMEngine has failed, terminating server process\r\nINFO:     10.151.92.18:51378 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nINFO:     Shutting down\r\nINFO:     Waiting for application shutdown.\r\nINFO:     Application shutdown complete.\r\nINFO:     Finished server process [1009]\r\nINFO 09-11 05:24:13 server.py:228] vLLM ZMQ RPC Server was interrupted.\r\nFuture exception was never retrieved\r\nfuture: <Future finished exception=RuntimeError('stack expects each tensor to be equal size, but got [7, 3, 448, 448] at entry 0 and [13, 3, 448, 448] at entry 1')>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 115, in generate\r\n    async for request_output in results_generator:\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 1073, in generate\r\n    async for output in await self.add_request(\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 111, in generator\r\n    raise result\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 115, in generate\r\n    async for request_output in results_generator:\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 1073, in generate\r\n    async for output in await self.add_request(\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 111, in generator\r\n    raise result\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 53, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 939, in run_engine_loop\r\n    result = task.result()\r\n             ^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 868, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 345, in step_async\r\n    outputs = await self.model_executor.execute_model_async(\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 177, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 231, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/lib/python3.12/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 303, in execute_model\r\n    inputs = self.prepare_input(execute_model_req)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 291, in prepare_input\r\n    return self._get_driver_input_and_broadcast(execute_model_req)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 253, in _get_driver_input_and_broadcast\r\n    self.model_runner.prepare_model_input(\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1380, in prepare_model_input\r\n    model_input = self._prepare_model_input_tensors(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1038, in _prepare_model_input_tensors\r\n    builder.add_seq_group(seq_group_metadata)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 664, in add_seq_group\r\n    per_seq_group_fn(inter_data, seq_group_metadata)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 636, in _compute_multi_modal_input\r\n    mm_kwargs = self.multi_modal_input_mapper(mm_data)\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/registry.py\", line 125, in map_input\r\n    input_dict = plugin.map_input(model_config, data_value)\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/multimodal/base.py\", line 265, in map_input\r\n    return mapper(InputContext(model_config), data)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/internvl.py\", line 279, in input_mapper_for_internvl\r\n    data = torch.stack(data)\r\n           ^^^^^^^^^^^^^^^^^\r\nRuntimeError: stack expects each tensor to be equal size, but got [7, 3, 448, 448] at entry 0 and [13, 3, 448, 448] at entry 1\r\nERROR 09-11 05:24:14 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 1215 died, exit code: -15\r\nINFO 09-11 05:24:14 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\nroot@fee87fa97dfb:/vllm-workspace# /usr/lib/python3.12/multiprocessing/resource_tracker.py:254: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```\r\n\r\n</details>\r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-11T12:34:36Z",
    "closed_at": "2024-09-12T17:10:37Z",
    "author": "stt-pafe",
    "comments_count": 2,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "This is similar to #8361. @Isotr0py can you look into this? I think the issue stems from different images potentially having different sizes even after postprocessing.",
        "created_at": "2024-09-11T13:29:52Z"
      },
      {
        "author": "Isotr0py",
        "body": "Seems that it's caused by different `num_patches` from different image size, similar to #7392.",
        "created_at": "2024-09-11T14:04:56Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6176": {
    "issue_number": 6176,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6176",
    "title": "[Bug]: LLaVA 1.6 in 0.5.1: Exceptions after some bigger image request, stuck in faulty mode",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-35-generic-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration:\r\nGPU 0: NVIDIA L40\r\nGPU 1: NVIDIA L40\r\nGPU 2: NVIDIA L40\r\nGPU 3: NVIDIA L40\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Gold 6438Y+\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nStepping:                           8\r\nBogoMIPS:                           4000.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hfi vnmi avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr ibt amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          3 MiB (64 instances)\r\nL1i cache:                          2 MiB (64 instances)\r\nL2 cache:                           128 MiB (64 instances)\r\nL3 cache:                           120 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU1    NODE     X      SYS     SYS     0,2,4,6,8,10    0               N/A\r\nGPU2    SYS     SYS      X      NODE    1,3,5,7,9,11    1               N/A\r\nGPU3    SYS     SYS     NODE     X      1,3,5,7,9,11    1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nConfig:\r\n```text\r\nservices:\r\n  vllm-mixtral-instruct:\r\n    image: vllm/vllm-openai:v0.5.1\r\n    container_name: vllm-llava\r\n    ipc: host\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              capabilities: [ gpu ]\r\n    environment:\r\n      - HTTPS_PROXY=http://proxy.dev.init:3128\r\n      - HTTP_PROXY=http://proxy.dev.init:3128\r\n      - NO_PROXY=10.9.240.0/22,127.0.0.0/8\r\n      - NVIDIA_VISIBLE_DEVICES=2,3\r\n      - HF_TOKEN=$HF_TOKEN\r\n      - VLLM_NO_USAGE_STATS=1\r\n    volumes:\r\n      - /mnt/sda/huggingface:/root/.cache/huggingface\r\n      - .:/opt/vllm\r\n    ports:\r\n      - \"8003:8000\"\r\n    command:\r\n      - --model=llava-hf/llava-v1.6-mistral-7b-hf\r\n      # - --chat-template=/opt/vllm/template_mixtral.jinja\r\n      # - --max-model-len=24576\r\n      - --tensor-parallel-size=2\r\n      # - --swap-space=5\r\n      - --gpu-memory-utilization=0.3\r\n      - --max-num-batched-tokens=2048\r\n      - --disable-log-requests\r\n      - --enforce-eager\r\n      - --enable-chunked-prefill\r\n    restart: unless-stopped\r\n```\r\n\r\nIt works for me via OpenAI Vision compatible API calls, e.g.:\r\n\r\n```text\r\ncurl 'https://ai1.dev.init/multimodal-llava/v1/chat/completions' -k -H 'Content-Type: application/json' -d @- <<EOF\r\n{\r\n    \"model\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": \"data:image/jpeg;base64,$(curl -s https://upload.wikimedia.org/wikipedia/commons/f/fa/Grayscale_8bits_palette_sample_image.png | base64 -w 0)\"\r\n                    }\r\n                },\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": \"Was ist in dem Bild?\"\r\n                }\r\n            ]\r\n        }\r\n    ],\r\n    \"temperature\": 0.2,\r\n    \"top_p\": 0.1,\r\n    \"top_k\": 20,\r\n    \"frequency_penalty\": 0.2\r\n}\r\nEOF\r\n```\r\n\r\nBut after some bigger image I get the following exception and after that, I have to restart vLLM - doesn't work again even for smaller images.\r\n\r\n```text\r\nINFO 07-06 11:30:48 api_server.py:206] vLLM API server version 0.5.1\r\nINFO 07-06 11:30:48 api_server.py:207] args: Namespace(host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='llava-hf/llava-v1.6-mistral-7b-hf', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.3, num_gpu_blocks_override=None, max_num_batched_tokens=2048, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=True, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=True, max_log_len=None)\r\nINFO 07-06 11:30:48 config.py:698] Defaulting to use mp for distributed inference\r\nINFO 07-06 11:30:48 config.py:787] Chunked prefill is enabled (EXPERIMENTAL).\r\nINFO 07-06 11:30:48 llm_engine.py:169] Initializing an LLM engine (v0.5.1) with config: model='llava-hf/llava-v1.6-mistral-7b-hf', speculative_config=None, tokenizer='llava-hf/llava-v1.6-mistral-7b-hf', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=llava-hf/llava-v1.6-mistral-7b-hf, use_v2_block_manager=False, enable_prefix_caching=False)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n(VllmWorkerProcess pid=76) INFO 07-06 11:30:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=76) INFO 07-06 11:30:50 utils.py:741] Found nccl from library libnccl.so.2\r\nINFO 07-06 11:30:50 utils.py:741] Found nccl from library libnccl.so.2\r\nINFO 07-06 11:30:50 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=76) INFO 07-06 11:30:50 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 07-06 11:30:50 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\r\n(VllmWorkerProcess pid=76) INFO 07-06 11:30:50 custom_all_reduce_utils.py:232] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\r\nINFO 07-06 11:30:51 weight_utils.py:218] Using model weights format ['*.safetensors']\r\n(VllmWorkerProcess pid=76) INFO 07-06 11:30:51 weight_utils.py:218] Using model weights format ['*.safetensors']\r\nINFO 07-06 11:31:04 model_runner.py:255] Loading model weights took 7.1712 GB\r\n(VllmWorkerProcess pid=76) INFO 07-06 11:31:04 model_runner.py:255] Loading model weights took 7.1712 GB\r\nWARNING 07-06 11:31:04 model_runner.py:831] Computed max_num_seqs (min(256, 2048 // 2928)) to be less than 1. Setting it to the minimum value of 1.\r\n(VllmWorkerProcess pid=76) WARNING 07-06 11:31:04 model_runner.py:831] Computed max_num_seqs (min(256, 2048 // 2928)) to be less than 1. Setting it to the minimum value of 1.\r\nINFO 07-06 11:31:05 distributed_gpu_executor.py:56] # GPU blocks: 6521, # CPU blocks: 4096\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 07-06 11:31:09 serving_chat.py:94] Using default chat template:\r\nINFO 07-06 11:31:09 serving_chat.py:94] {{ bos_token }}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if message['role'] == 'user' %}{{ '[INST] ' + message['content'] + ' [/INST]' }}{% elif message['role'] == 'assistant' %}{{ message['content'] + eos_token}}{% else %}{{ raise_exception('Only user and assistant roles are supported!') }}{% endif %}{% endfor %}\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nWARNING 07-06 11:31:09 serving_embedding.py:141] embedding_mode is False. Embedding API will not work.\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 07-06 11:31:14 metrics.py:295] Avg prompt throughput: 264.9 tokens/s, Avg generation throughput: 0.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 1.3%, CPU KV cache usage: 0.0%.\r\nINFO:     192.168.5.1:46884 - \"POST /v1/chat/completions HTTP/1.0\" 200 OK\r\nINFO 07-06 11:31:19 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.4 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-06 11:31:28 metrics.py:295] Avg prompt throughput: 92.8 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.8%, CPU KV cache usage: 0.0%.\r\nINFO:     192.168.5.1:44038 - \"POST /v1/chat/completions HTTP/1.0\" 200 OK\r\nINFO 07-06 11:31:39 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 12.9 tokens/s, Running: 0 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.0%, CPU KV cache usage: 0.0%.\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: Attempted to assign 2144 = 2144 image tokens to 2043 placeholders, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 64, in start_worker_execution_loop\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     output = self.execute_model(execute_model_req=None)\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     output = self.model_runner.execute_model(\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     inputs_embeds = merge_vision_embeddings(\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]     raise ValueError(\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226] ValueError: Attempted to assign 2144 = 2144 image tokens to 2043 placeholders\r\n(VllmWorkerProcess pid=76) ERROR 07-06 11:31:43 multiproc_worker_utils.py:226]\r\nERROR 07-06 11:31:43 async_llm_engine.py:53] Engine background task failed\r\nERROR 07-06 11:31:43 async_llm_engine.py:53] Traceback (most recent call last):\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     return_value = task.result()\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     result = task.result()\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     output = await self.model_executor.execute_model_async(\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 173, in execute_model_async\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 160, in _driver_execute_model_async\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     return await self.driver_exec_model(execute_model_req)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     output = self.model_runner.execute_model(\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     return func(*args, **kwargs)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     hidden_or_intermediate_states = model_executable(\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     return self._call_impl(*args, **kwargs)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     return forward_call(*args, **kwargs)\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     inputs_embeds = merge_vision_embeddings(\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\nERROR 07-06 11:31:43 async_llm_engine.py:53]     raise ValueError(\r\nERROR 07-06 11:31:43 async_llm_engine.py:53] ValueError: Attempted to assign 2144 = 2144 image tokens to 2043 placeholders\r\nException in callback functools.partial(<function _log_task_completion at 0x713f9a053880>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x713f822c8580>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x713f9a053880>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x713f822c8580>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 173, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 160, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\n    output = self.model_runner.execute_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\n    inputs_embeds = merge_vision_embeddings(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\n    raise ValueError(\r\nValueError: Attempted to assign 2144 = 2144 image tokens to 2043 placeholders\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\nINFO:     192.168.5.1:46956 - \"POST /v1/chat/completions HTTP/1.0\" 400 Bad Request\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-06T11:37:33Z",
    "closed_at": "2024-07-07T07:47:15Z",
    "author": "andrePankraz",
    "comments_count": 10,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Can you provide a link to the image that is causing the problem?",
        "created_at": "2024-07-06T12:47:03Z"
      },
      {
        "author": "andrePankraz",
        "body": "Thats just the officiel image, that the project is pushing to Docker Hub\r\n\r\nhttps://hub.docker.com/layers/vllm/vllm-openai/v0.5.1/images/sha256-e58fceffa6f8d3e4d535f9e7128361cd33469b232a8dc670967b62ae62bac5fe?context=explore\r\n",
        "created_at": "2024-07-06T15:48:12Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> Thats just the officiel image, that the project is pushing to Docker Hub\r\n> \r\n> https://hub.docker.com/layers/vllm/vllm-openai/v0.5.1/images/sha256-e58fceffa6f8d3e4d535f9e7128361cd33469b232a8dc670967b62ae62bac5fe?context=explore\r\n\r\nI'm referring to the image that you input to the model.",
        "created_at": "2024-07-06T15:51:31Z"
      },
      {
        "author": "andrePankraz",
        "body": "Oh sry, the word image is totally rewired now in my brain ;)\r\nI was using following image from your own tests in\r\nhttps://github.com/vllm-project/vllm/blob/main/tests/entrypoints/openai/test_vision.py:\r\n\r\nImage: https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\r\n\r\n\r\n```\r\ncurl 'https://ai1.dev.init/multimodal-llava/v1/chat/completions' -k -H 'Content-Type: application/json' -d @- <<EOF\r\n{\r\n    \"model\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": \"data:image/jpeg;base64,$(curl -s https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg | base64 -w 0)\"\r\n                    }\r\n                },\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": \"Was ist in dem Bild?\"\r\n                }\r\n            ]\r\n        }\r\n    ],\r\n    \"temperature\": 0.2,\r\n    \"top_p\": 0.1,\r\n    \"top_k\": 20,\r\n    \"frequency_penalty\": 0.2\r\n}\r\nEOF\r\n```\r\n",
        "created_at": "2024-07-07T06:57:24Z"
      },
      {
        "author": "DarkLight1337",
        "body": "For that particular image, does the same problem happen if you use offline inference? We should have fixed that already...",
        "created_at": "2024-07-07T07:04:27Z"
      },
      {
        "author": "andrePankraz",
        "body": "Sry cannot invest more than that. if next version already fixes it, I'm fine with it. Just wanted to forward the observed issue.\r\nI just use VLLM docker setup on remote GPU server. I would need to much time now to create a different setup with local GPU or create some debugging Docker image (cannot install/run directly there).",
        "created_at": "2024-07-07T07:08:48Z"
      },
      {
        "author": "DarkLight1337",
        "body": "I tried the same command using a local server:\r\n```\r\nvllm/entrypoints/openai/api_server.py --port 8001 --model llava-hf/llava-v1.6-mistral-7b-hf --enforce-eager\r\n```\r\n\r\nRequest:\r\n```\r\ncurl 'http://localhost:8001/v1/chat/completions' -k -H 'Content-Type: application/json' -d @- <<EOF\r\n{\r\n    \"model\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": \"data:image/jpeg;base64,$(curl -s https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg | base64 -w 0)\"\r\n                    }\r\n                },\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": \"Was ist in dem Bild?\"\r\n                }\r\n            ]\r\n        }\r\n    ],\r\n    \"temperature\": 0.2,\r\n    \"top_p\": 0.1,\r\n    \"top_k\": 20,\r\n    \"frequency_penalty\": 0.2\r\n}\r\nEOF\r\n```\r\n\r\nOutput:\r\n```\r\n{\"id\":\"cmpl-05652f0b1fda4b1ba895beb78d6f412d\",\"object\":\"chat.completion\",\"created\":1720336691,\"model\":\"llava-hf/llava-v1.6-mistral-7b-hf\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\" Das Bild zeigt eine Landschaft mit einem Weg durch ein Grasfeld. Der Weg ist gepflastert und f√ºhrt durch eine offene, grasige Fl√§che, die von einigen B√§umen und Str√§uchern begrenzt wird. Im Hintergrund sind weitere Grasfl√§chen und B√§ume zu sehen, die unter einem weiten Himmel stehen. Es ist ein sch√∂ner, nat√ºrlicher Landschaftsbild, das eine ruhige und ungest√∂rte Umgebung darstellt. \",\"tool_calls\":[]},\"logprobs\":null,\"finish_reason\":\"stop\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":2161,\"total_tokens\":2283,\"completion_tokens\":122}}\r\n```\r\n\r\nCan you try disabling `--enable-chunked-prefill` and/or running the model on a single GPU and see if it fixes the problem?\r\n\r\n**Edit:** Actually, `--max-num-batched-tokens=2048` might be causing the problem since the image takes up 2144 tokens, and the error message shows that only 2043 (`=2048-5` where there are 5 text tokens) image tokens are available.",
        "created_at": "2024-07-07T07:22:03Z"
      },
      {
        "author": "andrePankraz",
        "body": "I have deactivated prefill/maxnumbatch and it works with that. thx.\r\n\r\nconfig was a copy from some other bigger LLM.  ",
        "created_at": "2024-07-07T07:37:44Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Glad to help!",
        "created_at": "2024-07-07T07:48:59Z"
      },
      {
        "author": "andrePankraz",
        "body": "Yes thx.\r\nMay be it deserves some warning/error in startup if it has to be ruled out.\r\nAlso it's strange, that it's stuck in this faulty mode, even for other smaller images that worked.\r\nBut I'm happy at least. :)",
        "created_at": "2024-07-07T07:57:34Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6004": {
    "issue_number": 6004,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6004",
    "title": "[Bug]: `distributed_executor_backend=mp` does not work with GPTQ tp>1",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\n`distributed_executor_backed=\"mp\"` is now enabled by default for vLLM. However, this feature is currently incompatible with some GPTQ quantization for tp>1 due to the order in which torch is initialized. We get the classic `RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method`\r\n\r\n\r\nSetting `distributed_backend_executor=\"ray\"` works for GPTQ\r\n\r\nThe following fails:\r\n```python\r\nfrom vllm import LLM\r\n\r\nMODEL_NAME=\"TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ\"\r\nTENSOR_PARALLEL_SIZE=2\r\n\r\nmodel = LLM(MODEL_NAME, enforce_eager=True, tensor_parallel_size=TENSOR_PARALLEL_SIZE, distributed_executor_backend=\"mp\")\r\nprint(model.generate(\"The best thing about the internet is\")[0].outputs[0].text)\r\n```\r\n\r\nwith:\r\n\r\n```bash\r\n(vllm-upstream) rshaw@beaker:~/vllm$ python3 run.py \r\nINFO 06-30 14:26:18 gptq_marlin.py:140] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\r\nINFO 06-30 14:26:18 llm_engine.py:169] Initializing an LLM engine (v0.5.0.post1) with config: model='TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ', speculative_config=None, tokenizer='TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=TheBloke/TinyLlama-1.1B-Chat-v1.0-GPTQ)\r\n(VllmWorkerProcess pid=3438507) Process VllmWorkerProcess:\r\n(VllmWorkerProcess pid=3438507) Traceback (most recent call last):\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n(VllmWorkerProcess pid=3438507)     self.run()\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/.pyenv/versions/3.10.14/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n(VllmWorkerProcess pid=3438507)     self._target(*self._args, **self._kwargs)\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/executor/multiproc_worker_utils.py\", line 210, in _run_worker_process\r\n(VllmWorkerProcess pid=3438507)     worker = worker_factory()\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/executor/gpu_executor.py\", line 67, in _create_worker\r\n(VllmWorkerProcess pid=3438507)     wrapper.init_worker(**self._get_worker_kwargs(local_rank, rank,\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/worker/worker_base.py\", line 311, in init_worker\r\n(VllmWorkerProcess pid=3438507)     self.worker = worker_class(*args, **kwargs)\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/worker/worker.py\", line 86, in __init__\r\n(VllmWorkerProcess pid=3438507)     self.model_runner: GPUModelRunnerBase = ModelRunnerClass(\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/worker/model_runner.py\", line 196, in __init__\r\n(VllmWorkerProcess pid=3438507)     self.attn_backend = get_attn_backend(\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/attention/selector.py\", line 45, in get_attn_backend\r\n(VllmWorkerProcess pid=3438507)     backend = which_attn_to_use(num_heads, head_size, num_kv_heads,\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm/vllm/attention/selector.py\", line 151, in which_attn_to_use\r\n(VllmWorkerProcess pid=3438507)     if torch.cuda.get_device_capability()[0] < 8:\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm-upstream/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 430, in get_device_capability\r\n(VllmWorkerProcess pid=3438507)     prop = get_device_properties(device)\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm-upstream/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 444, in get_device_properties\r\n(VllmWorkerProcess pid=3438507)     _lazy_init()  # will define _get_device_properties\r\n(VllmWorkerProcess pid=3438507)   File \"/home/rshaw/vllm-upstream/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 279, in _lazy_init\r\n(VllmWorkerProcess pid=3438507)     raise RuntimeError(\r\n(VllmWorkerProcess pid=3438507) RuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method\r\nERROR 06-30 14:26:19 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 3438507 died, exit code: 1\r\nINFO 06-30 14:26:19 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n```\r\n\r\nWorkarounds for now:\r\n- Set `distributed_executor_backend=\"mp\"`\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-30T14:29:18Z",
    "closed_at": "2024-07-01T03:07:35Z",
    "author": "robertgshaw2-redhat",
    "comments_count": 5,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "It would be great if we can better control when cuda is initialized, not sure whether that's feasible though.\r\n\r\nDespite @youkaichao 's fix to the distributed tests, it's quite a headache to ensure that CUDA is not accidentally initialized wrongly during those tests.",
        "created_at": "2024-06-30T14:49:28Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "For sure - Im going to look into this when I get some time. Its specific to GPTQ (does not happen for fp8 quantization). I think the source is that we check cuda_device_capability when deciding if we can convert GPTQ-->Marlin. I think this happens \"too early\" in the lifecycle\r\n\r\nWill look into a workaround when I get some time. I have a couple PRs I want to wrap up before I look into this",
        "created_at": "2024-06-30T14:52:19Z"
      },
      {
        "author": "llmpros",
        "body": "met the same issue - happy to poke around / peek if folks are busy with higher priority tasks",
        "created_at": "2024-06-30T22:51:21Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "Feel free. I think this function is the culprit --- initializes torch:\r\n- https://github.com/vllm-project/vllm/blob/af9ad46fca6e594797b83e5ecb2e1f31ca5e9fac/vllm/model_executor/layers/quantization/gptq_marlin.py#L155\r\n \r\nIt is called by this function:\r\n- https://github.com/vllm-project/vllm/blob/af9ad46fca6e594797b83e5ecb2e1f31ca5e9fac/vllm/model_executor/layers/quantization/gptq_marlin.py#L125\r\n\r\nWhich is called by this function:\r\n- https://github.com/vllm-project/vllm/blob/af9ad46fca6e594797b83e5ecb2e1f31ca5e9fac/vllm/config.py#L205\r\n\r\nI think solutions are:\r\n- trying to see if there is a way to get the cuda device capability without calling `torch.cuda`",
        "created_at": "2024-06-30T23:29:16Z"
      },
      {
        "author": "youkaichao",
        "body": "we can use pynvml to check the compute capability, without calling `torch.cuda` (which will initialize cuda context).",
        "created_at": "2024-06-30T23:59:30Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6152": {
    "issue_number": 6152,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6152",
    "title": "[Bug]: When tensor_parallel_size>1,  RuntimeError: Cannot re-initialize CUDA in forked subprocess.",
    "body": "### Your current environment\n\nvllm version: '0.5.0.post1'\r\n\r\n\n\n### üêõ Describe the bug\n\nWhen I set tensor_parallel_size=1, it works well.\r\nBut, if I set tensor_parallel_size>1, below error occurs:\r\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the 'spawn' start method.\r\nAfter I add\r\n```\r\nimport torch\r\nimport multiprocessing\r\ntorch.multiprocessing.set_start_method('spawn')\r\n```\r\nthe same RuntimeError still occurs.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-05T10:08:15Z",
    "closed_at": "2024-07-08T02:44:38Z",
    "author": "excelsimon",
    "comments_count": 16,
    "comments": [
      {
        "author": "youkaichao",
        "body": "please paste your full code. you might initialized cuda before using vLLM.",
        "created_at": "2024-07-05T17:41:14Z"
      },
      {
        "author": "yuchenlin",
        "body": "I'm also having the same issue with the latest version of vllm + gemma-2-27B-it",
        "created_at": "2024-07-06T08:26:30Z"
      },
      {
        "author": "yuchenlin",
        "body": "`export VLLM_WORKER_MULTIPROC_METHOD=spawn` may help",
        "created_at": "2024-07-06T08:43:42Z"
      },
      {
        "author": "youkaichao",
        "body": "I can run the following code without any issues:\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\nprompts = [\r\n    \"Hello, my name is\",\r\n    \"The president of the United States is\",\r\n    \"The capital of France is\",\r\n    \"The future of AI is\",\r\n]\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n\r\nllm = LLM(model=\"google/gemma-2-27b-it\", tensor_parallel_size=2)\r\noutputs = llm.generate(prompts, sampling_params)\r\n\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n```",
        "created_at": "2024-07-06T17:56:54Z"
      },
      {
        "author": "excelsimon",
        "body": "> `export VLLM_WORKER_MULTIPROC_METHOD=spawn` may help\r\n\r\nIt works for me. Thank you~",
        "created_at": "2024-07-08T02:44:38Z"
      },
      {
        "author": "yuchenlin",
        "body": "> I can run the following code without any issues:\r\n> \r\n> ```python\r\n> from vllm import LLM, SamplingParams\r\n> prompts = [\r\n>     \"Hello, my name is\",\r\n>     \"The president of the United States is\",\r\n>     \"The capital of France is\",\r\n>     \"The future of AI is\",\r\n> ]\r\n> sampling_params = SamplingParams(temperature=0.8, top_p=0.95)\r\n> \r\n> llm = LLM(model=\"google/gemma-2-27b-it\", tensor_parallel_size=2)\r\n> outputs = llm.generate(prompts, sampling_params)\r\n> \r\n> # Print the outputs.\r\n> for output in outputs:\r\n>     prompt = output.prompt\r\n>     generated_text = output.outputs[0].text\r\n>     print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n> ```\r\n\r\ni'm not sure. maybe it's about the version issue of powerinfer? and I actually find that even I used the above solution to make vllm able to generate, the quality is not as good as other Gemma-2-27B inference (both under greedy decoding).",
        "created_at": "2024-07-08T04:27:50Z"
      },
      {
        "author": "henry-y",
        "body": "> `export VLLM_WORKER_MULTIPROC_METHOD=spawn` may help\r\n\r\nit also works for me! thank you!",
        "created_at": "2024-07-24T13:08:21Z"
      },
      {
        "author": "Charles-L-Chen",
        "body": "I have encountered the same issue. I solved it by making `VLLM_WORKER_MULTIPROC_METHOD=spawn` as mentioned by @yuchenlin \r\nNow I'm wondering why I get this error and why setting VLLM_WORKER_MULTIPROC_METHOD solves it. Please help us know and make it clear.",
        "created_at": "2024-08-09T08:18:27Z"
      },
      {
        "author": "lonngxiang",
        "body": "not work for me\r\n```\r\nexport VLLM_WORKER_MULTIPROC_METHOD=spawn;CUDA_VISIBLE_DEVICES=1 vllm serve  /ai/minicpmv --host 192.168.2.238 --port 10868 --max-model-len 10000 --trust-remote-code --api-key token-abc123 --gpu_memory_utilization 1 --trust-remote-code \r\n```\r\n\r\n![image](https://github.com/user-attachments/assets/4e70de8e-1b7a-4299-b31e-c3fff938e74f)\r\n",
        "created_at": "2024-08-12T01:56:55Z"
      },
      {
        "author": "rin2401",
        "body": "`from peft import PeftModel, PeftConfig\r\n`\r\ni import `peft` made this error too\r\n",
        "created_at": "2024-09-19T17:28:23Z"
      },
      {
        "author": "youkaichao",
        "body": "@rin2401 try to use `distributed_executor_backend=\"ray\"` ?",
        "created_at": "2024-09-20T19:44:01Z"
      },
      {
        "author": "IIDCII",
        "body": "> `export VLLM_WORKER_MULTIPROC_METHOD=spawn` may help\r\n\r\nif you're using this in a Python notebook run the following first on a reset kernel:\r\n\r\n```\r\nimport os\r\nos.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\r\n\r\n```",
        "created_at": "2024-10-22T17:11:01Z"
      },
      {
        "author": "emohebi",
        "body": "> @rin2401 try to use `distributed_executor_backend=\"ray\"` ?\r\n\r\nThis helped me",
        "created_at": "2025-01-14T01:28:33Z"
      },
      {
        "author": "micyan01",
        "body": "solved problem by add\n\n1. os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n2. if __name__ == \"__main__\":",
        "created_at": "2025-02-21T02:30:04Z"
      },
      {
        "author": "JiangXunyi",
        "body": "> `export VLLM_WORKER_MULTIPROC_METHOD=spawn` may help\n\nyeah, it works",
        "created_at": "2025-04-06T20:23:18Z"
      },
      {
        "author": "deep-video",
        "body": "> `export VLLM_WORKER_MULTIPROC_METHOD=spawn`ÂèØËÉΩÊúâÂ∏ÆÂä©\n\nIf not, check whether your call to `model = llm()` is within the `if __name__ == \"__main__\":`",
        "created_at": "2025-04-08T14:19:00Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5334": {
    "issue_number": 5334,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5334",
    "title": "[Bug]: Unexpected prompt token logprob behaviors of llama 2 when setting echo=True for openai-api server",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: 10.0.0-4ubuntu1 \r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.9.19 (main, May  6 2024, 19:43:03)  [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-169-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA RTX A6000\r\nGPU 1: NVIDIA RTX A6000\r\nGPU 2: NVIDIA RTX A6000\r\nGPU 3: NVIDIA RTX A6000\r\nGPU 4: NVIDIA RTX A6000\r\nGPU 5: NVIDIA RTX A6000\r\nGPU 6: NVIDIA RTX A6000\r\nGPU 7: NVIDIA RTX A6000\r\n\r\nNvidia driver version: 545.23.08\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8.7.0\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.7.0\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.7.0\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.7.0\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.7.0\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.7.0\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.7.0\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn.so.8.9.2\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn.so.9.1.1\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.2\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.2\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.2\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.2\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.2\r\n/usr/local/cuda-12.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn.so.9.1.1\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.9.2\r\n/usr/local/cuda-12.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.9.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              1\r\nModel name:                         AMD EPYC 7643 48-Core Processor\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU MHz:                            2770.121\r\nCPU max MHz:                        2300.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           4600.13\r\nVirtualization:                     AMD-V\r\nL1d cache:                          3 MiB\r\nL1i cache:                          3 MiB\r\nL2 cache:                           48 MiB\r\nL3 cache:                           512 MiB\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.1\r\n[pip3] triton==2.3.0\r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] transformers              4.41.1                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV4     SYS     SYS     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU1    NV4      X      SYS     SYS     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU2    SYS     SYS      X      NV4     SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU3    SYS     SYS     NV4      X      SYS     SYS     SYS     SYS     0-47,96-143     0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      NV4     SYS     SYS     48-95,144-191   1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     NV4      X      SYS     SYS     48-95,144-191   1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     48-95,144-191   1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      48-95,144-191   1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n### üêõ Describe the bug\n\nI set up the openai api server using\r\n```bash\r\npython -m vllm.entrypoints.openai.api_server \\\r\n    --model meta-llama/Llama-2-7b-hf \\\r\n    --max-logprobs 100 \\\r\n    --host 0.0.0.0 \\\r\n    --port xxxx\r\n```\r\nThen I try to get the logprobs of of tokens in a given prompt by setting `logprobs=1` and `echo=True` using the following code\r\n\r\n\r\n```Python\r\nfrom openai import OpenAI\r\nmodel = \"meta-llama/Llama-2-7b-hf\"\r\nopenai_api_key = \"EMPTY\"\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=HOST_DICT[model],\r\n)\r\nlogprobs = 1\r\nprompt = \"\\n1. Carol has 20\"\r\ncompletion = client.completions.create(model=model, \r\n                                       prompt=prompt,\r\n                                       max_tokens=5,\r\n                                       temperature=0,\r\n                                       logprobs=logprobs,\r\n                                       echo=True)\r\nprint(\"vLLM Completion text:\", completion.choices[0].text)\r\nprint(\"vLLM tokens:\", completion.choices[0].logprobs.tokens)\r\nprint(\"vLLM Completion logprobs:\", completion.choices[0].logprobs)\r\n```\r\nThe generated texts look great, but the logprobs seem to be really strange:\r\n```bash\r\nvLLM Completion text: \r\n1. Carol has 2000 books in her\r\nvLLM tokens: ['<s>', '', '\\n', '1', '.', '\\n Carol', '\\n\\n has', '\\n\\n\\n ', '\\n\\n\\n\\n2', '\\n\\n\\n\\n\\n0', '0', '0', ' books', ' in', ' her']\r\nvLLM Completion logprobs: Logprobs(text_offset=[0, 3, 3, 4, 5, 6, 13, 19, 23, 28, 34, 35, 36, 42, 45], token_logprobs=[None, -4.1065497398376465, -4.179874420166016, -5.703912258148193, -0.9323832988739014, -10.607088088989258, -6.339632987976074, -4.3729047775268555, -1.8068292140960693, -1.6547526121139526, -2.1797218322753906, -2.6753900051116943, -2.8350002765655518, -1.0952296257019043, -0.1524326652288437], tokens=['<s>', '', '\\n', '1', '.', '\\n Carol', '\\n\\n has', '\\n\\n\\n ', '\\n\\n\\n\\n2', '\\n\\n\\n\\n\\n0', '0', '0', ' books', ' in', ' her'], top_logprobs=[None, {'': -4.1065497398376465, 'Tags': -2.5245184898376465}, {'\\n': -4.179874420166016, '1': -1.4376866817474365}, {'1': -5.703912258148193, '\\n': -1.1257872581481934}, {'.': -0.9323832988739014}, {'\\n Carol': -10.607088088989258, '\\n The': -2.757479190826416}, {'\\n\\n has': -6.339632987976074, '\\n\\nyn': -1.4314298629760742}, {'\\n\\n\\n ': -4.3729047775268555, '\\n\\n\\n a': -1.7049362659454346}, {'\\n\\n\\n\\n2': -1.8068292140960693, '\\n\\n\\n\\n1': -1.3771417140960693}, {'\\n\\n\\n\\n\\n0': -1.6547526121139526}, {'0': -2.1797218322753906}, {'0': -2.6753900051116943}, {' books': -2.8350002765655518}, {' in': -1.0952296257019043}, {' her': -0.1524326652288437}])\r\n```\r\nIt returns \"tokens\" like `'\\n\\n\\n\\n2'` , which is neither in the original prompt or even a single token in the llama-2 vocabulary. On the other hand, I tried using some other AI provider (fireworks) and the behavior is expected:\r\n```Python\r\nfrom openai import OpenAI\r\n\r\nlogprobs = 1\r\nwith open(\"fireworks-key.txt\", \"r\") as f:\r\n    fireworks_key = f.read().strip()\r\n\r\nclient_fireworks = OpenAI(\r\n    base_url = \"https://api.fireworks.ai/inference/v1\",\r\n    api_key=fireworks_key,\r\n)\r\nprompt = \"\\n1. Carol has 20\"\r\n\r\ncompletion = client_fireworks.completions.create(model=\"accounts/fireworks/models/llama-v2-7b\",\r\n                                                    prompt=prompt,\r\n                                                    max_tokens=10,\r\n                                                    temperature=0,\r\n                                                    logprobs=logprobs,\r\n                                                    echo=True)\r\nprint(\"Fireworks Completion text:\", completion.choices[0].text)\r\nprint(\"Fireworks tokens:\", completion.choices[0].logprobs.tokens)\r\nprint(\"Fireworks Completion logprobs:\", completion.choices[0].logprobs)\r\n```\r\nOutputs:\r\n```bash\r\nFireworks Completion text:  \r\n1. Carol has 2000 books in her library. She has \r\nFireworks tokens: ['', ' ', '\\n', '1', '.', ' Carol', ' has', ' ', '2', '0', '0', '0', ' books', ' in', ' her', ' library', '.', ' She', ' has', ' ']\r\nFireworks Completion logprobs: Logprobs(text_offset=[0, 0, 1, 2, 3, 4, 10, 14, 15, 16, 17, 18, 19, 25, 28, 32, 40, 41, 45, 49], token_logprobs=[0.0, -3.16015625, -9.4765625, -5.703125, -0.92675781, -10.6015625, -6.33984375, -4.375, -1.80371094, -1.65429688, -2.17938995, -2.67908955, -2.83534431, -1.09457016, -0.15235297, -0.66951698, -0.24534534, -1.22056556, -1.91010427, -1.18275023], tokens=['', ' ', '\\n', '1', '.', ' Carol', ' has', ' ', '2', '0', '0', '0', ' books', ' in', ' her', ' library', '.', ' She', ' has', ' '], top_logprobs=[{' ‚Åá ': 0.0}, {' Tags': -2.546875}, {'1': -1.43945312}, {'\\n': -1.12597656}, {'.': -0.92675781}, {' The': -2.75976562}, {'yn': -1.43457031}, {' a': -1.70605469}, {'1': -1.38183594}, {'0': -1.65429688}, {'0': -2.17938995}, {'0': -2.67908955}, {' books': -2.83534431}, {' in': -1.09457016}, {' her': -0.15235297}, {' library': -0.66951698}, {'.': -0.24534534}, {' She': -1.22056556}, {' has': -1.91010427}, {' ': -1.18275023}], token_ids=[1, 29871, 13, 29896, 29889, 8562, 756, 29871, 29906, 29900, 29900, 29900, 8277, 297, 902, 3489, 29889, 2296, 756, 29871])\r\n```\r\nI tried llama-3 using vLLM and it works correctly, why is this happening to Llama-2 (I tried other sizes of Llama-2 and all have this problem)?",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-07T04:29:47Z",
    "closed_at": "2024-07-11T22:02:31Z",
    "author": "fywalter",
    "comments_count": 2,
    "comments": [
      {
        "author": "PastelBelem8",
        "body": "I have noticed the same issue. It's been preventing me from using vLLM on my day to day work. ",
        "created_at": "2024-06-07T04:58:47Z"
      },
      {
        "author": "fywalter",
        "body": "I think this problem is also related to #4772. After some investigation, I found the decoded token ids are correct for the prompt tokens while the decoded tokens seem to be wrong. Since the generated tokens behave correctly, I tried to fix the problem by replacing the code for processing the prompt tokens with the code to process generated tokens and this fixed the problem:\r\nSpecifically I modified `decode_prompt_logprobs_inplace` in `/vllm/transformers_utils/detokenizer.py`:\r\n```python\r\n def decode_prompt_logprobs_inplace(\r\n            self, seq_group: SequenceGroup,\r\n            prompt_logprobs: List[Optional[Dict[int, Logprob]]]) -> None:\r\n        \"\"\"Decodes the logprobs for the prompt of a sequence group.\r\n\r\n        Args:\r\n            seq_group: The sequence group to decode.\r\n            prompt_logprobs: The logprobs to decode.\r\n        \r\n        Returns:\r\n            The prompt logprobs with the decoded tokens.\r\n        \"\"\"\r\n        prms = seq_group.sampling_params\r\n        # We can pick any sequence for the prompt.\r\n        seq = next(iter(seq_group.seqs_dict.values()))\r\n        # Only prompt, without the generated token.\r\n        all_token_ids = seq.get_token_ids()\r\n        prompt_token_ids = all_token_ids[:-1]\r\n        tokenizer = self.get_tokenizer_for_seq(seq)\r\n        prefix_offset = 0\r\n        read_offset = 0\r\n        next_iter_prefix_offset = 0\r\n        next_iter_read_offset = 0\r\n        next_iter_tokens = []\r\n        prev_tokens = None\r\n        for token_position, prompt_logprobs_for_token in enumerate(\r\n                prompt_logprobs):\r\n            if not prompt_logprobs_for_token:\r\n                continue\r\n            for token_id, sample_logprob in prompt_logprobs_for_token.items():\r\n                if (sample_logprob.decoded_token is None\r\n                        and token_id != INVALID_TOKEN_ID):\r\n                    ###########################################\r\n                    # debug for llama 2\r\n                    # the prompt logprobs are incorrect for llama 2 models\r\n                    \r\n                    # +++++++++++++++++++++++++++++++++++++++++\r\n                    # original code\r\n                    # prompt_token_ids_with_token = (\r\n                    #     prompt_token_ids[:token_position] + [token_id])\r\n                    # +++++++++++++++++++++++++++++++++++++++++\r\n\r\n                    # ========================================\r\n                    # attempt to fix the bug\r\n                    # looks like for llama 2, the bos token is not included in the prompt_token_ids\r\n                    # and the prev_tokens and offsets are not correct, since the generated tokens look correct \r\n                    # use the same code as in decode_sequence_inplace\r\n                    \r\n                    if \"llama-2\" in tokenizer.name_or_path.lower():\r\n                        prompt_token_ids_with_token = (\r\n                            prompt_token_ids[:token_position+1] + [token_id])   # include the bos token\r\n                        (prev_tokens, prefix_offset,\r\n                            read_offset) = convert_prompt_ids_to_tokens(\r\n                            tokenizer=tokenizer,\r\n                            prompt_ids=prompt_token_ids[:token_position+1],     # include the bos token\r\n                            skip_special_tokens=prms.skip_special_tokens,\r\n                        )\r\n                    else:\r\n                        prompt_token_ids_with_token = (\r\n                            prompt_token_ids[:token_position] + [token_id])\r\n                    # ========================================\r\n                    # debug\r\n                    # print(f\"prompt_token_ids_with_token: {prompt_token_ids_with_token}\")\r\n                    # import pdb; pdb.set_trace()\r\n                    ###########################################\r\n                    (new_tokens, new_text, new_prefix_offset,\r\n                     new_read_offset) = detokenize_incrementally(\r\n                         tokenizer=tokenizer,\r\n                         all_input_ids=prompt_token_ids_with_token,\r\n                         prev_tokens=prev_tokens,\r\n                         prefix_offset=prefix_offset,\r\n                         read_offset=read_offset,\r\n                         skip_special_tokens=prms.skip_special_tokens,\r\n                         spaces_between_special_tokens=prms.\r\n                         spaces_between_special_tokens,\r\n                     )\r\n                    # import pdb; pdb.set_trace()\r\n                    sample_logprob.decoded_token = new_text\r\n\r\n                    # Use the offsets & prev tokens corresponding to\r\n                    # real tokens to ensure detokenization is consistent\r\n                    # actual with prompt.\r\n                    if token_id == all_token_ids[token_position]:\r\n                        next_iter_prefix_offset = new_prefix_offset\r\n                        next_iter_read_offset = new_read_offset\r\n                        next_iter_tokens = new_tokens\r\n\r\n            # Advance to the next token position.\r\n            prefix_offset = next_iter_prefix_offset\r\n            read_offset = next_iter_read_offset\r\n            if prev_tokens is None:\r\n                prev_tokens = next_iter_tokens\r\n            else:\r\n                prev_tokens.extend(next_iter_tokens)\r\n\r\n```\r\nAfter fixing the bug:\r\n![image](https://github.com/vllm-project/vllm/assets/37759111/0c107caa-ed3a-430a-b0f9-23e792036056)\r\n\r\nIt is not exactly clear why to use different methods for processing prompt and generated tokens. Should I start a pull request?\r\n\r\n",
        "created_at": "2024-06-24T18:57:45Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8923": {
    "issue_number": 8923,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8923",
    "title": "[Bug]: Model multimodal config initialisation unhandled and irrelevant error when no architectures found",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: macOS 15.0 (arm64)\r\nGCC version: Could not collect\r\nClang version: 16.0.0 (clang-1600.0.26.3)\r\nCMake version: Could not collect\r\nLibc version: N/A\r\n\r\nPython version: 3.12.5 (main, Aug  6 2024, 19:08:49) [Clang 15.0.0 (clang-1500.3.9.4)] (64-bit runtime)\r\nPython platform: macOS-15.0-arm64-arm-64bit\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: Could not collect\r\nNvidia driver version: Could not collect\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nApple M2 Pro\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nCould not collect\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### üêõ Describe the bug\n\nWhen initialising multimodal model config, if no architectures are found, an unhandled error happens and a not helpful error is show.\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-28T07:10:42Z",
    "closed_at": "2024-10-04T06:33:34Z",
    "author": "AminAlam",
    "comments_count": 1,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Closing as resolved by #7168",
        "created_at": "2024-10-04T06:33:33Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4904": {
    "issue_number": 4904,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4904",
    "title": "[Bug]: llm_engine_example.py (more requests) get stuck",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...                                                                                 \r\nPyTorch version: 2.3.0+cu121                                                                                          \r\nIs debug build: False                                                                                                 \r\nCUDA used to build PyTorch: 12.1                                                                                      \r\nROCM used to build PyTorch: N/A                                                                                       \r\n                                                                                                                      \r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)                                                                           \r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110                                                                        \r\nClang version: Could not collect                                                                                      \r\nCMake version: version 3.29.2                                                                                         \r\nLibc version: glibc-2.31                                                                                              \r\n                                                                                                                      \r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)                                    \r\nPython platform: Linux-5.10.0-23-amd64-x86_64-with-glibc2.31                                                          \r\nIs CUDA available: True                                                                                               \r\nCUDA runtime version: 12.1.66                                                                                         \r\nCUDA_MODULE_LOADING set to: LAZY                                                                                      \r\nGPU models and configuration:                                                                                         \r\nGPU 0: NVIDIA RTX A5000                                                                                               \r\nGPU 1: NVIDIA RTX A5000                                                                                               \r\nGPU 2: NVIDIA RTX A5000                                                                                               \r\nGPU 3: NVIDIA RTX A5000                                                                                               \r\n                                                                                                                      \r\nNvidia driver version: 545.23.08                                                                                      \r\ncuDNN version: Could not collect                                                                                      \r\nHIP runtime version: N/A                                                                                              \r\nMIOpen runtime version: N/A                                                                                           \r\nIs XNNPACK available: True                                                                                            \r\n                                                                                                                      \r\nCPU:                                                                                                                  \r\nArchitecture:                    x86_64                                                                               \r\nCPU op-mode(s):                  32-bit, 64-bit                                                                       \r\nByte Order:                      Little Endian                                                                        \r\nAddress sizes:                   43 bits physical, 48 bits virtual                                                    \r\nCPU(s):                          32                                                                                   \r\nOn-line CPU(s) list:             0-31                                                                                 \r\nThread(s) per core:              1                                                                                    \r\nCore(s) per socket:              16                                                                                   \r\nSocket(s):                       2                                                                                    \r\nNUMA node(s):                    8                                                                                    \r\nVendor ID:                       AuthenticAMD                            \r\nCPU family:                      23\r\nModel:                           49\r\nModel name:                      AMD EPYC 7302 16-Core Processor\r\nStepping:                        0\r\nFrequency boost:                 enabled\r\nCPU MHz:                         1730.206\r\nCPU max MHz:                     3310.5459\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        5988.84\r\nVirtualization:                  AMD-V\r\nL1d cache:                       1 MiB\r\nL1i cache:                       1 MiB\r\nL2 cache:                        16 MiB\r\nL3 cache:                        256 MiB\r\nNUMA node0 CPU(s):               0-3\r\nNUMA node1 CPU(s):               4-7\r\nNUMA node2 CPU(s):               8-11\r\nNUMA node3 CPU(s):               12-15\r\nNUMA node4 CPU(s):               16-19\r\nNUMA node5 CPU(s):               20-23\r\nNUMA node6 CPU(s):               24-27\r\nNUMA node7 CPU(s):               28-31\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT disabled\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP disabled, RSB filling, PBRSB-eIBRS No\r\nt affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx f\r\nxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid \r\naperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legac\r\ny svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb b\r\npext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall sev_es fsgsbase bmi1 avx2\r\n smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_tota\r\nl cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbya\r\nsid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     SYS     SYS     12-15           N/A             N/A\r\nGPU1    SYS      X      SYS     SYS     SYS     8-11    2               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     28-31           N/A             N/A\r\nGPU3    SYS     SYS     SYS      X      SYS     20-23   5               N/A\r\nNIC0    SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n```\r\n\n\n### üêõ Describe the bug\n\nI modified `examples/llm_engine_example.py` to test a large number of requests. With 200 requests of 32 random tokens, the engine get stuck and never produce a full answer.\r\n\r\n`llm_engine_example_heavy.py`:\r\n```python\r\nimport argparse\r\nfrom typing import List, Tuple\r\nfrom vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\r\nimport numpy as np\r\n\r\n\r\n\r\ndef create_test_prompt_nonsense(args: argparse.Namespace) -> List[Tuple[str, SamplingParams]]:\r\n    length = args.input_len\r\n    return (np.random.randint(10000, size=(length)).tolist(),\r\n            SamplingParams(temperature=0.0, logprobs=1, prompt_logprobs=1))\r\n\r\n\r\ndef process_requests(engine: LLMEngine,\r\n                     test_prompts: List[Tuple[str, SamplingParams]]):\r\n    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\r\n    request_id = 0\r\n\r\n    while test_prompts or engine.has_unfinished_requests():\r\n        if test_prompts:\r\n            prompt_token_ids, sampling_params = test_prompts.pop(0)\r\n            engine.add_request(str(request_id), None, sampling_params, prompt_token_ids)\r\n            request_id += 1\r\n\r\n        request_outputs: List[RequestOutput] = engine.step()\r\n\r\n        for request_output in request_outputs:\r\n            if request_output.finished:\r\n                print(request_output)\r\n\r\n\r\ndef initialize_engine(args: argparse.Namespace) -> LLMEngine:\r\n    \"\"\"Initialize the LLMEngine from the command line arguments.\"\"\"\r\n    engine_args = EngineArgs.from_cli_args(args)\r\n    return LLMEngine.from_engine_args(engine_args)\r\n\r\n\r\ndef main(args: argparse.Namespace):\r\n    \"\"\"Main function that sets up and runs the prompt processing.\"\"\"\r\n    engine = initialize_engine(args)\r\n    test_prompts = []\r\n    for i in range(args.test_num):\r\n        test_prompts.append(create_test_prompt_nonsense(args))\r\n    process_requests(engine, test_prompts)\r\n\r\n\r\nif __name__ == '__main__':\r\n    parser = argparse.ArgumentParser(\r\n        description='Demo on using the LLMEngine class directly')\r\n    parser = EngineArgs.add_cli_args(parser)\r\n    parser.add_argument('--input-len', type=int, default=128)\r\n    parser.add_argument('--test-num', type=int, default=100)\r\n    args = parser.parse_args()\r\n    main(args)\r\n```\r\n\r\nA successful run:\r\n```bash\r\npython llm_engine_example_heavy.py --model facebook/opt-125m \\\r\n --input-len 32 \\\r\n --test-num 10\r\n```\r\noutput:\r\n```\r\nINFO 05-18 22:55:28 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='facebook/opt-125m', spe\r\nculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, to\r\nkenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=\r\nLoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_c\r\nache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_back\r\nend='outlines'), seed=0, served_model_name=facebook/opt-125m)\r\nINFO 05-18 22:55:31 selector.py:37] Using FlashAttention-2 backend.\r\nINFO 05-18 22:55:31 weight_utils.py:199] Using model weights format ['*.bin']\r\nINFO 05-18 22:55:31 model_runner.py:145] Loading model weights took 0.2389 GB\r\nINFO 05-18 22:55:32 gpu_executor.py:83] # GPU blocks: 36865, # CPU blocks: 7281\r\nINFO 05-18 22:55:34 model_runner.py:824] Capturing the model for CUDA graphs. This may lead to unexpected consequences\r\n if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the \r\nCLI.\r\nINFO 05-18 22:55:34 model_runner.py:828] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running ou\r\nt of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_se\r\nqs` as needed to decrease memory usage.\r\nINFO 05-18 22:55:37 model_runner.py:894] Graph capturing finished in 3 secs.\r\nRequestOutput(request_id=0, prompt=None, prompt_token_ids=[2732, 9845, 3264, 4859, 9225, 7891, 4373, 5874, 6744, 3468,\r\n 705, 2599, 2222, 7768, 2897, 9893, 537, 6216, 6921, 6036, 2163, 5072, 4851, 7877, 2046, 1871, 7599, 2496, 8291, 755, \r\n797, 659], prompt_logprobs=[None, {9845: Logprob(logprob=-12.68840217590332, rank=7125, decoded_token=' emerge'), 4: L\r\nogprob(logprob=-1.9188706874847412, rank=1, decoded_token='.')}, {3264: Logprob(logprob=-14.21910285949707, rank=6125,\r\n decoded_token=' accept'), 31: Logprob(logprob=-1.1546498537063599, rank=1, decoded_token=' from')}, {4859: Logprob(lo\r\ngprob=-12.61778450012207, rank=4317, decoded_token=' buyers'), 4735: Logprob(logprob=-0.4537220001220703, rank=1, deco\r\nded_token='ably')}, {9225: Logprob(logprob=-16.543548583984375, rank=25278, decoded_token=' Kurdish'), \r\n```\r\n...\r\n\r\nI omitted the rest. All answers are printed. The program terminated normally.\r\n\r\nA failed run, change `--test-num` from 10 to 200:\r\n```bash\r\npython llm_engine_example_heavy.py --model facebook/opt-125m \\\r\n --input-len 32 \\\r\n --test-num 200\r\n```\r\noutput:\r\n```\r\nINFO 05-18 22:58:38 llm_engine.py:103] Initializing an LLM engine (v0.4.2) with config: model='facebook/opt-125m', speculative_config=None, tokenizer='facebook/opt-125m', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=facebook/opt-125m)\r\nINFO 05-18 22:58:42 selector.py:37] Using FlashAttention-2 backend.\r\nINFO 05-18 22:58:43 weight_utils.py:199] Using model weights format ['*.bin']\r\nINFO 05-18 22:58:43 model_runner.py:145] Loading model weights took 0.2389 GB\r\nINFO 05-18 22:58:43 gpu_executor.py:83] # GPU blocks: 36865, # CPU blocks: 7281\r\nINFO 05-18 22:58:45 model_runner.py:824] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 05-18 22:58:45 model_runner.py:828] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 05-18 22:58:48 model_runner.py:894] Graph capturing finished in 3 secs.\r\n^C[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/[my workpath]/vllm/examples/llm_engine_example_heavy.py\", line 54, in <module>\r\n[rank0]:     main(args)\r\n[rank0]:   File \"/[my workpath]/vllm/examples/llm_engine_example_heavy.py\", line 44, in main\r\n[rank0]:     process_requests(engine, test_prompts)\r\n[rank0]:   File \"/[my workpath]/vllm/examples/llm_engine_example_heavy.py\", line 25, in process_requests\r\n[rank0]:     request_outputs: List[RequestOutput] = engine.step()\r\n[rank0]:                                            ^^^^^^^^^^^^^\r\n[rank0]:   File \"/[my workpath]/vllm/vllm/engine/llm_engine.py\", line 686, in step\r\n[rank0]:     request_outputs = self._process_model_outputs(\r\n[rank0]:                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/[my workpath]/vllm/vllm/engine/llm_engine.py\", line 599, in _process_model_outputs\r\n[rank0]:     self.output_processor.process_prompt_logprob(seq_group, outputs)\r\n[rank0]:   File \"/[my workpath]/vllm/vllm/engine/output_processor/single_step.py\", line 65, in process_prompt_logprob\r\n[rank0]:     self.detokenizer.decode_prompt_logprobs_inplace(\r\n[rank0]:   File \"/[my workpath]/vllm/vllm/transformers_utils/detokenizer.py\", line 60, in decode_prompt_logprobs_inplace\r\n[rank0]:     new_read_offset) = detokenize_incrementally(\r\n[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/[my workpath]/vllm/vllm/transformers_utils/detokenizer.py\", line 287, in detokenize_incrementally\r\n[rank0]:     prefix_text = tokenizer.convert_tokens_to_string(\r\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/[my conda env path]/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py\", line 612, in convert_tokens_to_string\r\n[rank0]:     return self.backend_tokenizer.decoder.decode(tokens)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: KeyboardInterrupt\r\n```\r\n\r\nBefore I hit ctrl+c, the program stuck for 1 hour. The GPU activiy is 0%. \r\nThe traceback always show `self.backend_tokenizer.decoder.decode(tokens)` as the latest position.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-05-19T03:06:44Z",
    "closed_at": "2024-07-11T22:02:30Z",
    "author": "CsRic",
    "comments_count": 1,
    "comments": [
      {
        "author": "zifeitong",
        "body": "Minimal reproducer:\r\n\r\n```python\r\nimport numpy as np\r\n\r\nfrom vllm import LLM, SamplingParams\r\n\r\n\r\ndef main():\r\n    llm = LLM(model=\"facebook/opt-125m\")\r\n    test_prompts = np.random.randint(10000, size=(200, 32)).tolist()\r\n    outputs = llm.generate(\r\n        prompt_token_ids=test_prompts,\r\n        sampling_params=SamplingParams(\r\n            temperature=0.0, logprobs=1, prompt_logprobs=1\r\n        ),\r\n    )\r\n\r\n    for output in outputs:\r\n        print(output)\r\n\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n```\r\n\r\nIt's affecting using `EleutherAI/lm-evaluation-harness` with vLLM.",
        "created_at": "2024-05-30T19:41:32Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4362": {
    "issue_number": 4362,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4362",
    "title": "[Bug]: v0.4.1 VLLM_USE_MODELSCOPE not working",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nthe related code in the previous version :\r\n\r\n`if os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\":`\r\n\r\nnow the code:\r\n`if VLLM_USE_MODELSCOPE:`\r\nbut the `VLLM_USE_MODELSCOPE` is set to string \"true\" by\r\n```\r\nVLLM_USE_MODELSCOPE = os.environ.get(\"VLLM_USE_MODELSCOPE\",\r\n                                     \"False\").lower() == \"true\"\r\n```\r\nso, VLLM_USE_MODELSCOPE will never work",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-25T10:48:50Z",
    "closed_at": "2024-06-06T16:28:12Z",
    "author": "kratorado",
    "comments_count": 8,
    "comments": [
      {
        "author": "simon-mo",
        "body": "sorry why would it never work? if the env var is set `VLLM_USE_MODELSCOPE=true` then the statement would evaluate to `true`? ",
        "created_at": "2024-04-25T20:14:44Z"
      },
      {
        "author": "kratorado",
        "body": "> sorry why would it never work? if the env var is set `VLLM_USE_MODELSCOPE=true` then the statement would evaluate to `true`?\r\n\r\nah, I reread the code and found that I made a wrong debugging result. \r\nActually what I met is that when I run docker image `vllm/vllm-openai:v0.4.1` , passing env with `--env \"VLLM_USE_MODELSCOPE=True\" `, vllm still visits huggingface.co to download models. It seems that vllm doesnot take this env var into account.",
        "created_at": "2024-04-26T02:58:49Z"
      },
      {
        "author": "kratorado",
        "body": "My temp workaround is adding the origin snippet to vllm/config.py line 106:\r\n```python\r\n...\r\n        if VLLM_USE_MODELSCOPE:\r\n            # download model from ModelScope hub,\r\n            # lazy import so that modelscope is not required for normal use.\r\n            # pylint: disable=C.\r\n            from modelscope.hub.snapshot_download import snapshot_download\r\n\r\n            if not os.path.exists(model):\r\n                model_path = snapshot_download(model_id=model, revision=revision)\r\n            else:\r\n                model_path = model\r\n...\r\n```",
        "created_at": "2024-04-26T03:43:15Z"
      },
      {
        "author": "simon-mo",
        "body": "can you send a PR for what worked for you? ",
        "created_at": "2024-04-26T05:45:08Z"
      },
      {
        "author": "kratorado",
        "body": "Sending a PR is easy. But my workaround is reverting some changes of a refactor which I think is the cause, I have to read the whole refactored code to confirm what to do is best.\r\nrelated PR https://github.com/vllm-project/vllm/pull/4097",
        "created_at": "2024-04-26T05:57:13Z"
      },
      {
        "author": "kratorado",
        "body": "`vllm/config.py` , line 107\r\n```python\r\nself.hf_config = get_config(self.model, trust_remote_code, revision,\r\n                                    code_revision)\r\n```\r\ncalls `vllm/transformers_utils/config.py` , line 23,\r\n\r\n```python\r\n        config = AutoConfig.from_pretrained(\r\n            model,\r\n            trust_remote_code=trust_remote_code,\r\n            revision=revision,\r\n            code_revision=code_revision)\r\n```\r\nThe code above does not check the environment variable `VLLM_USE_MODELSCOPE` and it will download config file from huggingface.co by default.\r\n\r\nHave no clue how to fix it **elegantly** now.",
        "created_at": "2024-04-26T17:07:17Z"
      },
      {
        "author": "dashi6174",
        "body": "My friendsÔºåhow to fix it  ?\r\n",
        "created_at": "2024-04-28T14:47:38Z"
      },
      {
        "author": "kratorado",
        "body": "> My friendsÔºåhow to fix it ?\r\n\r\ninsert the code, at the line 106 of the file `vllm/config`. before `self.hf_config = get_config(self.model, trust_remote_code, revision, code_revision)`\r\n```python\r\n        if VLLM_USE_MODELSCOPE:\r\n            from modelscope.hub.snapshot_download import snapshot_download\r\n\r\n            if not os.path.exists(model):\r\n                model_path = snapshot_download(model_id=model,\r\n                                               revision=revision)\r\n            else:\r\n                model_path = model\r\n            self.model = model_path\r\n            self.download_dir = model_path\r\n            self.tokenizer = model_path\r\n```\r\n\r\nRemember it is not the best way, just a temp workaround.",
        "created_at": "2024-04-29T04:28:37Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7149": {
    "issue_number": 7149,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7149",
    "title": "[Bug]: The deployment function requires a divisible relationship, but the model structure does not meet this requirement. What should I do?",
    "body": "\r\n\r\n### üêõ Describe the bug\r\n\r\ndef get_siglip_patch_grid_length(*, image_size: int, patch_size: int) -> int:\r\n    assert image_size % patch_size == 0\r\n    return image_size // patch_size\r\n    \r\n<html>\r\n<body>\r\n<!--StartFragment-->\r\n\"vision_config\": {\r\n--\r\n¬† | \"hidden_act\": \"gelu_pytorch_tanh\",\r\n¬† | \"hidden_size\": 1152,\r\n¬† | \"image_size\": 384,\r\n¬† | \"intermediate_size\": 4304,\r\n¬† | \"layer_norm_eps\": 1e-06,\r\n¬† | \"model_type\": \"siglip_vision_model\",\r\n¬† | \"num_attention_heads\": 16,\r\n¬† | \"num_hidden_layers\": 27,\r\n¬† | \"patch_size\": 14\r\n\r\n<!--EndFragment-->\r\n</body>\r\n</html>",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-05T08:01:35Z",
    "closed_at": "2024-08-06T08:55:32Z",
    "author": "BrenchCC",
    "comments_count": 9,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Which HF repo are you using?",
        "created_at": "2024-08-05T08:27:42Z"
      },
      {
        "author": "BrenchCC",
        "body": "TIGER-Lab/Mantis-8B-siglip-llama3\nPerhaps I should write a new Python script to support this model.",
        "created_at": "2024-08-05T08:30:35Z"
      },
      {
        "author": "DarkLight1337",
        "body": "It should only take a small modification since the architecture is the same as `LlavaForConditionalGeneration`. We just need to support loading the SigLIP model inside vLLM.",
        "created_at": "2024-08-05T08:34:18Z"
      },
      {
        "author": "BrenchCC",
        "body": "> It should only take a small modification since the architecture is the same as `LlavaForConditionalGeneration`. We just need to support loading the SigLIP model inside vLLM.\n\nI have add the SigLIP to `LlavaForConditionalGeneration`, but the SigLIP need to get grid_length, which requires a divisible relationship. In Mantis, the `LlavaForConditionalGeneration` have been rewrite.",
        "created_at": "2024-08-05T08:46:07Z"
      },
      {
        "author": "DarkLight1337",
        "body": "I'm working on a PR to add SigLIP support to LLaVA architecture.",
        "created_at": "2024-08-05T08:50:34Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> > It should only take a small modification since the architecture is the same as `LlavaForConditionalGeneration`. We just need to support loading the SigLIP model inside vLLM.\r\n>\r\n> I have add the SigLIP to `LlavaForConditionalGeneration`, but the SigLIP need to get grid_length, which requires a divisible relationship. In Mantis, the `LlavaForConditionalGeneration` have been rewrite.\r\n\r\nI think the model is still the same (since the HuggingFace repo didn't supply its own `modeling_` file), just that a different vision encoder is being loaded.",
        "created_at": "2024-08-05T08:50:59Z"
      },
      {
        "author": "BrenchCC",
        "body": "Thank for your reply.\n[https://github.com/TIGER-AI-Lab/Mantis/blob/main/mantis/models/mllava/modeling_llava.py](https://github.com/TIGER-AI-Lab/Mantis/blob/main/mantis/models/mllava/modeling_llava.py) This is the `modeling_` file. I will continue to find how to solve this problem.",
        "created_at": "2024-08-05T08:57:20Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> Thank for your reply. [https://github.com/TIGER-AI-Lab/Mantis/blob/main/mantis/models/mllava/modeling_llava.py](https://github.com/TIGER-AI-Lab/Mantis/blob/main/mantis/models/mllava/modeling_llava.py) This is the `modeling_` file. I will continue to find how to solve this problem.\n\nOh, I didn't check the repo on GitHub. Thanks for pointing that out! I guess you can work on implementing it then.",
        "created_at": "2024-08-05T09:09:09Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Can you try out #7153 and see if vLLM can run the model without modification? There is a chance this could work since that particular model uses `LlavaForConditionalGeneration`, not `MLlavaForConditionalGeneration`.",
        "created_at": "2024-08-05T09:15:16Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4883": {
    "issue_number": 4883,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4883",
    "title": "[Bug]: assert parts[0] == \"base_model\" AssertionError",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (conda-forge gcc 13.2.0-7) 13.2.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-105-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 3090\r\nGPU 1: NVIDIA GeForce RTX 3090\r\nGPU 2: NVIDIA GeForce RTX 3090\r\nGPU 3: NVIDIA GeForce RTX 3090\r\n\r\nNvidia driver version: 535.171.04\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             88\r\nOn-line CPU(s) list:                0-87\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz\r\nCPU family:                         6\r\nModel:                              79\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 22\r\nSocket(s):                          2\r\nStepping:                           1\r\nCPU max MHz:                        3700.0000\r\nCPU min MHz:                        1200.0000\r\nBogoMIPS:                           4399.72\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\nVirtualization:                     VT-x\r\nL1d cache:                          1.4 MiB (44 instances)\r\nL1i cache:                          1.4 MiB (44 instances)\r\nL2 cache:                           11 MiB (44 instances)\r\nL3 cache:                           110 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-21,44-65\r\nNUMA node1 CPU(s):                  22-43,66-87\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm_nccl_cu12==2.18.1.0.4.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     SYS     SYS     0-21,44-65      0               N/A\r\nGPU1    PHB      X      SYS     SYS     0-21,44-65      0               N/A\r\nGPU2    SYS     SYS      X      PHB     22-43,66-87     1               N/A\r\nGPU3    SYS     SYS     PHB      X      22-43,66-87     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nCUDA_VISIBLE_DEVICES=0,1 python -m vllm.entrypoints.openai.api_server \\\r\n    --model /home/greatwall/app/edison/models/Qwen1.5-14B-Chat \\\r\n    --trust-remote-code \\\r\n    --served-model-name qwen14B \\\r\n    --max-model-len 4096 \\\r\n    --gpu-memory-utilization 0.9 \\\r\n    --enable-lora \\\r\n    --lora-modules lora1=/home/greatwall/app/edison/output/qwen1half-14b-chat/v65-20240515-143141/checkpoint-1110 \\\r\n    --host 0.0.0.0 \\\r\n    --port 8088 \\\r\n    --tensor-parallel-size 2 \\\r\n    --enforce-eager",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-05-17T08:57:24Z",
    "closed_at": "2024-06-10T11:38:51Z",
    "author": "Edisonwei54",
    "comments_count": 2,
    "comments": [
      {
        "author": "Edisonwei54",
        "body": "Traceback (most recent call last):\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 150, in _load_lora\r\n    lora = self._lora_model_cls.from_local_checkpoint(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/models.py\", line 246, in from_local_checkpoint\r\n    return cls.from_lora_tensors(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/models.py\", line 150, in from_lora_tensors\r\n    module_name, is_lora_a = parse_fine_tuned_lora_name(tensor_name)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/utils.py\", line 89, in parse_fine_tuned_lora_name\r\n    assert parts[0] == \"base_model\"\r\nAssertionError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 501, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    return fut.result()\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 475, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 221, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 110, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\"execute_model\",\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 326, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 146, in execute_method\r\n    raise e\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 137, in execute_method\r\n    return executor(*args, **kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 249, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 790, in execute_model\r\n    self.set_active_loras(lora_requests, lora_mapping)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 901, in set_active_loras\r\n    self.lora_manager.set_active_loras(lora_requests, lora_mapping)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 113, in set_active_loras\r\n    self._apply_loras(lora_requests)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 235, in _apply_loras\r\n    self.add_lora(lora)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 243, in add_lora\r\n    lora = self._load_lora(lora_request)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 162, in _load_lora\r\n    raise RuntimeError(\r\nRuntimeError: Loading lora /home/greatwall/app/edison/output/qwen1half-14b-chat/v65-20240515-143141/checkpoint-1110 failed\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 45, in _raise_exception_on_finish\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nINFO 05-17 08:38:11 async_llm_engine.py:154] Aborted request cmpl-8b4fdb5f840a472985d41587f7208686.\r\nINFO:     192.168.26.100:56198 - \"POST /v1/chat/completions HTTP/1.1\" 500 Internal Server Error\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 150, in _load_lora\r\n    lora = self._lora_model_cls.from_local_checkpoint(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/models.py\", line 246, in from_local_checkpoint\r\n    return cls.from_lora_tensors(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/models.py\", line 150, in from_lora_tensors\r\n    module_name, is_lora_a = parse_fine_tuned_lora_name(tensor_name)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/utils.py\", line 89, in parse_fine_tuned_lora_name\r\n    assert parts[0] == \"base_model\"\r\nAssertionError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 99, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 138, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 301, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 666, in generate\r\n    raise e\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 660, in generate\r\n    async for request_output in stream:\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 77, in __anext__\r\n    raise result\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 501, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    return fut.result()\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 475, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 221, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 110, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\"execute_model\",\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 326, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 146, in execute_method\r\n    raise e\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 137, in execute_method\r\n    return executor(*args, **kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 249, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 790, in execute_model\r\n    self.set_active_loras(lora_requests, lora_mapping)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 901, in set_active_loras\r\n    self.lora_manager.set_active_loras(lora_requests, lora_mapping)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 113, in set_active_loras\r\n    self._apply_loras(lora_requests)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 235, in _apply_loras\r\n    self.add_lora(lora)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 243, in add_lora\r\n    lora = self._load_lora(lora_request)\r\n  File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/vllm/lora/worker_manager.py\", line 162, in _load_lora\r\n    raise RuntimeError(\r\nRuntimeError: Loading lora /home/greatwall/app/edison/output/qwen1half-14b-chat/v65-20240515-143141/checkpoint-1110 failed",
        "created_at": "2024-05-17T09:05:23Z"
      },
      {
        "author": "Edisonwei54",
        "body": "@WoosukKwon \r\n@zhuohan123 \r\n\r\nhttps://github.com/vllm-project/vllm/pull/3177\r\nI see that this submission already supports Lora for Qwen2. What is the reason for it still not working? Is it due to Lora's issue?",
        "created_at": "2024-05-17T09:07:48Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8461": {
    "issue_number": 8461,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8461",
    "title": "[Bug]: Model architectures ['Qwen2AudioForConditionalGeneration'] are not supported for now.",
    "body": "### Your current environment\n\npip install+https://github.com/vllm-project/vllm.git\n\n### Model Input Dumps\n\n_No response_\n\n### üêõ Describe the bug\n\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/worker/worker.py\", line 183, in load_model\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     self.model_runner.load_model()\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 999, in load_model\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 19, in get_model\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 358, in load_model\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 170, in _initialize_model\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     model_class, _ = get_model_architecture(model_config)\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/model_executor/model_loader/utils.py\", line 39, in get_model_architecture\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     return ModelRegistry.resolve_model_cls(architectures)\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]   File \"/apps1/zhangfan/anaconda3/envs/new_swift/lib/python3.10/site-packages/vllm/model_executor/models/__init__.py\", line 178, in resolve_model_cls\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226]     raise ValueError(\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226] ValueError: Model architectures ['Qwen2AudioForConditionalGeneration'] are not supported for now. Supported architectures: ['AquilaModel', 'AquilaForCausalLM', 'BaiChuanForCausalLM', 'BaichuanForCausalLM', 'BloomForCausalLM', 'ChatGLMModel', 'ChatGLMForConditionalGeneration', 'CohereForCausalLM', 'DbrxForCausalLM', 'DeciLMForCausalLM', 'DeepseekForCausalLM', 'DeepseekV2ForCausalLM', 'ExaoneForCausalLM', 'FalconForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTJForCausalLM', 'GPTNeoXForCausalLM', 'InternLMForCausalLM', 'InternLM2ForCausalLM', 'JAISLMHeadModel', 'LlamaForCausalLM', 'LLaMAForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'QuantMixtralForCausalLM', 'MptForCausalLM', 'MPTForCausalLM', 'MiniCPMForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'OPTForCausalLM', 'OrionForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'PhiMoEForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen2VLForConditionalGeneration', 'RWForCausalLM', 'StableLMEpochForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'ArcticForCausalLM', 'XverseForCausalLM', 'Phi3SmallForCausalLM', 'MedusaModel', 'EAGLEModel', 'MLPSpeculatorPreTrainedModel', 'JambaForCausalLM', 'GraniteForCausalLM', 'MistralModel', 'Blip2ForConditionalGeneration', 'ChameleonForConditionalGeneration', 'FuyuForCausalLM', 'InternVLChatModel', 'LlavaForConditionalGeneration', 'LlavaNextForConditionalGeneration', 'LlavaNextVideoForConditionalGeneration', 'MiniCPMV', 'PaliGemmaForConditionalGeneration', 'Phi3VForCausalLM', 'PixtralForConditionalGeneration', 'QWenLMHeadModel', 'UltravoxModel', 'BartModel', 'BartForConditionalGeneration']\r\n(VllmWorkerProcess pid=544861) ERROR 09-13 18:22:53 multiproc_worker_utils.py:226] \n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-13T10:30:44Z",
    "closed_at": "2024-10-23T17:54:24Z",
    "author": "zhangfan-algo",
    "comments_count": 3,
    "comments": [
      {
        "author": "jeejeelee",
        "body": "See: https://github.com/vllm-project/vllm/issues/8394",
        "created_at": "2024-09-13T10:33:16Z"
      },
      {
        "author": "zhangfan-algo",
        "body": "We don‚Äôt support Qwen2Audio yetÔºü",
        "created_at": "2024-09-14T01:16:47Z"
      },
      {
        "author": "jeejeelee",
        "body": "> We don‚Äôt support Qwen2Audio yetÔºü\r\n\r\nYep",
        "created_at": "2024-09-14T02:07:28Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6952": {
    "issue_number": 6952,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6952",
    "title": "[Bug]: `RuntimeError: b_q_weight is not on GPU` CPU Offloading",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n\r\nCollecting environment information...\r\nPyTorch version: N/A\r\nIs debug build: N/A\r\nCUDA used to build PyTorch: N/A\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.6 (main, Jul 17 2024, 12:33:27) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-45-generic-x86_64-with-glibc2.35\r\nIs CUDA available: N/A\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: N/A\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4070 Laptop GPU\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: N/A\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      39 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          GenuineIntel\r\nModel name:                         13th Gen Intel(R) Core(TM) i9-13900HX\r\nCPU family:                         6\r\nModel:                              183\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          1\r\nStepping:                           1\r\nCPU max MHz:                        5400.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4838.40\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize arch_lbr ibt flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          896 KiB (24 instances)\r\nL1i cache:                          1.3 MiB (24 instances)\r\nL2 cache:                           32 MiB (12 instances)\r\nL3 cache:                           36 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] No relevant packages\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nI am getting the error and stack trace below when I turn on and adjust the `cpu_offload_gb` parameter for consumption by the vLLM `AsyncLLMEngine`. I have adjusted the parameter between several different offload numbers (2, 4, 8) but none have worked.\r\n\r\nAttached further at the bottom is the custom backend wrapper code used to serve vLLM. The code works, and vLLM is able to inference, when turning off (removing) `cpu_offload_gb`.\r\n\r\nI am using the following AWQ quantization of Phi-3-128k-instruct: `bsmit1659/Phi-3-mini-128k-instruct-0.2-awq`\r\n\r\n```bash\r\nInitializing Leapfrog\r\nINFO 07-30 15:18:35 awq_marlin.py:77] The model is convertible to awq_marlin during runtime. Using awq_marlin kernel.\r\nINFO 07-30 15:18:35 llm_engine.py:176] Initializing an LLM engine (v0.5.3) with config: model='.model/', speculative_config=None, tokenizer='.model/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=awq_marlin, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=.model/, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 07-30 15:18:35 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 07-30 15:18:35 selector.py:54] Using XFormers backend.\r\nINFO 07-30 15:18:37 model_runner.py:680] Starting to load model .model/...\r\nINFO 07-30 15:18:37 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 07-30 15:18:37 selector.py:54] Using XFormers backend.\r\nLoading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.91it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.91it/s]\r\n\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"<frozen runpy>\", line 198, in _run_module_as_main\r\n[rank0]:   File \"<frozen runpy>\", line 88, in _run_code\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/leapfrogai_sdk/cli.py\", line 41, in <module>\r\n[rank0]:     cli()  # pragma: no cover\r\n[rank0]:     ^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/click/core.py\", line 1157, in __call__\r\n[rank0]:     return self.main(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/click/core.py\", line 1078, in main\r\n[rank0]:     rv = self.invoke(ctx)\r\n[rank0]:          ^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/click/core.py\", line 1434, in invoke\r\n[rank0]:     return ctx.invoke(self.callback, **ctx.params)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/click/core.py\", line 783, in invoke\r\n[rank0]:     return __callback(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/leapfrogai_sdk/cli.py\", line 37, in cli\r\n[rank0]:     asyncio.run(serve(app(), host, port))\r\n[rank0]:                       ^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/leapfrogai_sdk/llm.py\", line 106, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/home/leapfrogai/packages/vllm/src/main.py\", line 166, in __init__\r\n[rank0]:     self.engine = AsyncLLMEngine.from_engine_args(self.engine_args)\r\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 466, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:              ^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 380, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 547, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 251, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:                           ^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/executor/gpu_executor.py\", line 36, in _init_executor\r\n[rank0]:     self.driver_worker.load_model()\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/worker/worker.py\", line 139, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 682, in load_model\r\n[rank0]:     self.model = get_model(model_config=self.model_config,\r\n[rank0]:                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n[rank0]:     return loader.load_model(model_config=model_config,\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py\", line 294, in load_model\r\n[rank0]:     quant_method.process_weights_after_loading(module)\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/model_executor/layers/quantization/awq_marlin.py\", line 224, in process_weights_after_loading\r\n[rank0]:     marlin_qweight = ops.awq_marlin_repack(\r\n[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/_custom_ops.py\", line 34, in wrapper\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/vllm/_custom_ops.py\", line 282, in awq_marlin_repack\r\n[rank0]:     return torch.ops._C.awq_marlin_repack(b_q_weight, size_k, size_n, num_bits)\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]:   File \"/home/leapfrogai/.venv/lib/python3.11/site-packages/torch/_ops.py\", line 854, in __call__\r\n[rank0]:     return self_._op(*args, **(kwargs or {}))\r\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n[rank0]: RuntimeError: b_q_weight is not on GPU\r\n```\r\n\r\n```python\r\nimport asyncio\r\nimport json\r\nimport logging\r\nimport os\r\nimport queue\r\nimport random\r\nimport sys\r\nimport threading\r\nimport time\r\nfrom typing import Any, Dict, AsyncGenerator\r\n\r\nfrom confz import EnvSource\r\nfrom dotenv import load_dotenv\r\nfrom vllm import SamplingParams\r\nfrom vllm.engine.arg_utils import AsyncEngineArgs\r\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine\r\nfrom vllm.outputs import RequestOutput\r\nfrom vllm.utils import random_uuid\r\n\r\nfrom config import AppConfig\r\nfrom leapfrogai_sdk import (\r\n    BackendConfig,\r\n    ChatCompletionRequest,\r\n    CompletionRequest,\r\n)\r\nfrom leapfrogai_sdk.llm import (\r\n    GenerationConfig,\r\n    LLM,\r\n)\r\n\r\nload_dotenv()\r\n\r\n\r\ndef clamp(n: float | int, smallest: float | int, largest: float | int):\r\n    return max(smallest, min(n, largest))\r\n\r\n\r\nclass RandomAsyncIterator:\r\n    \"\"\"Manages multiple async iterables and allows iterating over them in a random order.\"\"\"\r\n\r\n    def __init__(self, async_iterables):\r\n        # Convert each iterable into an async iterator\r\n        self.async_iterators = [ai.__aiter__() for ai in async_iterables]\r\n\r\n    def __aiter__(self):\r\n        return self\r\n\r\n    async def __anext__(self):\r\n        \"\"\"Return the next item from a randomly chosen iterator. If all iterators are exhausted, stop iteration.\"\"\"\r\n        if not self.async_iterators:  # Check if there are no iterators left\r\n            raise StopAsyncIteration\r\n\r\n        # Select a random iterator from the list\r\n        random_index = random.randint(0, len(self.async_iterators) - 1)\r\n        try:\r\n            # Attempt to get the next item from the randomly selected iterator\r\n            return await self.async_iterators[random_index].__anext__()\r\n        except StopAsyncIteration:\r\n            # If the selected iterator is exhausted, remove it from the list\r\n            del self.async_iterators[random_index]\r\n\r\n        # If all iterators are exhausted, raise StopAsyncIteration\r\n        raise StopAsyncIteration\r\n\r\n    def is_empty(self):\r\n        \"\"\"Check if there are any iterators left.\"\"\"\r\n        return len(self.async_iterators) <= 0\r\n\r\n    def add_iterator(self, async_iterable):\r\n        \"\"\"Add a new async iterable to the pool of iterators.\"\"\"\r\n        self.async_iterators.append(async_iterable.__aiter__())\r\n\r\n    def remove_iterator(self, async_iterable):\r\n        \"\"\"Attempt to remove an async iterable from the pool if it exists.\"\"\"\r\n        try:\r\n            self.async_iterators.remove(async_iterable.__aiter__())\r\n        except ValueError:\r\n            pass  # If the iterable is not found, ignore the error\r\n\r\n\r\ndef get_backend_configs():\r\n    # Manually load env var as ConfZ does not handle complex types (list)\r\n    stop_tokens: str | None = os.getenv(\"LAI_STOP_TOKENS\")\r\n    if stop_tokens:\r\n        processed_stop_tokens = json.loads(stop_tokens)\r\n    else:\r\n        processed_stop_tokens = []\r\n    del os.environ[\"LAI_STOP_TOKENS\"]\r\n\r\n    env_source = EnvSource(\r\n        allow_all=True,\r\n        prefix=\"LAI_\",\r\n        remap={\r\n            \"model_source\": \"model.source\",\r\n            \"max_context_length\": \"max_context_length\",\r\n            \"stop_tokens\": \"stop_tokens\",\r\n            \"prompt_format_chat_system\": \"prompt_format.chat.system\",\r\n            \"prompt_format_chat_assistant\": \"prompt_format.chat.assistant\",\r\n            \"prompt_format_chat_user\": \"prompt_format.chat.user\",\r\n            \"prompt_format_defaults_top_p\": \"prompt_format.defaults.top_p\",\r\n            \"prompt_format_defaults_top_k\": \"prompt_format.defaults.top_k\",\r\n        },\r\n    )\r\n\r\n    BackendConfig.CONFIG_SOURCES = env_source\r\n    # Initialize an immutable config from env variables without stop_tokens list\r\n    backend_configs: BackendConfig = BackendConfig()\r\n    # Updates \"processed_stop_tokens\" without triggering Pydantic validation errors\r\n    backend_configs.model_copy(update={\"stop_tokens\": processed_stop_tokens})\r\n\r\n    return backend_configs\r\n\r\n\r\ndef get_config_from_request(request: ChatCompletionRequest | CompletionRequest):\r\n    return GenerationConfig(\r\n        max_new_tokens=request.max_new_tokens,\r\n        temperature=request.temperature,\r\n        top_k=request.top_k,\r\n        top_p=request.top_p,\r\n        do_sample=request.do_sample,\r\n        n=request.n,\r\n        stop=list(request.stop),\r\n        repetition_penalty=request.repetition_penalty,\r\n        presence_penalty=request.presence_penalty,\r\n        best_of=str(request.best_of),\r\n        logit_bias=request.logit_bias,\r\n        return_full_text=request.return_full_text,\r\n        truncate=request.truncate,\r\n        typical_p=request.typical_p,\r\n        watermark=request.watermark,\r\n        seed=request.seed,\r\n    )\r\n\r\n\r\n@LLM\r\nclass Model:\r\n    \"\"\"Implements an LLM model with concurrent output generation and management.\"\"\"\r\n\r\n    done_by_id: Dict[str, bool] = {}\r\n    delta_queue_by_id: Dict[str, queue.Queue] = {}\r\n    result_by_id: Dict[str, RequestOutput] = {}\r\n    random_iterator: RandomAsyncIterator = RandomAsyncIterator([])\r\n\r\n    def __init__(self):\r\n        logging.getLogger().setLevel(logging.DEBUG)\r\n\r\n        # Background thread for managing output iteration\r\n        _thread = threading.Thread(target=asyncio.run, args=(self.iterate_outputs(),))\r\n        _thread.start()\r\n\r\n        self.backend_config = get_backend_configs()\r\n        self.model = self.backend_config.model.source\r\n        self.engine_args = AsyncEngineArgs(\r\n            engine_use_ray=False,\r\n            model=self.model,\r\n            trust_remote_code=AppConfig().backend_options.trust_remote_code,\r\n            max_seq_len_to_capture=self.backend_config.max_context_length,\r\n            max_model_len=self.backend_config.max_context_length,\r\n            dtype=\"auto\",\r\n            worker_use_ray=False,\r\n            gpu_memory_utilization=0.99,\r\n            cpu_offload_gb=4,\r\n            tensor_parallel_size=AppConfig().backend_options.tensor_parallel_size,\r\n            enforce_eager=True\r\n        )\r\n        self.engine = AsyncLLMEngine.from_engine_args(self.engine_args)\r\n        print(self.engine_args)\r\n\r\n    async def iterate_outputs(self):\r\n        \"\"\"Continuously processes outputs from the random iterator and manages state by request IDs.\"\"\"\r\n\r\n        t0_by_id: dict[str, float] = {}\r\n        index_by_id: dict[str, int] = {}\r\n        num_tokens_by_id: dict[str, int] = {}\r\n\r\n        while True:\r\n            if not self.random_iterator.is_empty():\r\n                request_output: RequestOutput\r\n                async for request_output in self.random_iterator:\r\n                    request_id = request_output.request_id\r\n\r\n                    if request_output.finished:\r\n                        # Signal that the \"generate\" function can stop waiting for additional inputs\r\n                        logging.info(\r\n                            f\"Generated {num_tokens_by_id[request_id]} tokens in {time.time() - t0_by_id[request_id]:.2f}s\"\r\n                        )\r\n                        self.done_by_id[request_id] = True\r\n                    else:\r\n                        # Initialize dictionary entries\r\n                        if t0_by_id.get(request_id) is None:\r\n                            t0_by_id[request_id] = time.time()\r\n\r\n                        if index_by_id.get(request_id) is None:\r\n                            index_by_id[request_id] = 0\r\n\r\n                        if num_tokens_by_id.get(request_id) is None:\r\n                            num_tokens_by_id[request_id] = 0\r\n\r\n                    if (\r\n                        request_output.outputs[0].text\r\n                        and \"\\ufffd\" == request_output.outputs[0].text[-1]\r\n                    ):\r\n                        continue\r\n\r\n                    # Update tracking information\r\n                    text_delta = request_output.outputs[0].text[\r\n                        index_by_id[request_id] :\r\n                    ]\r\n                    index_by_id[request_id] = len(request_output.outputs[0].text)\r\n                    num_tokens_by_id[request_id] = len(\r\n                        request_output.outputs[0].token_ids\r\n                    )\r\n\r\n                    # Add the result to the queue for this request\r\n                    self.delta_queue_by_id[request_id].put(text_delta)\r\n            time.sleep(0)\r\n\r\n    async def create_response(\r\n        self, request_id: str, prompt: str, config: GenerationConfig\r\n    ):\r\n        \"\"\"Initiate a response generation for the given prompt and configuration, adding the result to the iterator\r\n        pool.\"\"\"\r\n\r\n        sampling_params = SamplingParams(\r\n            temperature=config.temperature,\r\n            # Clamp top_p value to prevent float errors\r\n            top_p=clamp(config.top_p, 0.0 + sys.float_info.epsilon, 1.0),\r\n            # Restrict top_k to valid values, -1 disables top_k\r\n            top_k=config.top_k if config.top_k >= 1 else -1,\r\n            stop=self.backend_config.stop_tokens,\r\n            max_tokens=config.max_new_tokens,\r\n            skip_special_tokens=False,\r\n        )\r\n        logging.debug(sampling_params)\r\n        logging.info(f\"Begin generation for request {request_id}\")\r\n        # Generate texts from the prompts. The output is a list of RequestOutput objects\r\n        # that contain the prompt, generated text, and other information.\r\n        gen_iter = self.engine.generate(prompt, sampling_params, request_id)\r\n        logging.info(f\"Begin iteration for request {request_id}\")\r\n        self.random_iterator.add_iterator(gen_iter)\r\n\r\n    async def generate_session(\r\n        self, session: str, prompt: str, config: GenerationConfig\r\n    ):\r\n        \"\"\"Manage a session's lifecycle for generating output, including setting up necessary state and iterators.\"\"\"\r\n\r\n        if self.delta_queue_by_id.get(session) is None:\r\n            self.delta_queue_by_id[session] = queue.Queue()\r\n\r\n        await self.create_response(session, prompt, config)\r\n\r\n    def is_queue_empty(self, request_id) -> bool:\r\n        \"\"\"Check if the queue for a given request ID is empty or non-existent.\"\"\"\r\n\r\n        cur_request_queue = self.delta_queue_by_id.get(request_id)\r\n        return cur_request_queue is None or cur_request_queue.empty()\r\n\r\n    async def generate(\r\n        self, prompt: str, config: GenerationConfig\r\n    ) -> AsyncGenerator[str, Any]:\r\n        \"\"\"Initiate and manage the generation process for a given prompt, yielding generated text segments.\"\"\"\r\n\r\n        request_id = random_uuid()\r\n        self.done_by_id[request_id] = False\r\n\r\n        # Spawns a thread to request a response for the prompt\r\n        _thread = threading.Thread(\r\n            target=asyncio.run,\r\n            args=(self.generate_session(request_id, prompt, config),),\r\n        )\r\n        _thread.start()\r\n\r\n        logging.info(f\"Begin reading the output for request {request_id}\")\r\n\r\n        while not self.done_by_id.get(request_id) or not self.is_queue_empty(\r\n            request_id\r\n        ):\r\n            result = \"\"\r\n            if not self.is_queue_empty(request_id):\r\n                result = self.delta_queue_by_id.get(request_id).get()\r\n            yield result\r\n\r\n        logging.info(f\"Finished request {request_id}\")\r\n\r\n    async def count_tokens(self, raw_text: str) -> int:\r\n        tokens: list[int] | list[str] = (await self.engine.get_tokenizer()).tokenize(\r\n            raw_text\r\n        )\r\n        return len(tokens)\r\n\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-30T15:26:23Z",
    "closed_at": "2024-07-31T19:47:48Z",
    "author": "justinthelaw",
    "comments_count": 1,
    "comments": [
      {
        "author": "youkaichao",
        "body": "might be related with https://github.com/vllm-project/vllm/issues/6765 ",
        "created_at": "2024-07-30T17:49:39Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/10324": {
    "issue_number": 10324,
    "issue_url": "https://github.com/vllm-project/vllm/issues/10324",
    "title": "[Bug] custom chat template sends to model [{'type': 'text', 'text': '...'}]",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\n\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.12.7 (main, Oct  1 2024, 08:52:12) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-118-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 555.42.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nNVIDIA_VISIBLE_DEVICES=all\r\nNVIDIA_REQUIRE_CUDA=cuda>=12.4 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\r\nNVIDIA_DRIVER_CAPABILITIES=compute,utility\r\nVLLM_USAGE_SOURCE=production-docker-image\r\nCUDA_VERSION=12.4.1\r\nLD_LIBRARY_PATH=/usr/local/lib/python3.12/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\r\nCUDA_MODULE_LOADING=LAZY\r\n```\r\n\r\n</details>\r\n\r\n### Model Input Dumps\r\n\r\n```\r\nprompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Nov 2024\\n\\n[{'type': 'text', 'text': 'you are a helpful assistant'}]<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n[{'type': 'text', 'text': 'hello\\\\n'}]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=131004, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=GuidedDecodingParams(json=None, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None)\r\n```\r\n### üêõ Describe the bug\r\n\r\nHello.\r\nI created a simple container image that contains latest [tool_chat_template_llama3.2_json.jinja](https://github.com/vllm-project/vllm/blob/main/examples/tool_chat_template_llama3.2_json.jinja)\r\n\r\n```\r\nFROM docker.io/vllm/vllm-openai:v0.6.3.post1\r\nCOPY tool_chat_template_llama3.2_json.jinja vllm-workspace/tool_chat_template_llama3.2_json.jinja\r\n```\r\n\r\nThe container is started using\r\n```\r\nlocalhost/vllm/vllm-openai:v0.6.3.post1-tools \\\r\n  --model neuralmagic/Llama-3.2-90B-Vision-Instruct-FP8-dynamic \\\r\n  --tensor-parallel-size 8 \\\r\n  --served-model-name \"Llama3.2 90B\" \\\r\n  --trust-remote-code \\\r\n  --gpu-memory-utilization 0.95 \\\r\n  --distributed-executor-backend mp \\\r\n  --enforce-eager \\\r\n  --max-num-seqs 2 \\\r\n  --limit-mm-per-prompt image=5 \\\r\n  --tool-call-parser llama3_json --chat-template /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja --enable-auto-tool-choice\r\n```\r\n\r\nVllm openai receives following request\r\n```\r\ncurl -v http://localhost:8000/v1/chat/completions -H 'content-type: application/json' --data '{\"stream\": false, \"model\": \"Llama3.2 90B\", \"messages\": [{\"role\": \"system\", \"content\": \"you are a helpful assistant\"}, {\"role\": \"user\", \"content\": \"hello\\n\"}]}'\r\n```\r\nbut in vllm logs i see user<|end_header_id|>\\n\\n[{'type': 'text', 'text': 'hello\\\\n'}]<|eot_id|\r\n```\r\nINFO 11-14 03:51:42 logger.py:37] Received request chat-585357994ead43ab8d485844b632d641: prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Nov 2024\\n\\n[{'type': 'text', 'text': 'you are a helpful assistant'}]<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\n[{'type': 'text', 'text': 'hello\\\\n'}]<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=131004, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=GuidedDecodingParams(json=None, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None), prompt_token_ids: [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 975, 4723, 220, 2366, 19, 271, 58, 13922, 1337, 1232, 364, 1342, 518, 364, 1342, 1232, 364, 9514, 527, 264, 11190, 18328, 8439, 60, 128009, 128006, 882, 128007, 271, 58, 13922, 1337, 1232, 364, 1342, 518, 364, 1342, 1232, 364, 15339, 1734, 8439, 60, 128009, 128006, 78191, 128007, 271], lora_request: None, prompt_adapter_request: None.\r\n```\r\n\r\nHowever, if i remove only \r\n```\r\n--chat-template /vllm-workspace/examples/tool_chat_template_llama3.2_json.jinja\r\n```\r\nfrom vllm start options, the model receives expected text (user<|end_header_id|>\\n\\nhello\\n<|eot_id|)\r\n```\r\ncurl -v http://localhost:8000/v1/chat/completions -H 'content-type: application/json' --data '{\"stream\": false, \"model\": \"Llama3.2 90B\", \"messages\": [{\"role\": \"system\", \"content\": \"you are a helpful assistant\"}, {\"role\": \"user\", \"content\": \"hello\\n\"}]}'\r\n```\r\n```\r\nINFO 11-14 04:00:43 logger.py:37] Received request chat-fb75d50bb91b4eb68814b86dbe0d4833: prompt: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Nov 2024\\n\\n[{'type': 'text', 'text': 'you are a helpful assistant'}]<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nhello\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\", params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=131017, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), guided_decoding=GuidedDecodingParams(json=None, regex=None, choice=None, grammar=None, json_object=None, backend=None, whitespace_pattern=None), prompt_token_ids: [128000, 128006, 9125, 128007, 271, 38766, 1303, 33025, 2696, 25, 6790, 220, 2366, 18, 198, 15724, 2696, 25, 220, 975, 4723, 220, 2366, 19, 271, 58, 13922, 1337, 1232, 364, 1342, 518, 364, 1342, 1232, 364, 9514, 527, 264, 11190, 18328, 8439, 60, 128009, 128006, 882, 128007, 271, 15339, 198, 128009, 128006, 78191, 128007, 271], lora_request: None, prompt_adapter_request: None.\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-11-14T12:06:50Z",
    "closed_at": "2024-11-23T02:17:40Z",
    "author": "victorserbu2709",
    "comments_count": 2,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Can you try out #10164?",
        "created_at": "2024-11-14T12:33:04Z"
      },
      {
        "author": "victorserbu2709",
        "body": "Thank you @DarkLight1337 , it works",
        "created_at": "2024-11-14T13:13:35Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8275": {
    "issue_number": 8275,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8275",
    "title": "[Bug]: RuntimeError: shape mismatch: value tensor of shape [3328, 7168] cannot be broadcast to indexing result of shape [3328] for OpenGVLab/InternVL2-40B",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...                                                                    \r\nPyTorch version: 2.4.0+cu121                                                                             \r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.12.5 (main, Aug 17 2024, 16:46:07) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-113-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3090\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             12\r\nOn-line CPU(s) list:                0-11\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 6\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) W-2133 CPU @ 3.60GHz\r\nStepping:                           4\r\nCPU MHz:                            3600.000\r\nCPU max MHz:                        3900.0000\r\nCPU min MHz:                        1200.0000\r\nBogoMIPS:                           7200.00\r\nVirtualization:                     VT-x\r\nL1d cache:                          192 KiB\r\nL1i cache:                          192 KiB\r\nL2 cache:                           6 MiB\r\nL3 cache:                           8.3 MiB\r\nNUMA node0 CPU(s):                  0-11\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Mitigation; IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; IBRS; IBPB conditional; STIBP conditional; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd mba ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb intel_pt avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req md_clear flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.6+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-11    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nWhile serving the **OpenGVLab/InternVL2-40B** using Multi-Node Multi-GPU (tensor parallel plus pipeline parallel inference) facing these issue \r\n**RuntimeError: shape mismatch: value tensor of shape [3328, 7168] cannot be broadcast to indexing result of shape [3328]**\r\n\r\nBut I don't face these issue while serving the **OpenGVLab/InternVL2-8B** and **OpenGVLab/InternVL2-26B** \r\n\r\nCommand: **vllm serve OpenGVLab/InternVL2-40B --tensor-parallel-size 1 --pipeline-parallel-size 4 --dtype bfloat16 --gpu-memory-utilization 0.9 --max-model-len 6000 --enforce-eager --trust-remo\r\nte-code --tokenizer-mode \"auto\"**\r\n\r\nLog:\r\n```\r\nINFO 09-08 10:17:29 api_server.py:495] vLLM API server version 0.6.0                                                                                                                                               \r\nINFO 09-08 10:17:29 api_server.py:496] args: Namespace(model_tag='OpenGVLab/InternVL2-40B', config='', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_meth\r\nods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, ro\r\not_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, enable_auto_tool_choice=False, tool_call_parser=None, model='OpenGVLab/InternVL2-40B', tokenizer=None, skip_\r\ntokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', config_format='auto', dtype='bfloat16', kv_\r\ncache_dtype='auto', quantization_param_path=None, max_model_len=6000, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=4, tensor_parallel_size=1\r\n, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=\r\n4, cpu_offload_gb=10.0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, ro\r\npe_theta=None, enforce_eager=True, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=Non\r\ne, limit_mm_per_prompt=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_\r\nprompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, speculative_model_q\r\nuantization=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_look\r\nup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None,\r\n model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_p\r\nroc=False, override_neuron_config=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7f9a44236200>)                                                  \r\nINFO 09-08 10:17:31 api_server.py:162] Multiprocessing frontend to use ipc:///tmp/80d2a87d-aa22-4d9d-9e31-26a457cc7256 for RPC Path.                                                                               \r\nINFO 09-08 10:17:31 api_server.py:178] Started engine process with PID 5081                                                                                                                                        \r\nINFO 09-08 10:17:37 config.py:896] Defaulting to use ray for distributed inference                                                                                                                                 \r\nWARNING 09-08 10:17:37 config.py:364] Async output processing can not be enabled with pipeline parallel                                                                                                            \r\n2024-09-08 10:17:37,156 INFO worker.py:1598 -- Connecting to existing Ray cluster at address: 172.18.10.139:6380...                                                                                               \r\n2024-09-08 10:17:37,193 INFO worker.py:1783 -- Connected to Ray cluster.                                                                                                                                           \r\nINFO 09-08 10:17:37 llm_engine.py:213] Initializing an LLM engine (v0.6.0) with config: model='OpenGVLab/InternVL2-40B', speculative_config=None, tokenizer='OpenGVLab/InternVL2-40B', skip_tokenizer_init=False, t\r\nokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=6000, download_dir=None, loa\r\nd_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_confi\r\ng=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False)\r\n, seed=0, served_model_name=OpenGVLab/InternVL2-40B, use_v2_block_manager=False, num_scheduler_steps=1, enable_prefix_caching=False, use_async_output_proc=False)                                                  \r\nINFO 09-08 10:17:39 ray_gpu_executor.py:134] use_ray_spmd_worker: False                                                                                                                                            \r\nINFO 09-08 10:17:58 utils.py:977] Found nccl from library libnccl.so.2                                                                                                                                             \r\nINFO 09-08 10:17:58 pynccl.py:63] vLLM is using nccl==2.20.5                                                                                                                                                       \r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) INFO 09-08 10:17:58 utils.py:977] Found nccl from library libnccl.so.2                                                                                              \r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) INFO 09-08 10:17:58 pynccl.py:63] vLLM is using nccl==2.20.5                                                                                                        \r\nINFO 09-08 10:17:59 model_runner.py:915] Starting to load model OpenGVLab/InternVL2-40B...                                                                                                                         \r\n/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.l\r\nibrary.impl_abstract` in a future version of PyTorch.                                                                                                                                                              \r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")                                                                                                                                                        \r\n(RayWorkerWrapper pid=17129, ip=172.18.10.141) INFO 09-08 10:17:59 model_runner.py:915] Starting to load model OpenGVLab/InternVL2-40B...                                                                         \r\n/usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.l\r\nibrary.impl_abstract` in a future version of PyTorch.                                                                                                                                                              \r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")                                                                                                                                                        \r\n(RayWorkerWrapper pid=17129, ip=172.18.10.141) /usr/local/lib/python3.12/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. \r\nPlease use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.                                                                                                              \r\n(RayWorkerWrapper pid=17129, ip=172.18.10.141)   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n(RayWorkerWrapper pid=17203, ip=172.18.10.141)   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\nINFO 09-08 10:18:07 weight_utils.py:235] Using model weights format ['*.safetensors']\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) INFO 09-08 10:18:08 weight_utils.py:235] Using model weights format ['*.safetensors']\r\n(RayWorkerWrapper pid=17203, ip=172.18.10.141) INFO 09-08 10:17:58 utils.py:977] Found nccl from library libnccl.so.2 [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disa\r\nble log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\r\n(RayWorkerWrapper pid=17203, ip=172.18.10.141) INFO 09-08 10:17:58 pynccl.py:63] vLLM is using nccl==2.20.5 [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) INFO 09-08 10:17:59 model_runner.py:915] Starting to load model OpenGVLab/InternVL2-40B... [repeated 2x across cluster]\r\nLoading safetensors checkpoint shards:   0% Completed | 0/17 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  24% Completed | 4/17 [00:00<00:02,  4.34it/s]\r\nLoading safetensors checkpoint shards:  47% Completed | 8/17 [00:01<00:02,  4.30it/s]\r\nLoading safetensors checkpoint shards:  82% Completed | 14/17 [00:02<00:00,  8.11it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 17/17 [00:02<00:00,  8.16it/s]\r\n\r\nINFO 09-08 10:18:15 model_runner.py:926] Loading model weights took 16.9311 GB\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) INFO 09-08 10:18:17 model_runner.py:926] Loading model weights took 16.0756 GB\r\n(RayWorkerWrapper pid=17203, ip=172.18.10.141) INFO 09-08 10:18:11 weight_utils.py:235] Using model weights format ['*.safetensors'] [repeated 2x across cluster]\r\n(RayWorkerWrapper pid=17203, ip=172.18.10.141) INFO 09-08 10:18:24 model_runner.py:926] Loading model weights took 16.9311 GB\r\n(RayWorkerWrapper pid=17129, ip=172.18.10.141) INFO 09-08 10:18:24 model_runner.py:926] Loading model weights took 16.0756 GB\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464] Error executing method determine_num_available_blocks. This might cause deadlock in distributed execution.\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464] Traceback (most recent call last):\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker_base.py\", line 456, in execute_method\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     return executor(*args, **kwargs)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/worker.py\", line 222, in determine_num_available_blocks\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     self.model_runner.profile_run()\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1133, in profile_run\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     return func(*args, **kwargs)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/vllm/worker/model_runner.py\", line 1450, in execute_model\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     hidden_or_intermediate_states = model_executable(\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]                                     ^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     return forward_call(*args, **kwargs)\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/internvl.py\", line 487, in forward\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     inputs_embeds = merge_multimodal_embeddings(\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]   File \"/usr/local/lib/python3.12/dist-packages/vllm/model_executor/models/utils.py\", line 146, in merge_multimodal_embeddi\r\nngs\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     inputs_embeds[mask] = flattened\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464]     ~~~~~~~~~~~~~^^^^^^\r\n(RayWorkerWrapper pid=2043, ip=172.18.10.140) ERROR 09-08 10:18:28 worker_base.py:464] **RuntimeError: shape mismatch: value tensor of shape [3328, 7168] cannot be broadcast to indexing result of shape [3328]**\r\nProcess SpawnProcess-1:                                                                                                                                                                                 [210/17369]\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.12/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 236, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, rpc_path)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 34, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 735, in from_engine_args\r\n    engine = cls(\r\n             ^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 615, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs) \r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ \r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 835, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/async_llm_engine.py\", line 262, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 319, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/engine/llm_engine.py\", line 448, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 39, in determine_num_available_blocks\r\n    num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/vllm/executor/ray_gpu_executor.py\", line 421, in _run_workers\r\n    ray_worker_outputs = ray.get(ray_worker_outputs) \r\n                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^ \r\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2661, in get\r\n    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\r\n                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 871, in get_objects\r\n    raise value.as_instanceof_cause()\r\n```\r\n\r\n**Debug Info that I found:**\r\n\r\nDebug: input_ids shape: torch.Size([6000])                                                                                                                                                                         \r\nDebug: inputs_embeds shape: torch.Size([6000, 7168])                                                    \r\nDebug: mask shape: torch.Size([6000]), num True values: 3328                                                                                                                                                       \r\nDebug: flattened shape: torch.Size([3328, 7168]) \r\n\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-08T17:36:36Z",
    "closed_at": "2024-09-11T02:11:02Z",
    "author": "Manikandan-Thangaraj-ZS0321",
    "comments_count": 2,
    "comments": [
      {
        "author": "youkaichao",
        "body": "cc @ywang96 @DarkLight1337 @Isotr0py \r\n\r\nlooks like an issue from the visual modality part",
        "created_at": "2024-09-09T07:15:39Z"
      },
      {
        "author": "Isotr0py",
        "body": "I think this is an issue related to pipeline parallelism, because I can reproduce this on `InternVL2-4B` with `pp_size=2`, while `pp_size=1` and `tp_size=2` have no issue.",
        "created_at": "2024-09-09T08:34:47Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6765": {
    "issue_number": 6765,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6765",
    "title": "[Bug]: FP8 Quantization (static and dynamic) incompatible with `--cpu-offload-gb`",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.4.0a0+f70bd71a48.nv24.06\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.5\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (aarch64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1009-nvidia-64k-aarch64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.40\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GH200 480GB\r\nNvidia driver version: 555.42.06\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/aarch64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/aarch64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         aarch64\r\nCPU op-mode(s):                       64-bit\r\nByte Order:                           Little Endian\r\nCPU(s):                               72\r\nOn-line CPU(s) list:                  0-71\r\nVendor ID:                            ARM\r\nModel name:                           Neoverse-V2\r\nModel:                                0\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   72\r\nSocket(s):                            1\r\nStepping:                             r0p0\r\nFrequency boost:                      disabled\r\nCPU max MHz:                          3483.0000\r\nCPU min MHz:                          81.0000\r\nBogoMIPS:                             2000.00\r\nFlags:                                fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 sm3 sm4 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 svesm4 flagm2 frint svei8mm svebf16 i8mm bf16 dgh bti\r\nL1d cache:                            4.5 MiB (72 instances)\r\nL1i cache:                            4.5 MiB (72 instances)\r\nL2 cache:                             72 MiB (72 instances)\r\nL3 cache:                             114 MiB (1 instance)\r\nNUMA node(s):                         9\r\nNUMA node0 CPU(s):                    0-71\r\nNUMA node1 CPU(s):\r\nNUMA node2 CPU(s):\r\nNUMA node3 CPU(s):\r\nNUMA node4 CPU(s):\r\nNUMA node5 CPU(s):\r\nNUMA node6 CPU(s):\r\nNUMA node7 CPU(s):\r\nNUMA node8 CPU(s):\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Not affected\r\nVulnerability Spectre v1:             Mitigation; __user pointer sanitization\r\nVulnerability Spectre v2:             Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.4\r\n[pip3] onnx==1.16.0\r\n[pip3] optree==0.11.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pytorch-triton==3.0.0+989adb9a2\r\n[pip3] torch==2.4.0a0+f70bd71a48.nv24.6\r\n[pip3] torch-tensorrt==2.4.0a0\r\n[pip3] torchvision==0.19.0a0\r\n[pip3] transformers==4.43.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     0-71    0               1\r\nNIC0    SYS      X      PIX\r\nNIC1    SYS     PIX      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n\r\n```\r\n\n\n### üêõ Describe the bug\n\nWhen attempting to perform a CPU offload on GH200 and a FP8 quantization, the following error is thrown. Tested with Llama 3.1-70B-FP8.\r\n\r\nThis occurs with both static FP8 (`Meta-Llama-3.1-70B-Instruct-FP8`) and dynamic quantization (`--model Meta-Llama-3.1-70B-Instruct --quantization=\"fp8\"`\r\n\r\n```\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 294, in load_model\r\n[rank0]:     quant_method.process_weights_after_loading(module)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/fp8.py\", line 174, in process_weights_after_loading\r\n[rank0]:     qweight, weight_scale = ops.scaled_fp8_quant(layer.weight,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 34, in wrapper\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/_custom_ops.py\", line 353, in scaled_fp8_quant\r\n[rank0]:     torch.ops._C.dynamic_scaled_fp8_quant(output, input, scale)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/_ops.py\", line 1024, in __call__\r\n[rank0]:     return self_._op(*args, **(kwargs or {}))\r\n[rank0]: NotImplementedError: Could not run '_C::dynamic_scaled_fp8_quant' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. '_C::dynamic_scaled_fp8_quant' is only available for these backends: [CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradMPS, AutogradXPU, AutogradHPU, AutogradLazy, AutogradMeta, Tracer, AutocastCPU, AutocastXPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\r\n\r\n[rank0]: CUDA: registered at /workspace/csrc/torch_bindings.cpp:18 [kernel]\r\n[rank0]: Meta: registered at /opt/pytorch/pytorch/aten/src/ATen/core/MetaFallbackKernel.cpp:23 [backend fallback]\r\n[rank0]: BackendSelect: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\r\n[rank0]: Python: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:153 [backend fallback]\r\n[rank0]: FuncTorchDynamicLayerBackMode: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:497 [backend fallback]\r\n[rank0]: Functionalize: registered at /opt/pytorch/pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:342 [backend fallback]\r\n[rank0]: Named: registered at /opt/pytorch/pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\r\n[rank0]: Conjugate: registered at /opt/pytorch/pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\r\n[rank0]: Negative: registered at /opt/pytorch/pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\r\n[rank0]: ZeroTensor: registered at /opt/pytorch/pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\r\n[rank0]: ADInplaceOrView: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:86 [backend fallback]\r\n[rank0]: AutogradOther: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:53 [backend fallback]\r\n[rank0]: AutogradCPU: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:57 [backend fallback]\r\n[rank0]: AutogradCUDA: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:65 [backend fallback]\r\n[rank0]: AutogradXLA: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:69 [backend fallback]\r\n[rank0]: AutogradMPS: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:77 [backend fallback]\r\n[rank0]: AutogradXPU: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:61 [backend fallback]\r\n[rank0]: AutogradHPU: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:90 [backend fallback]\r\n[rank0]: AutogradLazy: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:73 [backend fallback]\r\n[rank0]: AutogradMeta: registered at /opt/pytorch/pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:81 [backend fallback]\r\n[rank0]: Tracer: registered at /opt/pytorch/pytorch/torch/csrc/autograd/TraceTypeManual.cpp:297 [backend fallback]\r\n[rank0]: AutocastCPU: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:358 [backend fallback]\r\n[rank0]: AutocastXPU: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:500 [backend fallback]\r\n[rank0]: AutocastCUDA: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/autocast_mode.cpp:314 [backend fallback]\r\n[rank0]: FuncTorchBatched: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\r\n[rank0]: BatchedNestedTensor: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\r\n[rank0]: FuncTorchVmapMode: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\r\n[rank0]: Batched: registered at /opt/pytorch/pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\r\n[rank0]: VmapMode: fallthrough registered at /opt/pytorch/pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\r\n[rank0]: FuncTorchGradWrapper: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:207 [backend fallback]\r\n[rank0]: PythonTLSSnapshot: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:161 [backend fallback]\r\n[rank0]: FuncTorchDynamicLayerFrontMode: registered at /opt/pytorch/pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:493 [backend fallback]\r\n[rank0]: PreDispatch: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:165 [backend fallback]\r\n[rank0]: PythonDispatcher: registered at /opt/pytorch/pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:157 [backend fallback]\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-25T01:40:10Z",
    "closed_at": "2024-07-31T19:47:47Z",
    "author": "drikster80",
    "comments_count": 5,
    "comments": [
      {
        "author": "youkaichao",
        "body": "it is caused by this line:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/316a41ac1de4e6e46933cadb39b9b7af65b01abd/vllm/model_executor/model_loader/loader.py#L294\r\n\r\none workaround, is to put the module in gpu, do the quantization, and then offload it to cpu again. can you have a try?\r\n\r\nthe cpu offloading code is basically \r\n\r\nhttps://github.com/vllm-project/vllm/blob/316a41ac1de4e6e46933cadb39b9b7af65b01abd/vllm/model_executor/models/utils.py#L77-L104",
        "created_at": "2024-07-25T06:35:29Z"
      },
      {
        "author": "KivenChen",
        "body": "We were struggling with the same problem trying to fit a 70B model in a 4xA10 node, and we made it to patch the DefaultModelLoader in this simple way:\r\n\r\n```\r\nif quant_method is not None:\r\n    if next(module.parameters()).device == torch.device(\"cpu\"):  module = module.cuda()\r\n    // process weight\r\n    // call offload function\r\n```\r\n\r\nIt worked, but only in the case where `offload_size > model_size`, which means the model stays with CPU until `forward` call. We also noticed that the `named_parameter` layout seems to be corrupted\r\n\r\nIf the model has a mixed device mapping, a shape mismatch error will occur. For now we are still looking into this issue but we are not familiar enough with vllm device mapping and the marlin kernel. Can we use some advice?\r\n",
        "created_at": "2024-07-26T15:46:41Z"
      },
      {
        "author": "youkaichao",
        "body": "Instead of `module = module.cuda()` which modifies the module, I recommend dive deep into `process_weights_after_loading`, find out which operation requires weight in GPU, and then add `.cuda()` to that tensor.\r\n\r\ne.g.\r\n\r\n```diff\r\n- qweight, weight_scale = ops.scaled_fp8_quant(layer.weight\r\n+ qweight, weight_scale = ops.scaled_fp8_quant(layer.weight.cuda()\r\n```",
        "created_at": "2024-07-26T16:21:15Z"
      },
      {
        "author": "KivenChen",
        "body": "Thank you for your guidance! We've managed to each `scaled_fp8_quant` op in the `ModelLoader` side, which was relatively trivial. I wonder, though, whether we can ensure with ease that the quantized `forward` call works with CPU offload, especially with the all the latest fp8 kernel supports.\r\n\r\nWill look further into that once I get my hands on the test cluster.",
        "created_at": "2024-07-27T03:51:31Z"
      },
      {
        "author": "drikster80",
        "body": "@KivenChen, if you have a patch or fork, I'm happy to run some tests. I haven't been able to get the modifications to work on static FP8. I'm getting either incorrect dimensions from pytorch:\r\n```\r\nRuntimeError: Expected c.size(0) == a.size(0) && a.size(1) == b.size(0) && b.size(1) == c.size(1) to be true, but got false.\r\n```\r\n or split tensors across 2 devices:\r\n\r\n```\r\nRuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!\r\n```\r\n",
        "created_at": "2024-07-29T14:25:53Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7188": {
    "issue_number": 7188,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7188",
    "title": "[Bug]: No metrics exposed at /metrics with 0.5.4 and metrics completely unavailable",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.118.1.el7.x86_64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              106\r\nModel name:                         Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz\r\nStepping:                           6\r\nCPU MHz:                            2800.000\r\nBogoMIPS:                           5600.08\r\nHypervisor vendor:                  Xen\r\nVirtualization type:                full\r\nL1d cache:                          2.3 MiB\r\nL1i cache:                          1.5 MiB\r\nL2 cache:                           60 MiB\r\nL3 cache:                           1.7 GiB\r\nNUMA node0 CPU(s):                  0-95\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush acpi mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl eagerfpu pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd rsb_ctxsw ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 umip pku ospke gfni vaes vpclmulqdq md_clear spec_ctrl intel_stibp flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.1.2+cu121torch2.4\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pyzmq==26.1.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.43.4\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-95    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nv0.5.4 docker release still missing /metrics endpoint\r\n\r\nI try use main branch to build docker image,The metrics information has become completely unavailable,no longer contains any statistical information related to the model.\r\n```\r\n# HELP python_gc_objects_collected_total Objects collected during gc\r\n# TYPE python_gc_objects_collected_total counter\r\npython_gc_objects_collected_total{generation=\"0\"} 7313.0\r\npython_gc_objects_collected_total{generation=\"1\"} 5988.0\r\npython_gc_objects_collected_total{generation=\"2\"} 45.0\r\n# HELP python_gc_objects_uncollectable_total Uncollectable objects found during GC\r\n# TYPE python_gc_objects_uncollectable_total counter\r\npython_gc_objects_uncollectable_total{generation=\"0\"} 0.0\r\npython_gc_objects_uncollectable_total{generation=\"1\"} 0.0\r\npython_gc_objects_uncollectable_total{generation=\"2\"} 0.0\r\n# HELP python_gc_collections_total Number of times this generation was collected\r\n# TYPE python_gc_collections_total counter\r\npython_gc_collections_total{generation=\"0\"} 922.0\r\npython_gc_collections_total{generation=\"1\"} 83.0\r\npython_gc_collections_total{generation=\"2\"} 6.0\r\n# HELP python_info Python platform information\r\n# TYPE python_info gauge\r\npython_info{implementation=\"CPython\",major=\"3\",minor=\"10\",patchlevel=\"14\",version=\"3.10.14\"} 1.0\r\n# HELP process_virtual_memory_bytes Virtual memory size in bytes.\r\n# TYPE process_virtual_memory_bytes gauge\r\nprocess_virtual_memory_bytes 1.3022793728e+010\r\n# HELP process_resident_memory_bytes Resident memory size in bytes.\r\n# TYPE process_resident_memory_bytes gauge\r\nprocess_resident_memory_bytes 7.00747776e+08\r\n# HELP process_start_time_seconds Start time of the process since unix epoch in seconds.\r\n# TYPE process_start_time_seconds gauge\r\nprocess_start_time_seconds 1.72291887821e+09\r\n# HELP process_cpu_seconds_total Total user and system CPU time spent in seconds.\r\n# TYPE process_cpu_seconds_total counter\r\nprocess_cpu_seconds_total 18.240000000000002\r\n# HELP process_open_fds Number of open file descriptors.\r\n# TYPE process_open_fds gauge\r\nprocess_open_fds 22.0\r\n# HELP process_max_fds Maximum number of open file descriptors.\r\n# TYPE process_max_fds gauge\r\nprocess_max_fds 1.048576e+06\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-06T06:03:25Z",
    "closed_at": "2024-08-18T20:19:49Z",
    "author": "ZeroYuJie",
    "comments_count": 5,
    "comments": [
      {
        "author": "michelg10",
        "body": "also experiencing this here.",
        "created_at": "2024-08-06T06:57:23Z"
      },
      {
        "author": "tfisnl",
        "body": "I have the same issue. No metrics forwarded to datadog. \r\n\r\nThe only metrics that are exposed are python-specific metrics.\r\n\r\ne.g.:\r\npython_gc_objects_collected_total{generation=\"0\"} 9178.0\r\npython_gc_objects_collected_total{generation=\"1\"} 4566.0\r\npython_gc_objects_collected_total{generation=\"2\"} 1294.0\r\n",
        "created_at": "2024-08-06T09:40:04Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "Thanks for reporting this. Likely due to our recent shift to multiprocessing on the frontend. I will look into the issue today\r\n\r\nIn the meantime, can you try running with `--disable-frontend-multiprocessing`?",
        "created_at": "2024-08-06T12:13:54Z"
      },
      {
        "author": "ZeroYuJie",
        "body": "> Thanks for reporting this. Likely due to our recent shift to multiprocessing on the frontend. I will look into the issue today\r\n> \r\n> In the meantime, can you try running with `--disable-frontend-multiprocessing`?\r\n\r\nI tried the v0.5.4 version and the docker image I made with the main branch. After adding `--disable-frontend-multiprocessing`, the /metrics interface became usable.",
        "created_at": "2024-08-07T01:59:31Z"
      },
      {
        "author": "jischein",
        "body": "+1",
        "created_at": "2024-08-07T22:43:19Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6137": {
    "issue_number": 6137,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6137",
    "title": "[Bug]: Spec. decode fails for requests with n>1 or best_of>1",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 | packaged by conda-forge | (main, Apr 19 2024, 18:36:13) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8474C\r\nCPU family:                         6\r\nModel:                              143\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           8\r\nCPU max MHz:                        3800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           4200.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          4.5 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           192 MiB (96 instances)\r\nL3 cache:                           195 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] sentence-transformers     3.0.1                    pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tNIC0\tNIC1\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tPIX\tPIX\t0-47,96-143\t0\t\tN/A\r\nGPU1\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU2\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU3\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tNV18\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU4\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU5\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU6\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tNV18\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU7\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\tNV18\t X \tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nNIC0\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tPIX\t\t\t\t\r\nNIC1\tPIX\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tPIX\t X \t\t\t\t\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\n\n### üêõ Describe the bug\n\nIf one sends a request with `n>1` to a server with spec. decode enabled, the request with fail with an unhelpful error message. \r\n\r\nTo reproduce, start an inference server with:\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server \\\r\n    --model meta-llama/Llama-2-13b-chat-hf \\\r\n    --speculative_model https://huggingface.co/ibm-fms/llama-13b-accelerator \\\r\n    --use-v2-block-manager \\\r\n    --enforce-eager\r\n```\r\nand then send a request via:\r\n```python\r\nfrom openai import OpenAI\r\n\r\n# Modify OpenAI's API key and API base to use vLLM's API server.\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\n\r\n# Completion API\r\nstream = True\r\ncompletion = client.completions.create(\r\n    model=model,\r\n    prompt=\"A robot may not injure a human being\",\r\n    echo=False,\r\n    n=2,\r\n    stream=stream)\r\n\r\nprint(\"Completion results:\")\r\nif stream:\r\n    for c in completion:\r\n        print(c)\r\nelse:\r\n    print(completion)\r\n```\r\nThe client will see:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/user/vllm/send_request.py\", line 27, in <module>\r\n    for c in completion:\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/openai/_streaming.py\", line 46, in __iter__\r\n    for item in self._iterator:\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/openai/_streaming.py\", line 58, in __stream__\r\n    for sse in iterator:\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/openai/_streaming.py\", line 50, in _iter_events\r\n    yield from self._decoder.iter_bytes(self.response.iter_bytes())\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/openai/_streaming.py\", line 280, in iter_bytes\r\n    for chunk in self._iter_chunks(iterator):\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/openai/_streaming.py\", line 291, in _iter_chunks\r\n    for chunk in iterator:\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/httpx/_models.py\", line 829, in iter_bytes\r\n    for raw_bytes in self.iter_raw():\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/httpx/_models.py\", line 883, in iter_raw\r\n    for raw_stream_bytes in self.stream:\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/httpx/_client.py\", line 126, in __iter__\r\n    for chunk in self._stream:\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/httpx/_transports/default.py\", line 112, in __iter__\r\n    with map_httpcore_exceptions():\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/contextlib.py\", line 158, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/httpx/_transports/default.py\", line 86, in map_httpcore_exceptions\r\n    raise mapped_exc(message) from exc\r\nhttpx.RemoteProtocolError: peer closed connection without sending complete message body (incomplete chunked read)\r\n```\r\nand the error on the server-side is:\r\n```\r\n    | Traceback (most recent call last):\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/starlette/responses.py\", line 261, in wrap\r\n    |     await func()\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/starlette/responses.py\", line 250, in stream_response\r\n    |     async for chunk in self.body_iterator:\r\n    |   File \"/home/user/vllm/vllm/entrypoints/openai/serving_completion.py\", line 222, in completion_stream_generator\r\n    |     async for prompt_idx, res in result_generator:\r\n    |   File \"/home/user/vllm/vllm/utils.py\", line 319, in consumer\r\n    |     raise e\r\n    |   File \"/home/user/vllm/vllm/utils.py\", line 310, in consumer\r\n    |     raise item\r\n    |   File \"/home/user/vllm/vllm/utils.py\", line 294, in producer\r\n    |     async for item in iterator:\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 746, in generate\r\n    |     async for output in self._process_request(\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 859, in _process_request\r\n    |     raise e\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 855, in _process_request\r\n    |     async for request_output in stream:\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 90, in __anext__\r\n    |     raise result\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    |     return_value = task.result()\r\n    |                    ^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\n    |     result = task.result()\r\n    |              ^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\n    |     request_outputs = await self.engine.step_async(virtual_engine)\r\n    |                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\n    |     output = await self.model_executor.execute_model_async(\r\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/executor/gpu_executor.py\", line 122, in execute_model_async\r\n    |     output = await make_async(self.driver_worker.execute_model\r\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\r\n    |     result = self.fn(*self.args, **self.kwargs)\r\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    |     return func(*args, **kwargs)\r\n    |            ^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/spec_decode/spec_decode_worker.py\", line 338, in execute_model\r\n    |     return self._run_no_spec(execute_model_req,\r\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/contextlib.py\", line 81, in inner\r\n    |     return func(*args, **kwds)\r\n    |            ^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/spec_decode/spec_decode_worker.py\", line 386, in _run_no_spec\r\n    |     sampler_output = self.scorer_worker.execute_model(execute_model_req)\r\n    |                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/worker/worker_base.py\", line 271, in execute_model\r\n    |     output = self.model_runner.execute_model(\r\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    |     return func(*args, **kwargs)\r\n    |            ^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/worker/model_runner.py\", line 1245, in execute_model\r\n    |     output: SamplerOutput = self.model.sample(\r\n    |                             ^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/model_executor/models/llama.py\", line 416, in sample\r\n    |     next_tokens = self.sampler(logits, sampling_metadata)\r\n    |                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    |     return self._call_impl(*args, **kwargs)\r\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/miniforge3/envs/dev-env/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    |     return forward_call(*args, **kwargs)\r\n    |            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/model_executor/layers/sampler.py\", line 96, in forward\r\n    |     sample_results, maybe_sampled_tokens_tensor = _sample(\r\n    |                                                   ^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/model_executor/layers/sampler.py\", line 658, in _sample\r\n    |     return _sample_with_torch(\r\n    |            ^^^^^^^^^^^^^^^^^^^\r\n    |   File \"/home/user/vllm/vllm/model_executor/layers/sampler.py\", line 528, in _sample_with_torch\r\n    |     sampled_token_ids_tensor[\r\n    | RuntimeError: shape mismatch: value tensor of shape [2] cannot be broadcast to indexing result of shape [1, 1]\r\n    +------------------------------------\r\n```\r\n",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-07-04T09:58:18Z",
    "closed_at": "2024-11-24T02:08:38Z",
    "author": "tdoublep",
    "comments_count": 3,
    "comments": [
      {
        "author": "Hongtao-Xu",
        "body": "I encountered the same bug when n=3",
        "created_at": "2024-07-05T09:28:16Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-25T02:02:52Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-24T02:08:38Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/9076": {
    "issue_number": 9076,
    "issue_url": "https://github.com/vllm-project/vllm/issues/9076",
    "title": "[Bug]: vllm overrides transformer's Autoconfig for mllama",
    "body": "### Your current environment\n\nvllm 0.6.2\n\n### Model Input Dumps\n\n_No response_\n\n### üêõ Describe the bug\n\nThis line overrides transformer's autoconfig for mllama, which should be removed\r\nhttps://github.com/vllm-project/vllm/blob/e5dc713c2343b3549b43d6e2764a1036e4052bf8/vllm/transformers_utils/config.py#L41\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-10-04T17:45:59Z",
    "closed_at": "2024-10-05T13:56:41Z",
    "author": "lyuqin-scale",
    "comments_count": 7,
    "comments": [
      {
        "author": "heheda12345",
        "body": "We override it only for changing `is_encoder_decoder=True`, as described [here](https://github.com/vllm-project/vllm/blob/26aa325f4ffe8bf1d9b921535cc02fb31d80a96d/vllm/transformers_utils/configs/mllama.py#L6-L8). Other parts are not modified.",
        "created_at": "2024-10-04T18:10:37Z"
      },
      {
        "author": "lyuqin-scale",
        "body": "@heheda12345 but it does change the behavior of AutoConfig if vllm is imported together\r\n<img width=\"1032\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1d668497-d48f-4624-a9cb-1595f64ffa53\">\r\n",
        "created_at": "2024-10-04T18:12:02Z"
      },
      {
        "author": "lyuqin-scale",
        "body": "comparing with \r\n<img width=\"1063\" alt=\"image\" src=\"https://github.com/user-attachments/assets/8d6290d6-9e5e-45f4-b642-35d9fa5c66a0\">\r\n",
        "created_at": "2024-10-04T18:13:10Z"
      },
      {
        "author": "lyuqin-scale",
        "body": "due to the above, transformer's AutoModelForPreTraining.from_pretrained failed\r\n<img width=\"1587\" alt=\"image\" src=\"https://github.com/user-attachments/assets/298c6b4f-639d-413a-b04f-81d55c4d0619\">\r\n",
        "created_at": "2024-10-04T18:23:03Z"
      },
      {
        "author": "lyuqin-scale",
        "body": "and if we don't import vLLM, it loads successfully. However, we need to import both transformers and vLLM, and use transformer to load model at runtime for some tasks\r\n<img width=\"1194\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3a66dd54-7e09-4176-a6bb-95599355ec13\">\r\n",
        "created_at": "2024-10-04T18:24:36Z"
      },
      {
        "author": "heheda12345",
        "body": "For a quick fix, you can do something like [here](https://github.com/vllm-project/vllm/blob/26aa325f4ffe8bf1d9b921535cc02fb31d80a96d/tests/models/encoder_decoder/vision_language/test_mllama.py#L202) and [here](https://github.com/vllm-project/vllm/blob/26aa325f4ffe8bf1d9b921535cc02fb31d80a96d/tests/models/encoder_decoder/vision_language/test_mllama.py#L217)\r\n\r\nWe are thinking of how to make it cleaner.\r\n\r\n",
        "created_at": "2024-10-04T18:31:09Z"
      },
      {
        "author": "lyuqin-scale",
        "body": "We've had temporary workaround by explicitly using transformers.MllamaForConditionalGeneration to load the mllama model. But we believe a fix from vLLM side could benefit the other vLLM users who aren't aware of this override",
        "created_at": "2024-10-04T18:38:44Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/905": {
    "issue_number": 905,
    "issue_url": "https://github.com/vllm-project/vllm/issues/905",
    "title": "vLLM doesn't support context length exceeding about 13k",
    "body": "I use 4 A100 80G.\r\nvLLM runs perfectly with my code when context length is up to 11k, but when it exceeds around 13k, I've got the error shown below:\r\n\r\n```\r\nRuntimeError: CUDA error: invalid argument \r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect. \r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1. Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n```\r\n\r\nGiven the fact that pure hf code can run up to that length with 4 A100*80G, it can be a memory-related issue, so I'm wondering what might be the issue. (I've set `max_num_batched_tokens` and `get_max_model_len` large enough)\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2023-08-29T13:18:15Z",
    "closed_at": "2023-09-27T05:27:14Z",
    "author": "Arist12",
    "comments_count": 12,
    "comments": [
      {
        "author": "LiuXiaoxuanPKU",
        "body": "Yeah, this is a known issue in vLLM, we are actively fixing this.",
        "created_at": "2023-08-29T17:08:35Z"
      },
      {
        "author": "adamydwang",
        "body": "> \r\n\r\n\r\n\r\n> Yeah, this is a known issue in vLLM, we are actively fixing this.\r\n\r\nHi, xiaoxuan, how is the progress? When is it expected to be fixed?",
        "created_at": "2023-08-31T05:34:03Z"
      },
      {
        "author": "LiuXiaoxuanPKU",
        "body": "Hi want to confirm here, did you run your code with the VLLM main branch or with this [pr](https://github.com/vllm-project/vllm/pull/555)?",
        "created_at": "2023-09-01T17:34:31Z"
      },
      {
        "author": "nicobasile",
        "body": "Hi @LiuXiaoxuanPKU I'm also running into the same issue with `codellama/CodeLlama-34b-Instruct-hf` running from the main vLLM branch. API Server only returns 500 Internal Errors after receiving a request > 13k context length",
        "created_at": "2023-09-01T22:19:46Z"
      },
      {
        "author": "lonngxiang",
        "body": "RuntimeError: CUDA error: no kernel image is available for execution on the device                                                                                                        \r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.                                                                   \r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.                                                                                                                                    \r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions. ",
        "created_at": "2023-09-02T01:26:53Z"
      },
      {
        "author": "babytdream",
        "body": "@Arist12 @nicobasile \r\nHow can I make the context up to 11k? Are you using the` meta-llama2-70b-cha`t model?can you share your commandÔºüThanksÔºÅ",
        "created_at": "2023-09-05T02:21:44Z"
      },
      {
        "author": "Arist12",
        "body": "> @Arist12 @nicobasile How can I make the context up to 11k? Are you using the` meta-llama2-70b-cha`t model?can you share your commandÔºüThanksÔºÅ\r\n\r\nHi, I use the meta-llama2-7b-chat model. All the llama-2 models originally supported a 4k context window. All you need to do is set `max_num_batched_tokens` and `get_max_model_len large` (you might directly overwrite this function or change the model configuration file) enough and find a long enough input prompt.",
        "created_at": "2023-09-05T04:04:19Z"
      },
      {
        "author": "babytdream",
        "body": "![image](https://github.com/vllm-project/vllm/assets/74184102/7e896ee3-43c6-41d8-9fab-9b313aa83c18)\r\nhi, I need to set `\"max_length\"` large in this config and set --max_num_batched_tokens large in command:`python -m vllm.entrypoints.openai.api_server --model meta-llama/Llama-2-70b-hf --tensor-parallel-size 16 --max_num_batched_tokens 10000`.Right?Thanks.",
        "created_at": "2023-09-05T04:11:36Z"
      },
      {
        "author": "chengbofeng",
        "body": "@LiuXiaoxuanPKU\r\n I suspect the issue may be related to 'shared_mem_size' in attention_kernels.cu. When the context is too long, this value may exceed the GPU's shared memory limit, but I don‚Äòt know how to fix it.\r\n\r\n`#define LAUNCH_ATTENTION_KERNEL(T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS)                        \\\r\n  vllm::single_query_cached_kv_attention_kernel<T, HEAD_SIZE, BLOCK_SIZE, NUM_THREADS>        \\\r\n  <<<grid, block, shared_mem_size, stream>>>(                                                 \\`",
        "created_at": "2023-09-06T15:27:51Z"
      },
      {
        "author": "zhuxiaobin",
        "body": "> Yeah, this is a known issue in vLLM, we are actively fixing this.\r\n\r\nsame question, how's the progress?",
        "created_at": "2023-09-08T18:40:53Z"
      },
      {
        "author": "ShengdingHu",
        "body": "Same Question, how's the progress?",
        "created_at": "2023-09-17T14:09:08Z"
      },
      {
        "author": "Yard1",
        "body": "I opened a PR to mitigate this issue for now (and to raise a more informative error) - this will allow >16k context length on most supported GPUs. https://github.com/vllm-project/vllm/pull/1154",
        "created_at": "2023-09-23T03:22:02Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4127": {
    "issue_number": 4127,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4127",
    "title": "[Bug][Chunked prefill]: head size has to be power of two",
    "body": "### üêõ Describe the bug\r\n\r\nThe chunked prefill doesn't support head sizes that are not powers of two. For example, phi2 has head size of 80 (which is supported by flash attn, but the _flash_fwd triton kernel doesn't support it).\r\n\r\nFix PR is coming.\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\nsampling_params = SamplingParams(temperature=0.8)\r\nllm = LLM(model=\"microsoft/phi-2\", enable_chunked_prefill=True)\r\nprint(llm.generate(\"Hello, \", sampling_params))\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/workspaces/aici/py/vllm/test.py\", line 7, in <module>\r\n    print(llm.generate(\"Hello, \", sampling_params))\r\n  File \"/workspaces/aici/py/vllm/vllm/entrypoints/llm.py\", line 190, in generate\r\n    return self._run_engine(use_tqdm)\r\n  File \"/workspaces/aici/py/vllm/vllm/entrypoints/llm.py\", line 218, in _run_engine\r\n    step_outputs = self.llm_engine.step()\r\n  File \"/workspaces/aici/py/vllm/vllm/engine/llm_engine.py\", line 735, in step\r\n    output = self.model_executor.execute_model(\r\n  File \"/workspaces/aici/py/vllm/vllm/executor/gpu_executor.py\", line 91, in execute_model\r\n    output = self.driver_worker.execute_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/worker/worker.py\", line 235, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/worker/model_runner.py\", line 834, in execute_model\r\n    hidden_states = model_executable(**execute_model_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/model_executor/models/phi.py\", line 249, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/model_executor/models/phi.py\", line 213, in forward\r\n    hidden_states = layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/model_executor/models/phi.py\", line 175, in forward\r\n    attn_outputs = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/model_executor/models/phi.py\", line 120, in forward\r\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1511, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1520, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/attention/layer.py\", line 48, in forward\r\n    return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\n  File \"/workspaces/aici/py/vllm/vllm/attention/backends/flash_attn.py\", line 240, in forward\r\n    output[:num_prefill_tokens] = PagedAttention.forward_prefix(\r\n  File \"/workspaces/aici/py/vllm/vllm/attention/ops/paged_attn.py\", line 177, in forward_prefix\r\n    context_attention_fwd(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/workspaces/aici/py/vllm/vllm/attention/ops/prefix_prefill.py\", line 639, in context_attention_fwd\r\n    assert Lk in {16, 32, 64, 128}\r\nAssertionError\r\n```\r\n\r\n### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.27.6\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jun 11 2023, 05:26:28) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-1060-azure-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe\r\nNvidia driver version: 470.239.06\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.5\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.5\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             24\r\nOn-line CPU(s) list:                0-23\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7V13 64-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 1\r\nCore(s) per socket:                 24\r\nSocket(s):                          1\r\nStepping:                           1\r\nBogoMIPS:                           4890.87\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl tsc_reliable nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw topoext perfctr_core invpcid_single vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves clzero xsaveerptr rdpru arat umip vaes vpclmulqdq rdpid fsrm\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          768 KiB (24 instances)\r\nL1i cache:                          768 KiB (24 instances)\r\nL2 cache:                           12 MiB (24 instances)\r\nL3 cache:                           96 MiB (3 instances)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-23\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==0.991\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.22.2\r\n[pip3] onnx==1.14.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] torch==2.2.1\r\n[pip3] torch-tensorrt==0.0.0\r\n[pip3] torchdata==0.7.0a0\r\n[pip3] torchtext==0.16.0a0\r\n[pip3] torchvision==0.16.0a0\r\n[pip3] triton==2.2.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: 8.0; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity\r\nGPU0     X      0-23            N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-16T23:03:08Z",
    "closed_at": "2024-04-18T07:51:29Z",
    "author": "mmoskal",
    "comments_count": 0,
    "comments": []
  },
  "https://github.com/vllm-project/vllm/issues/9834": {
    "issue_number": 9834,
    "issue_url": "https://github.com/vllm-project/vllm/issues/9834",
    "title": "[Bug]: Sampling parameter fixed issue while doing speculative sampling verification step",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.27.9\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.4.239-1.el7.elrepo.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.3.107\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.7\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.7\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   43 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          64\r\nOn-line CPU(s) list:             0-63\r\nVendor ID:                       AuthenticAMD\r\nModel name:                      AMD EPYC 7302 16-Core Processor\r\nCPU family:                      23\r\nModel:                           49\r\nThread(s) per core:              2\r\nCore(s) per socket:              16\r\nSocket(s):                       2\r\nStepping:                        0\r\nBogoMIPS:                        5989.21\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\r\nVirtualization:                  AMD-V\r\nL1d cache:                       1 MiB (32 instances)\r\nL1i cache:                       1 MiB (32 instances)\r\nL2 cache:                        16 MiB (32 instances)\r\nL3 cache:                        256 MiB (16 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-15,32-47\r\nNUMA node1 CPU(s):               16-31,48-63\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Vulnerable\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled, PBRSB-eIBRS: Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-dali-cuda120==1.32.0\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.77\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] nvidia-pyindex==1.0.9\r\n[pip3] onnx==1.15.0rc2\r\n[pip3] optree==0.10.0\r\n[pip3] pynvml==11.4.1\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pyzmq==25.1.2\r\n[pip3] torch==2.4.0\r\n[pip3] torch-tensorrt==2.2.0a0\r\n[pip3] torchdata==0.7.0a0\r\n[pip3] torchtext==0.17.0a0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.39.3\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.4\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    NODE    0-15,32-47      0         N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    NODE    0-15,32-47      0         N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    PXB     0-15,32-47      0         N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    PXB     0-15,32-47      0         N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    SYS     16-31,48-63     1         N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    SYS     16-31,48-63     1         N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    SYS     16-31,48-63     1         N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      SYS     16-31,48-63     1         N/A\r\nNIC0    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n\r\n</details>\r\n\r\n\r\n### Model Input Dumps\r\n\r\n_No response_\r\n\r\n### üêõ Describe the bug\r\n\r\n**Example code**\r\n\r\n1. server\r\n`python vllm/vllm/entrypoints/openai/api_server.py -tp 4 --model meta-llama/Llama-2-70b-hf --port 8000 --gpu-memory-utilization 0.8 --trust-remote-code --speculative-model meta-llama/Llama-2-7b-hf --use-v2-block-manager --num_speculative_tokens 1`\r\n\r\n2. completion\r\n```\r\nopenai_api_base = \"http://0.0.0.0:8041/v1\"\r\nclient = OpenAI(base_url=openai_api_base,)\r\n\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\n\r\ncompletion = client.completions.create(\r\n        model=model,\r\n        prompt=\"Hello, my name is\",\r\n        echo=False,\r\n        n=1,\r\n        stream=False,\r\n        temperature=0.6,\r\n        top_p=0.6,\r\n        max_tokens=128,\r\n)\r\nprint(completion)\r\n```\r\n\r\n**Bug**\r\n- Setting\r\n  I am running **speculative sampling** with num_speculative_tokens = 1.\r\nIn this setting, verification with target model should sample two logits in parallel.\r\n\r\n- Problem\r\n  - The bug here is that when I print out the top_p value of sampling parameters, \r\n**the first top_p is always fixed to 1,** \r\nwhich affects the acceptance of the token.\r\n(FYI, same with the example above, I used top_p as 0.6)\r\n  - <img src=\"https://github.com/user-attachments/assets/cee62039-77bf-4af1-b795-41ad2610d9f6\" width=\"70%\" height=\"70%\">\r\n\r\n  -  It is also same when the num_speculative_tokens value is different.\r\n The first top_p value is always fixed to 1.\r\n \r\n - The first top_p value should also be set to the specific sampling parameter value that is given as input.\r\n\r\n\r\n- Test setting \r\n  - sampling param: top_p = 0.9 / temperature = 0.6\r\n  - Target model / Draft model : LLaMA3-70B / LLaMA3-8B\r\n\r\n- As-Is \r\n  - sampling param: top_p = 0.9\r\n  - Speculative metrics: Draft acceptance rate: 0.651, System efficiency: 0.825, Number of speculative tokens: 1, Number of accepted tokens: 13496, Number of draft tokens: 20747, Number of emitted tokens: 34243.     \r\n  - Draft probs are filtered, and target probs are not filtered when comparing the probabilities.  \r\n  - <img width=\"478\" alt=\"·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-10 ·Ñã·Ö©·Ñí·ÖÆ 8 54 56\" src=\"https://github.com/user-attachments/assets/2326722d-a648-480a-ba0c-de83bc2b0329\">\r\n\r\n- Modified \r\n  -  Speculative metrics: Draft acceptance rate: 0.819, System efficiency: 0.910, Number of speculative tokens: 1, Number of accepted tokens: 15311, Number of draft tokens: 18694, Number of emitted tokens: 34005.\r\n\r\n   - Both draft probs and target probs are filtered  \r\n   - <img width=\"436\" alt=\"·Ñâ·Ö≥·Ñè·Ö≥·ÑÖ·Öµ·Ü´·Ñâ·Ö£·Ü∫ 2024-11-10 ·Ñã·Ö©·Ñí·ÖÆ 10 03 52\" src=\"https://github.com/user-attachments/assets/5d23c3a2-181e-4ee6-a5cd-fd281d12de57\">\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-10-30T07:45:01Z",
    "closed_at": "2024-11-27T05:07:32Z",
    "author": "jeongin601",
    "comments_count": 0,
    "comments": []
  },
  "https://github.com/vllm-project/vllm/issues/8983": {
    "issue_number": 8983,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8983",
    "title": "[Bug]: Distributed inference fails on certain multimodal models",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\n\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 24.04 LTS (x86_64)\r\nGCC version: (Ubuntu 13.2.0-23ubuntu4) 13.2.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.12.3 (main, Jul 31 2024, 17:43:48) [GCC 13.2.0] (64-bit runtime)\r\nPython platform: Linux-6.8.0-1012-aws-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A10G\r\nGPU 1: NVIDIA A10G\r\nGPU 2: NVIDIA A10G\r\nGPU 3: NVIDIA A10G\r\n\r\nNvidia driver version: 550.54.14\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               48\r\nOn-line CPU(s) list:                  0-47\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD EPYC 7R32\r\nCPU family:                           23\r\nModel:                                49\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   24\r\nSocket(s):                            1\r\nStepping:                             0\r\nBogoMIPS:                             5599.33\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save rdpid\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            768 KiB (24 instances)\r\nL1i cache:                            768 KiB (24 instances)\r\nL2 cache:                             12 MiB (24 instances)\r\nL3 cache:                             96 MiB (6 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-47\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec rstack overflow:   Vulnerable: Safe RET, no microcode\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.1\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tPHB\tPHB\tPHB\t0-47\t0\t\tN/A\r\nGPU1\tPHB\t X \tPHB\tPHB\t0-47\t0\t\tN/A\r\nGPU2\tPHB\tPHB\t X \tPHB\t0-47\t0\t\tN/A\r\nGPU3\tPHB\tPHB\tPHB\t X \t0-47\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n[err_execute_model_input_20240930-213352.pkl.zip](https://github.com/user-attachments/files/17198766/err_execute_model_input_20240930-213352.pkl.zip)\r\n\n\n### üêõ Describe the bug\n\nSampel code:\r\n\r\nfrom vllm import LLM, SamplingParams\r\nllm = LLM(model=\"adept/fuyu-8b\", tensor_parallel_size=4, pipeline_parallel_size=1)\r\n\r\nThis sample code throws the following error on an instance with 4 A10G GPUs.\r\n\r\nRuntimeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20240930-231823.pkl): shape mismatch: value tensor of shape [16128, 1024] cannot be broadcast to indexing result of shape [16128, 4096]\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-10-01T02:59:45Z",
    "closed_at": "2024-10-01T09:51:42Z",
    "author": "suna-123",
    "comments_count": 3,
    "comments": [
      {
        "author": "Isotr0py",
        "body": "Can you provide the full error logs? So that I can figure out which part is going wrong.",
        "created_at": "2024-10-01T03:03:35Z"
      },
      {
        "author": "suna-123",
        "body": "@Isotr0py here you go.\r\n\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/worker/model_runner_base.py:116, in dump_input_when_exception.<locals>._inner.<locals>._wrapper(*args, **kwargs)\r\n    115 try:\r\n--> 116     return func(*args, **kwargs)\r\n    117 except Exception as err:\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1590, in ModelRunner.execute_model(self, model_input, kv_caches, intermediate_tensors, num_steps)\r\n   1588     model_forward_start.record()\r\n-> 1590 hidden_or_intermediate_states = model_executable(\r\n   1591     input_ids=model_input.input_tokens,\r\n   1592     positions=model_input.input_positions,\r\n   1593     kv_caches=kv_caches,\r\n   1594     attn_metadata=model_input.attn_metadata,\r\n   1595     intermediate_tensors=intermediate_tensors,\r\n   1596     **MultiModalInputs.as_kwargs(multi_modal_kwargs,\r\n   1597                                  device=self.device),\r\n   1598     **seqlen_agnostic_kwargs)\r\n   1600 if (self.observability_config is not None\r\n   1601         and self.observability_config.collect_model_forward_time):\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1553, in Module._wrapped_call_impl(self, *args, **kwargs)\r\n   1552 else:\r\n-> 1553     return self._call_impl(*args, **kwargs)\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1562, in Module._call_impl(self, *args, **kwargs)\r\n   1559 if not (self._backward_hooks or self._backward_pre_hooks or self._forward_hooks or self._forward_pre_hooks\r\n   1560         or _global_backward_pre_hooks or _global_backward_hooks\r\n   1561         or _global_forward_hooks or _global_forward_pre_hooks):\r\n-> 1562     return forward_call(*args, **kwargs)\r\n   1564 try:\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/model_executor/models/fuyu.py:285, in FuyuForCausalLM.forward(self, input_ids, positions, kv_caches, attn_metadata, intermediate_tensors, **kwargs)\r\n    284     inputs_embeds = self.language_model.model.embed_tokens(input_ids)\r\n--> 285     inputs_embeds = merge_multimodal_embeddings(\r\n    286         input_ids, inputs_embeds, vision_embeddings,\r\n    287         self.image_token_id)\r\n    289 else:\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/model_executor/models/utils.py:180, in merge_multimodal_embeddings(input_ids, inputs_embeds, multimodal_embeddings, placeholder_token_id)\r\n    176     raise ValueError(\r\n    177         f\"Attempted to assign {expr} = {flattened.shape[0]} \"\r\n    178         f\"multimodal tokens to {num_expected_tokens} placeholders\")\r\n--> 180 inputs_embeds[mask] = flattened\r\n    181 return inputs_embeds\r\n\r\nRuntimeError: shape mismatch: value tensor of shape [16128, 1024] cannot be broadcast to indexing result of shape [16128, 4096]\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nRuntimeError                              Traceback (most recent call last)\r\nCell In[2], line 1\r\n----> 1 llm = LLM(model=\"adept/fuyu-8b\", tensor_parallel_size=4, pipeline_parallel_size=1)\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/entrypoints/llm.py:214, in LLM.__init__(self, model, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, max_context_len_to_capture, max_seq_len_to_capture, disable_custom_all_reduce, disable_async_output_proc, mm_processor_kwargs, **kwargs)\r\n    189     raise TypeError(\r\n    190         \"There is no need to pass vision-related arguments anymore.\")\r\n    191 engine_args = EngineArgs(\r\n    192     model=model,\r\n    193     tokenizer=tokenizer,\r\n   (...)\r\n    212     **kwargs,\r\n    213 )\r\n--> 214 self.llm_engine = LLMEngine.from_engine_args(\r\n    215     engine_args, usage_context=UsageContext.LLM_CLASS)\r\n    216 self.request_counter = Counter()\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:564, in LLMEngine.from_engine_args(cls, engine_args, usage_context, stat_loggers)\r\n    562 executor_class = cls._get_executor_cls(engine_config)\r\n    563 # Create the LLM engine.\r\n--> 564 engine = cls(\r\n    565     **engine_config.to_dict(),\r\n    566     executor_class=executor_class,\r\n    567     log_stats=not engine_args.disable_log_stats,\r\n    568     usage_context=usage_context,\r\n    569     stat_loggers=stat_loggers,\r\n    570 )\r\n    572 return engine\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:339, in LLMEngine.__init__(self, model_config, cache_config, parallel_config, scheduler_config, device_config, load_config, lora_config, speculative_config, decoding_config, observability_config, prompt_adapter_config, executor_class, log_stats, usage_context, stat_loggers, input_registry, use_cached_outputs)\r\n    325 self.model_executor = executor_class(\r\n    326     model_config=model_config,\r\n    327     cache_config=cache_config,\r\n   (...)\r\n    335     observability_config=self.observability_config,\r\n    336 )\r\n    338 if not self.model_config.embedding_mode:\r\n--> 339     self._initialize_kv_caches()\r\n    341 # If usage stat is enabled, collect relevant info.\r\n    342 if is_usage_stats_enabled():\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/engine/llm_engine.py:474, in LLMEngine._initialize_kv_caches(self)\r\n    467 def _initialize_kv_caches(self) -> None:\r\n    468     \"\"\"Initialize the KV cache in the worker(s).\r\n    469 \r\n    470     The workers will determine the number of blocks in both the GPU cache\r\n    471     and the swap CPU cache.\r\n    472     \"\"\"\r\n    473     num_gpu_blocks, num_cpu_blocks = (\r\n--> 474         self.model_executor.determine_num_available_blocks())\r\n    476     if self.cache_config.num_gpu_blocks_override is not None:\r\n    477         num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/executor/distributed_gpu_executor.py:39, in DistributedGPUExecutor.determine_num_available_blocks(self)\r\n     29 \"\"\"Determine the number of available KV blocks.\r\n     30 \r\n     31 This invokes `determine_num_available_blocks` on each worker and takes\r\n   (...)\r\n     36     - tuple[num_gpu_blocks, num_cpu_blocks]\r\n     37 \"\"\"\r\n     38 # Get the maximum number of blocks that can be allocated on GPU and CPU.\r\n---> 39 num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n     41 # Since we use a shared centralized controller, we take the minimum\r\n     42 # number of blocks across all workers to make sure all the memory\r\n     43 # operators can be applied to all workers.\r\n     44 num_gpu_blocks = min(b[0] for b in num_blocks)\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/executor/multiproc_gpu_executor.py:185, in MultiprocessingGPUExecutor._run_workers(self, method, async_run_tensor_parallel_workers_only, max_concurrent_workers, *args, **kwargs)\r\n    179 worker_outputs = [\r\n    180     worker.execute_method(method, *args, **kwargs)\r\n    181     for worker in self.workers\r\n    182 ]\r\n    184 driver_worker_method = getattr(self.driver_worker, method)\r\n--> 185 driver_worker_output = driver_worker_method(*args, **kwargs)\r\n    187 # Get the results of the workers.\r\n    188 return [driver_worker_output\r\n    189         ] + [output.get() for output in worker_outputs]\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/worker/worker.py:223, in Worker.determine_num_available_blocks(self)\r\n    219 torch.cuda.empty_cache()\r\n    221 # Execute a forward pass with dummy inputs to profile the memory usage\r\n    222 # of the model.\r\n--> 223 self.model_runner.profile_run()\r\n    225 # Calculate the number of blocks that can be allocated with the\r\n    226 # profiled peak memory.\r\n    227 torch.cuda.synchronize()\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/worker/model_runner.py:1236, in GPUModelRunnerBase.profile_run(self)\r\n   1231 if not get_pp_group().is_first_rank:\r\n   1232     intermediate_tensors = self.model.make_empty_intermediate_tensors(\r\n   1233         batch_size=batch_size,\r\n   1234         dtype=self.model_config.dtype,\r\n   1235         device=self.device)\r\n-> 1236 self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n   1237 torch.cuda.synchronize()\r\n   1238 return\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116, in context_decorator.<locals>.decorate_context(*args, **kwargs)\r\n    113 @functools.wraps(func)\r\n    114 def decorate_context(*args, **kwargs):\r\n    115     with ctx_factory():\r\n--> 116         return func(*args, **kwargs)\r\n\r\nFile ~/vllm_venv/lib/python3.12/site-packages/vllm/worker/model_runner_base.py:152, in dump_input_when_exception.<locals>._inner.<locals>._wrapper(*args, **kwargs)\r\n    146         raise type(err)(f\"Error in model execution: \"\r\n    147                         f\"{str(err)}\") from err\r\n    149     logger.info(\r\n    150         \"Completed writing input of failed execution to %s.\",\r\n    151         filename)\r\n--> 152 raise type(err)(\r\n    153     f\"Error in model execution (input dumped to {filename}): \"\r\n    154     f\"{str(err)}\") from err\r\n\r\nRuntimeError: Error in model execution (input dumped to /tmp/err_execute_model_input_20240930-231823.pkl): shape mismatch: value tensor of shape [16128, 1024] cannot be broadcast to indexing result of shape [16128, 4096]\r\n",
        "created_at": "2024-10-01T03:12:50Z"
      },
      {
        "author": "Isotr0py",
        "body": "@suna-123 #8986 should fix this (tested with `tp_size=2`)",
        "created_at": "2024-10-01T04:24:15Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8947": {
    "issue_number": 8947,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8947",
    "title": "[Bug]: vllm serve --config.yaml - Order of arguments matters?",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Rocky Linux release 8.10 (Green Obsidian) (x86_64)\r\nGCC version: (GCC) 11.3.0\r\nClang version: Could not collect\r\nCMake version: version 3.26.5\r\nLibc version: glibc-2.28\r\n\r\nPython version: 3.11.5 (main, Sep 11 2023, 13:54:46) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-553.16.1.el8_10.x86_64-x86_64-with-glibc2.28\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA H100\r\nGPU 1: NVIDIA H100\r\nGPU 2: NVIDIA H100\r\n  MIG 1g.12gb     Device  0:\r\n  MIG 1g.12gb     Device  1:\r\n  MIG 1g.12gb     Device  2:\r\n  MIG 1g.12gb     Device  3:\r\n  MIG 1g.12gb     Device  4:\r\n  MIG 1g.12gb     Device  5:\r\n  MIG 1g.12gb     Device  6:\r\nGPU 3: NVIDIA H100\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              96\r\nOn-line CPU(s) list: 0-95\r\nThread(s) per core:  1\r\nCore(s) per socket:  48\r\nSocket(s):           2\r\nNUMA node(s):        8\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               143\r\nModel name:          Intel(R) Xeon(R) Platinum 8468\r\nStepping:            8\r\nCPU MHz:             3701.598\r\nCPU max MHz:         3800.0000\r\nCPU min MHz:         800.0000\r\nBogoMIPS:            4200.00\r\nL1d cache:           48K\r\nL1i cache:           32K\r\nL2 cache:            2048K\r\nL3 cache:            107520K\r\nNUMA node0 CPU(s):   0-11\r\nNUMA node1 CPU(s):   12-23\r\nNUMA node2 CPU(s):   24-35\r\nNUMA node3 CPU(s):   36-47\r\nNUMA node4 CPU(s):   48-59\r\nNUMA node5 CPU(s):   60-71\r\nNUMA node6 CPU(s):   72-83\r\nNUMA node7 CPU(s):   84-95\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.45.1\r\n[pip3] triton==3.0.0\r\n[conda] No relevant packages\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1.dev238+ge2c6e0a82\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV6     NV6     NV6     SYS     SYS     0-11    0               N/A\r\nGPU1    NV6      X      NV6     SYS     PIX     SYS     24-35   2               N/A\r\nGPU2    SYS     SYS      X      SYS     SYS     SYS     48-59   4               N/A\r\nGPU3    NV6     SYS     NV6      X      SYS     PIX     72-83   6               N/A\r\nNIC0    SYS     PIX     SYS     SYS      X      SYS\r\nNIC1    SYS     SYS     SYS     PIX     SYS      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### üêõ Describe the bug\n\nWhen serving a vllm server with ``vllm serve path/to/model --config path/to/config.yaml`` the position of the argument ``served-model-name`` seems to be cruical to successfully run the server.\r\n\r\nP.s. \r\nWhy is `collect_env.py` showing `vLLM Version: 0.6.1.dev238+ge2c6e0a82`\r\nI definitely used vllm=0.6.2 and my pip shows the same.\r\n\r\nConfig that works flawlessly:\r\n```\r\nserved-model-name: \"MyModel\"\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\n```\r\nHere, the server runs and I can call the model using the name \"MyModel\".\r\n\r\nConfig that does not work:\r\n```\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\nserved-model-name: \"MyModel\"\r\n```\r\n\r\nWith the latter config, I get the following error:\r\n``vllm serve: error: the following arguments are required: model_tag``\r\n\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug",
      "good first issue"
    ],
    "created_at": "2024-09-29T15:06:36Z",
    "closed_at": "2024-10-05T17:35:13Z",
    "author": "FloWsnr",
    "comments_count": 11,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "The `model_tag` is a positional argument that is separate from `served_model_name`. The previous setting worked because you passed the model name as both `model_tag` (implicitly by placing it as the first argument) and `served_model_name` (via keyword argument). To avoid this issue, I recommend you pass `model_tag` only as the first argument, then `served_model_name` should be automatically set to be the same as `model_tag`. Example:\r\n\r\n```\r\nmodel-tag: \"MyModel\"\r\nhost: \"127.0.0.1\"\r\nport: 6379\r\nuvicorn-log-level: \"info\"\r\n```",
        "created_at": "2024-09-29T15:12:40Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> P.s.\r\n> Why is collect_env.py showing vLLM Version: 0.6.1.dev238+ge2c6e0a82\r\n> I definitely used vllm=0.6.2 and my pip shows the same.\r\n\r\nWhat are you using to manage your Python environment?",
        "created_at": "2024-09-29T15:26:16Z"
      },
      {
        "author": "FloWsnr",
        "body": "> The `model_tag` is a positional argument that is separate from `served_model_name`. The previous setting worked because you passed the model name as both `model_tag` (implicitly by placing it as the first argument) and `served_model_name` (via keyword argument). To avoid this issue, I recommend you pass `model_tag` only as the first argument, then `served_model_name` should be automatically set to be the same as `model_tag`. Example:\r\n> \r\n> ```\r\n> model-tag: \"MyModel\"\r\n> host: \"127.0.0.1\"\r\n> port: 6379\r\n> uvicorn-log-level: \"info\"\r\n> ```\r\n\r\nHi DarkLight, thanks for the help.\r\nI don't think your solution works if I want to specify a local model path but keep a clean/simple model name?\r\n\r\nSo my current actual command is:\r\n`vllm serve \"/home/projectX/Meta-Llama-3.1-8B-Instruct\" --config \"/home/projectX/server_config.yaml\"`\r\nHowever, I want my model to be named `MyModel` which is then the name used in the OpenAI api:\r\n```\r\nresponse = self.client.chat.completions.create(\r\n            model=\"MyModel\",\r\n            messages=chat_history,\r\n        )\r\n```\r\n\r\nIf I use your config, I get the following error:\r\n`vllm: error: unrecognized arguments: --model-tag /home/projectX/Meta-Llama-3.1-8B-Instruct`\r\n\r\n\r\n> > P.s.\r\n> > Why is collect_env.py showing vLLM Version: 0.6.1.dev238+ge2c6e0a82\r\n> > I definitely used vllm=0.6.2 and my pip shows the same.\r\n> \r\n> What are you using to manage your Python environment?\r\n\r\nI am using poetry and updated my vllm version today with `poetry add vllm@latest`",
        "created_at": "2024-09-29T16:05:39Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> Hi DarkLight, thanks for the help.\n> I don't think your solution works if I want to specify a local model path but keep a clean/simple model name?\n> \n> So my current actual command is:\n> `vllm serve \"/home/projectX/Meta-Llama-3.1-8B-Instruct\" --config \"/home/projectX/server_config.yaml\"`\n> However, I want my model to be named `MyModel` which is then the name used in the OpenAI api:\n> ```\n> response = self.client.chat.completions.create(\n>             model=\"MyModel\",\n>             messages=chat_history,\n>         )\n> ```\n> \n> If I use your config, I get the following error:\n> `vllm: error: unrecognized arguments: --model-tag /home/projectX/Meta-Llama-3.1-8B-Instruct`\n\nI see what you mean now, yeah the config parsing could definitely use some improvement...\n\n> \n> > > P.s.\n> > > Why is collect_env.py showing vLLM Version: 0.6.1.dev238+ge2c6e0a82\n> > > I definitely used vllm=0.6.2 and my pip shows the same.\n> > \n> > What are you using to manage your Python environment?\n> \n> I am using poetry and updated my vllm version today with `poetry add vllm@latest`\n\nCan you try wrapping `vllm serve` inside `poetry run` to ensure that you are using the version installed by Poetry?",
        "created_at": "2024-09-30T01:36:14Z"
      },
      {
        "author": "Imss27",
        "body": "Hi @FloWsnr, could you please try my PR?\r\nThe reason I found out is due to the `--served-model-names` argument takes all the values behind it such that the positional argument `model_tag` is not set properly when `served-model-names` is specified at the end of the yaml file.\r\n\r\n@DarkLight1337 Could you please review my PR when you get a chance?üòä",
        "created_at": "2024-09-30T08:53:37Z"
      },
      {
        "author": "FloWsnr",
        "body": "Hi guys, thanks for helping!\r\n>Hi @FloWsnr, could you please try my PR?\r\n\r\nYour PR seems to work for my usecase. As you discussed in the PR, some more work on the arg-parsing might be needed in the future to provide an elegant solution.\r\n\r\n>Why is collect_env.py showing vLLM Version: 0.6.1.dev238+ge2c6e0a82\r\n>I definitely used vllm=0.6.2 and my pip shows the same.\r\nThis is still the case for both:\r\n1. reinstalling the poetry env `poetry install`.\r\n2. `pip install vllm==0.6.2` in a new conda-env.\r\n \r\nI think it might be a problem with the released version since the content of `_version.py` is:\r\n```python\r\n# _version.py\r\n__version__ = version = '0.6.1.dev238+ge2c6e0a82'\r\n__version_tuple__ = version_tuple = (0, 6, 1, 'dev238', 'ge2c6e0a82')\r\n```\r\nShould I open a new issue to track this separately?\r\n",
        "created_at": "2024-10-05T15:39:11Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> I think it might be a problem with the released version since the content of _version.py is: ...\r\n> Should I open a new issue to track this separately?\r\n\r\nv0.6.2 (release) broke `collect_env.py`. It has been fixed by https://github.com/vllm-project/vllm/pull/8894",
        "created_at": "2024-10-05T15:49:18Z"
      },
      {
        "author": "youqugit",
        "body": "> > Hi DarkLight, thanks for the help.\r\n> > I don't think your solution works if I want to specify a local model path but keep a clean/simple model name?\r\n> > So my current actual command is:\r\n> > `vllm serve \"/home/projectX/Meta-Llama-3.1-8B-Instruct\" --config \"/home/projectX/server_config.yaml\"`\r\n> > However, I want my model to be named `MyModel` which is then the name used in the OpenAI api:\r\n> > ```\r\n> > response = self.client.chat.completions.create(\r\n> >             model=\"MyModel\",\r\n> >             messages=chat_history,\r\n> >         )\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > If I use your config, I get the following error:\r\n> > `vllm: error: unrecognized arguments: --model-tag /home/projectX/Meta-Llama-3.1-8B-Instruct`\r\n> \r\n> I see what you mean now, yeah the config parsing could definitely use some improvement...\r\n> \r\n> > > > P.s.\r\n> > > > Why is collect_env.py showing vLLM Version: 0.6.1.dev238+ge2c6e0a82\r\n> > > > I definitely used vllm=0.6.2 and my pip shows the same.\r\n> > > \r\n> > > \r\n> > > What are you using to manage your Python environment?\r\n> > \r\n> > \r\n> > I am using poetry and updated my vllm version today with `poetry add vllm@latest`\r\n> \r\n> Can you try wrapping `vllm serve` inside `poetry run` to ensure that you are using the version installed by Poetry?\r\n\r\nI meet same issue„ÄÇ\r\ni move `served-model-name` to first argument„ÄÇ\r\nerrorÔºö\r\n```text\r\nhuggingface_hub.errors.HFValidationError: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/data/models/Qwen2.5-7B-Instruct-GPTQ-Int4'. Use `repo_type` argument if needed.\r\n```\r\n\r\nsever commandÔºö\r\n```\r\nvllm serve \"/data/models/Qwen2.5-7B-Instruct-GPTQ-Int4\" \\\r\n    --served-model-name Qwen-72B-Chat-Int4 \\\r\n    --distributed-executor-backend ray \\\r\n    --tensor-parallel-size 2 \\\r\n    --quantization gptq \\\r\n    --enable-auto-tool-choice --tool-call-parser hermes \\\r\n    --port 6546\\\r\n    --host 0.0.0.0 \r\n```\r\n\r\n",
        "created_at": "2024-10-08T03:11:00Z"
      },
      {
        "author": "Imss27",
        "body": "@youqugit It seems not to be an argument parsing issue of vLLM, please double check your ` \"/data/models/Qwen2.5-7B-Instruct-GPTQ-Int4\"` string, whether it is in the form of 'repo_name' or 'namespace/repo_name'",
        "created_at": "2024-10-08T03:34:56Z"
      },
      {
        "author": "Sasha-BabyBird",
        "body": "Encountered the same bug today with vLLM ver. 0.9.0.1.\n\nA Qwen model runs normally when the config file looks like this:\n\n```served-model-name: \"qwen2:14bq\" \nmax-model-len: 16384 \ntrust-remote-code: yes \ngpu-memory-utilization: 0.99 \ntool-call-parser: \"hermes\" \nenable-auto-tool-choice: yes \nport: 8888 \nenforce-eager: yes\n```\n\n\nBut if I move the `served-model-name` arg to the end, vLLM throws an \"unrecognized arguments\" error.\n",
        "created_at": "2025-06-06T11:06:59Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Can you open a new issue?",
        "created_at": "2025-06-06T11:22:35Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8411": {
    "issue_number": 8411,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8411",
    "title": "[Bug]: Pixtral inference not working correctly with LLMEngine/AsyncEngine",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-119-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100 80GB PCIe\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.6\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.6\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               12\r\nOn-line CPU(s) list:                  0-11\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz\r\nCPU family:                           6\r\nModel:                                106\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   12\r\nSocket(s):                            1\r\nStepping:                             6\r\nBogoMIPS:                             5600.12\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush acpi mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single intel_ppin ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves umip pku ospke gfni vaes vpclmulqdq rdpid md_clear flush_l1d arch_capabilities\r\nHypervisor vendor:                    Xen\r\nVirtualization type:                  full\r\nL1d cache:                            576 KiB (12 instances)\r\nL1i cache:                            384 KiB (12 instances)\r\nL2 cache:                             15 MiB (12 instances)\r\nL3 cache:                             432 MiB (12 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-11\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI SW loop, KVM SW loop\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-cublas-cu12==12.1.3.1\r\n[pip3] nvidia-cuda-cupti-cu12==12.1.105\r\n[pip3] nvidia-cuda-nvrtc-cu12==12.1.105\r\n[pip3] nvidia-cuda-runtime-cu12==12.1.105\r\n[pip3] nvidia-cudnn-cu12==9.1.0.70\r\n[pip3] nvidia-cufft-cu12==11.0.2.54\r\n[pip3] nvidia-curand-cu12==10.3.2.106\r\n[pip3] nvidia-cusolver-cu12==11.4.5.107\r\n[pip3] nvidia-cusparse-cu12==12.1.0.106\r\n[pip3] nvidia-ml-py==11.525.150\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] nvidia-nvjitlink-cu12==12.6.68\r\n[pip3] nvidia-nvtx-cu12==12.1.105\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.1@3fd2b0d21cd9ec78de410fdf8aa1de840e9ad77a\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \t0-11\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### Model Input Dumps\n\n_No response_\n\n### üêõ Describe the bug\n\nThis code snippets does not work\r\n\r\n```python\r\nimport PIL.Image\r\nimport uuid\r\nfrom vllm import EngineArgs, LLMEngine\r\nfrom vllm import SamplingParams, TextPrompt\r\nfrom vllm.multimodal import MultiModalDataBuiltins\r\n\r\nMODEL_ID = \"mistral-community/pixtral-12b-240910\"\r\nENGINE_ARGS = EngineArgs(\r\n    model=MODEL_ID,\r\n    tokenizer_mode=\"mistral\",\r\n    enable_prefix_caching=True,\r\n    limit_mm_per_prompt=dict(image=4),\r\n    max_num_batched_tokens=16384,\r\n)\r\nSAMPLING_PARAM = SamplingParams()\r\n\r\nengine = LLMEngine.from_engine_args(ENGINE_ARGS)\r\nprompt = \"describe the image\"\r\n\r\nengine_inputs = TextPrompt(prompt=prompt)\r\n\r\nimage = PIL.Image.open(\"demo.jpg\")\r\nmm_data = MultiModalDataBuiltins(image=[image])\r\nengine_inputs[\"multi_modal_data\"] = mm_data\r\n\r\nengine.add_request(uuid.uuid4().hex, engine_inputs, SAMPLING_PARAM)\r\n\r\nwhile True:\r\n    out = engine.step()\r\n    for request_output in request_outputs:\r\n        if request_output.finished:\r\n            print(request_output)\r\n    if not engine.has_unfinished_requests():\r\n        break\r\n```\r\n\r\nThis will give an output message like:\r\n\r\n```\r\n File \"/workspace/codes/example/vllm/pixtral-12b/venv/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py\", line 117, in merge_multimodal_embeddings\r\n    assert (seq_len == N_txt +\r\nAssertionError: seq_len 7 should be equal to N_txt + N_img (7, 1200, 0)\r\n```\r\n\r\nIt seems that I need to manually padding the input token ids with image_token_ids like this:\r\n\r\n```python\r\ntokenizer = engine.get_tokenizer()\r\ntoken_ids = tokenizer(prompt).input_ids\r\nimage_token_id = 10\r\ntoken_ids = [image_token_id] * image_token_num + token_ids\r\n\r\nengine_inputs = TokensPrompt(prompt_token_ids=token_ids)\r\n\r\nmm_data = MultiModalDataBuiltins(image=[image])\r\nengine_inputs[\"multi_modal_data\"] = mm_data\r\n```\r\n\r\nTo make it work.\r\n\r\n`AsyncLLMEngine` also see the same limitation and I need similar modification to make it works like: https://github.com/bentoml/BentoVLLM/pull/71/files#diff-357f77bce00e63217bb5ec382293bca653276e58af9bdfb6e7c50ca9487e27aeR84-R94\r\n\r\nIs this behavior intended or a bug?\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-12T12:26:29Z",
    "closed_at": "2024-09-12T22:21:52Z",
    "author": "larme",
    "comments_count": 8,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Please see if the solutions in #8382 can solve the issue encountered by you",
        "created_at": "2024-09-12T12:56:01Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "Also currently looking into the problem - also maybe see: https://github.com/vllm-project/vllm/pull/8415",
        "created_at": "2024-09-12T13:31:59Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "Hey @larme,\r\n\r\nUpon taking a closer look the problem here is actually not related to the initialization of the model, but occurs because images and the prompt are independently passed before being processed by mistral common's tokenizer.\r\n\r\nWhen using the image and prompt have to be processed together by mistral common to ensure that the tokens are in the right format. See post below for code snippet.",
        "created_at": "2024-09-12T13:59:37Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "@DarkLight1337 I'm not sure what the best way is to make sure users always pre-process with the MistralTokenizer. It's done automatically whenever requests are passed in chat format. Should we maybe throw an error if people try to pass a raw prompt to Pixtral so that the above error doesn't happen too much?",
        "created_at": "2024-09-12T14:01:48Z"
      },
      {
        "author": "DarkLight1337",
        "body": "If you have special tokens that are only available via your tokenizer, you may search the text for those tokens inside the input processor and throw an error if none are found.",
        "created_at": "2024-09-12T14:13:53Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "> If you have special tokens that are only available via your tokenizer, you may search the text for those tokens inside the input processor and throw an error if none are found.\r\n\r\nGreat idea - I'll add this to #8415 ",
        "created_at": "2024-09-12T14:42:37Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "To close the loop - #8415 should fix images with incorrect image init & resizing.\r\n\r\nTwo things that I noticed:\r\n- 1. As @DarkLight1337 already mentioned before we have to disable chunked prefilling for now, so always set `enable_chunked_prefill=True`. @DarkLight1337 can we maybe set this as a default for pixtral? \r\n- 2. Previously, the image processing was done incorrectly - I've accidentally only taken the last image because it worked and because I didn't know about 1.), but this should be fixed now in #8415 \r\n\r\nHere a complete more complex example that should illustrate how to use the model:\r\n\r\n```py\r\nimport PIL.Image\r\nimport uuid\r\nfrom vllm import EngineArgs, LLMEngine\r\nfrom vllm import SamplingParams, TokensPrompt\r\nfrom vllm.multimodal import MultiModalDataBuiltins\r\n\r\nfrom mistral_common.protocol.instruct.messages import (\r\n    UserMessage,\r\n    TextChunk,\r\n    ImageURLChunk,\r\n    ImageChunk,\r\n)\r\nfrom PIL import Image\r\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\r\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\r\n\r\n# MODEL_ID = \"mistral-community/pixtral-12b-240910\"\r\nMODEL_ID = \"mistralai/Pixtral-12B-2409\"\r\nENGINE_ARGS = EngineArgs(\r\n    model=MODEL_ID,\r\n    tokenizer_mode=\"mistral\",\r\n    enable_chunked_prefill=False,\r\n    limit_mm_per_prompt=dict(image=4),\r\n    max_num_batched_tokens=16384,\r\n    max_model_len=16384,\r\n)\r\nSAMPLING_PARAM = SamplingParams(temperature=0.0, max_tokens=512)\r\n\r\nprompt = \"describe the images\"\r\nimage = PIL.Image.open(\"demo.jpg\").resize((400, 500))\r\nimage_2 = PIL.Image.open(\"demo_2.jpg\").resize((560, 800))\r\nimage_3 = PIL.Image.open(\"demo_3.jpg\").resize((150, 200))\r\nimage_4 = PIL.Image.open(\"demo_4.jpg\").resize((344, 444))\r\n\r\nengine = LLMEngine.from_engine_args(ENGINE_ARGS)\r\n\r\ntokenizer = engine.tokenizer.tokenizer.mistral\r\n\r\ndef create_image_input(images, prompt):\r\n# tokenize images and text\r\n    tokenized = tokenizer.encode_chat_completion(\r\n        ChatCompletionRequest(\r\n            messages=[\r\n                UserMessage(\r\n                    content=[\r\n                        TextChunk(text=prompt),\r\n                    ] + [ImageChunk(image=img) for img in images]\r\n                )\r\n            ],\r\n            model=\"pixtral\",\r\n        )\r\n    )\r\n\r\n    engine_inputs = TokensPrompt(prompt_token_ids=tokenized.tokens)\r\n\r\n    mm_data = MultiModalDataBuiltins(image=images)\r\n    engine_inputs[\"multi_modal_data\"] = mm_data\r\n\r\n    return engine_inputs\r\n\r\nengine.add_request(uuid.uuid4().hex, create_image_input([image, image_3], prompt), SAMPLING_PARAM)\r\nengine.add_request(uuid.uuid4().hex, create_image_input([image_2], prompt), SAMPLING_PARAM)\r\n\r\ncount = 0\r\nwhile True:\r\n    out = engine.step()\r\n    count += 1\r\n    for request_output in out:\r\n        if request_output.finished:\r\n            print(request_output.outputs[0].text)\r\n\r\n    if count == 2:\r\n        engine.add_request(uuid.uuid4().hex, create_image_input([image, image_4], prompt), SAMPLING_PARAM)\r\n    if not engine.has_unfinished_requests():\r\n        break\r\n```",
        "created_at": "2024-09-12T14:44:59Z"
      },
      {
        "author": "larme",
        "body": "thanks @patrickvonplaten ! This also works wonderfully in AsyncLLMEngine. We made an example here: https://github.com/bentoml/BentoVLLM/blob/main/pixtral-12b/service.py",
        "created_at": "2024-09-13T07:22:15Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8382": {
    "issue_number": 8382,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8382",
    "title": "[Bug]: Pixtral fails when limit_mm_per_prompt not set",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\n\n### üêõ Describe the bug\n\nThe below command does not work\r\n```\r\nCUDA_VISIBLE_DEVICES=3 vllm serve mistralai/Pixtral-12B-2409 --port 21010 --max_num_batched_tokens 16384 --trust-remote-code --gpu-memory-utilization 0.50 --tokenizer_mode mistral\r\n```\r\n\r\nIt leads to this error: \r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/home/lmsys/vllm/vllm/entrypoints/openai/rpc/server.py\", line 236, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, rpc_path)\r\n  File \"/home/lmsys/vllm/vllm/entrypoints/openai/rpc/server.py\", line 34, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(\r\n  File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 735, in from_engine_args\r\n    engine = cls(\r\n  File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 615, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 835, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/engine/async_llm_engine.py\", line 262, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/engine/llm_engine.py\", line 338, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/home/lmsys/vllm/vllm/engine/llm_engine.py\", line 467, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n  File \"/home/lmsys/vllm/vllm/executor/gpu_executor.py\", line 114, in determine_num_available_blocks\r\n    return self.driver_worker.determine_num_available_blocks()\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/worker/worker.py\", line 223, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/worker/model_runner.py\", line 1216, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/worker/model_runner.py\", line 1543, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/home/lmsys/miniconda3/envs/vllm-source/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/home/lmsys/vllm/vllm/model_executor/models/pixtral.py\", line 178, in forward\r\n    inputs_embeds = merge_multimodal_embeddings(\r\n  File \"/home/lmsys/vllm/vllm/model_executor/models/pixtral.py\", line 117, in merge_multimodal_embeddings\r\n    assert (seq_len == N_txt +\r\nAssertionError: seq_len 16640 should be equal to N_txt + N_img (256, 4096, 16384)\r\n```\r\n\r\nBut the below works (following huggingface):\r\n```\r\nCUDA_VISIBLE_DEVICES=3 vllm serve mistralai/Pixtral-12B-2409 --port 21010 --max_num_batched_tokens 16384 --max-model-len 8192 --trust-remote-code --gpu-memory-utilization 0.50 --tokenizer_mode mistral --limit_mm_per_prompt 'image=4'\r\n```\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-11T23:19:30Z",
    "closed_at": "2024-09-12T22:21:52Z",
    "author": "BabyChouSr",
    "comments_count": 12,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "@patrickvonplaten it looks like `profile_run` is creating more image placeholder tokens (16640) than what's expected by the model (16384) in this case. Perhaps you have to adjust how the dummy data is constructed.",
        "created_at": "2024-09-12T03:01:18Z"
      },
      {
        "author": "jdf-prog",
        "body": "also encouter same error when processing this image:\r\n[https://f2c628843e9892f5c7.gradio.live/file=/tmp/gradio/3036880890cf17b59a0cc838afc217dcd4d91ba5bc294ff42a99f6a2090f8bf2/equation.png]\r\n\r\nWhat's really weird is that, once I resize it to `(3844, 2408)`, then it will work.\r\n\r\nError:\r\n```txt\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/dongfuj/WorkSpace/LMM-Engines/test_vllm_pixtral.py\", line 34, in <module>\r\n[rank0]:     outputs = llm.chat(messages, sampling_params=sampling_params)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 422, in chat\r\n[rank0]:     return self.generate(\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/utils.py\", line 1032, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 348, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 720, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 1600, in step\r\n[rank0]:     outputs = self.model_executor.execute_model(\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 130, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 327, in execute_model\r\n[rank0]:     output = self.model_runner.execute_model(\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 1543, in execute_model\r\n[rank0]:     hidden_or_intermediate_states = model_executable(\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py\", line 181, in forward\r\n[rank0]:     inputs_embeds = merge_multimodal_embeddings(\r\n[rank0]:   File \"/home/dongfuj/.conda/envs/lmm-engines/lib/python3.10/site-packages/vllm/model_executor/models/pixtral.py\", line 117, in merge_multimodal_embeddings\r\n[rank0]:     assert (seq_len == N_txt +\r\n[rank0]: AssertionError: seq_len 12 should be equal to N_txt + N_img (12, 4032, 0)\r\n```",
        "created_at": "2024-09-12T06:01:30Z"
      },
      {
        "author": "jdf-prog",
        "body": "I also tried to resize the image to `(1024, 1024)`, still error. Seems there will be error if the image is more like a square shape?",
        "created_at": "2024-09-12T06:03:21Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Can you try out https://github.com/vllm-project/vllm/pull/8399 and see if it fixes the issue which you've encountered?",
        "created_at": "2024-09-12T06:29:31Z"
      },
      {
        "author": "ywang96",
        "body": "Hello @jdf-prog! Just to confirm, you were able to launch the server, but only this particular image ran into an issue, correct?",
        "created_at": "2024-09-12T07:18:17Z"
      },
      {
        "author": "ywang96",
        "body": "Hmm, I was able to run inference with that image without any resizing\r\n\r\n```\r\nProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:02<00:00,  2.71s/it, est. speed input: 1512.24 toks/s, output: 77.73 toks/s]\r\nThe document discusses a novel approach to policy optimization using preferences, specifically designed to tackle the challenges of fine-tuning language models with reinforcement learning (RL). The key innovation is the derivation of an optimal policy without an RL training loop, instead leveraging an analytical mapping from reward functions to optimal policies. This method transforms the reward functions into a loss function over policies, optimizing models of human preferences, particularly the Bradley-Terry model.\r\n\r\nThe text cites several prior works to derive an RL objective guarantee for the optimal policy based on a reward function, leading to a general solution involving a partition function. The complexity of estimating this function necessitates a reparameterization, which ultimately cancels out the partition function in practical models like the Bradley-Terry model. This reparameterization leads to the final expression of the optimal policy in terms of the optimal and reference policies alone, simplifying the preference model considerably. The main insight is to convert a loss function dependent on reward functions into one based directly on policies, improving the efficiency and feasibility of the optimization process.\r\n```",
        "created_at": "2024-09-12T07:46:53Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "Double checking this command:\r\n```\r\nCUDA_VISIBLE_DEVICES=3 vllm serve mistralai/Pixtral-12B-2409 --port 21010 --max_num_batched_tokens 16384 --trust-remote-code --gpu-memory-utilization 0.50 --tokenizer_mode mistral\r\n```\r\n\r\nBTW there is no need to pass --trust-remote-code here",
        "created_at": "2024-09-12T07:49:33Z"
      },
      {
        "author": "patrickvonplaten",
        "body": "Ah yes I see when passing --max_num_batched_tokens , but not `--limit_mm_per_prompt 'image=4'` then the profile_run throws an error I think that's because the image token create in pixtral is not great. Let me open a PR to fix it! ",
        "created_at": "2024-09-12T07:51:17Z"
      },
      {
        "author": "jdf-prog",
        "body": "> Hello @jdf-prog! Just to confirm, you were able to launch the server, but only this particular image ran into an issue, correct?\r\n\r\nYes, only this particular image. The following is the code I encounter this error. It shall be simple to be reproduced.\r\n```python\r\nfrom vllm import LLM\r\nfrom vllm.sampling_params import SamplingParams\r\n\r\nmodel_name = \"mistralai/Pixtral-12B-2409\"\r\n\r\nsampling_params = SamplingParams(max_tokens=8192)\r\n\r\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", max_model_len=65536, limit_mm_per_prompt={\"image\":4})\r\n\r\nprompt = \"Can you derive Equation 6 from the image?\"\r\nimage_url=\"https://f2c628843e9892f5c7.gradio.live/file=/tmp/gradio/3036880890cf17b59a0cc838afc217dcd4d91ba5bc294ff42a99f6a2090f8bf2/equation.png\"\r\n\r\nmessages = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}]\r\n    },\r\n]\r\n\r\noutputs = llm.chat(messages, sampling_params=sampling_params)\r\n\r\nprint(outputs[0].outputs[0].text)\r\n```",
        "created_at": "2024-09-12T07:56:16Z"
      },
      {
        "author": "jdf-prog",
        "body": "And the code it will work again after the resize:\r\n```python\r\nfrom vllm import LLM\r\nfrom vllm.sampling_params import SamplingParams\r\nfrom PIL import Image\r\nfrom io import BytesIO\r\nimport base64\r\nimport requests\r\n\r\ndef encode_image(image:Image.Image, image_format=\"PNG\") -> str:\r\n    im_file = BytesIO()\r\n    image.save(im_file, format=image_format)\r\n    im_bytes = im_file.getvalue()\r\n    im_64 = base64.b64encode(im_bytes).decode(\"utf-8\")\r\n    return im_64\r\n    \r\nmodel_name = \"mistralai/Pixtral-12B-2409\"\r\n\r\nsampling_params = SamplingParams(max_tokens=8192)\r\n\r\nllm = LLM(model=model_name, tokenizer_mode=\"mistral\", max_model_len=65536, limit_mm_per_prompt={\"image\":4})\r\n\r\nprompt = \"Can you derive Equation 6 from the image?\"\r\nimage_url=\"https://f2c628843e9892f5c7.gradio.live/file=/tmp/gradio/3036880890cf17b59a0cc838afc217dcd4d91ba5bc294ff42a99f6a2090f8bf2/equation.png\"\r\n\r\nimage = Image.open(BytesIO(requests.get(image_url).content))\r\nimage = image.resize((3844, 2408))\r\nnew_image_url = f\"data:image/png;base64,{encode_image(image, image_format='PNG')}\"\r\n\r\nmessages = [\r\n    {\r\n        \"role\": \"user\",\r\n        \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": new_image_url}}]\r\n    },\r\n]\r\n\r\noutputs = llm.chat(messages, sampling_params=sampling_params)\r\n\r\nprint(outputs[0].outputs[0].text)\r\n```",
        "created_at": "2024-09-12T08:02:26Z"
      },
      {
        "author": "ywang96",
        "body": "> > Hello @jdf-prog! Just to confirm, you were able to launch the server, but only this particular image ran into an issue, correct?\r\n> \r\n> Yes, only this particular image. The following is the code I encounter this error. It shall be simple to be reproduced.\r\n> \r\n> ```python\r\n> from vllm import LLM\r\n> from vllm.sampling_params import SamplingParams\r\n> \r\n> model_name = \"mistralai/Pixtral-12B-2409\"\r\n> \r\n> sampling_params = SamplingParams(max_tokens=8192)\r\n> \r\n> llm = LLM(model=model_name, tokenizer_mode=\"mistral\", max_model_len=65536, limit_mm_per_prompt={\"image\":4})\r\n> \r\n> prompt = \"Can you derive Equation 6 from the image?\"\r\n> image_url=\"https://f2c628843e9892f5c7.gradio.live/file=/tmp/gradio/3036880890cf17b59a0cc838afc217dcd4d91ba5bc294ff42a99f6a2090f8bf2/equation.png\"\r\n> \r\n> messages = [\r\n>     {\r\n>         \"role\": \"user\",\r\n>         \"content\": [{\"type\": \"text\", \"text\": prompt}, {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}}]\r\n>     },\r\n> ]\r\n> \r\n> outputs = llm.chat(messages, sampling_params=sampling_params)\r\n> \r\n> print(outputs[0].outputs[0].text)\r\n> ```\r\n\r\n@jdf-prog I'm pretty certain this is due to the fact that chunked prefill is working pretty flakily with VLMs. By default, when the `max-model-len` is bigger than 32768, chunked prefill will be turned on by default with `max-num-batched-tokens` set to a fixed number (4096 for VLMs), and this is something we should definitely address in the near future. Perhaps, we should encouraging explicitly turning off chunked prefill for VLMs for now.\r\n\r\nIn the mean time, can you modify your model initialization similar to what's in `examples/offline_inference_pixtral.py`?\r\n```python\r\n    model_name = \"mistralai/Pixtral-12B-2409\"\r\n    max_img_per_msg = 5\r\n    max_tokens_per_img = 4096\r\n\r\n    sampling_params = SamplingParams(max_tokens=8192, temperature=0.7)\r\n    llm = LLM(\r\n        model=model_name,\r\n        tokenizer_mode=\"mistral\",\r\n        limit_mm_per_prompt={\"image\": max_img_per_msg},\r\n        max_num_batched_tokens=max_img_per_msg * max_tokens_per_img,\r\n    )\r\n```",
        "created_at": "2024-09-12T08:25:59Z"
      },
      {
        "author": "jdf-prog",
        "body": "> >\r\n> In the mean time, can you modify your model initialization similar to what's in `examples/offline_inference_pixtral.py`?\r\n> \r\n> ```python\r\n>     model_name = \"mistralai/Pixtral-12B-2409\"\r\n>     max_img_per_msg = 5\r\n>     max_tokens_per_img = 4096\r\n> \r\n>     sampling_params = SamplingParams(max_tokens=8192, temperature=0.7)\r\n>     llm = LLM(\r\n>         model=model_name,\r\n>         tokenizer_mode=\"mistral\",\r\n>         limit_mm_per_prompt={\"image\": max_img_per_msg},\r\n>         max_num_batched_tokens=max_img_per_msg * max_tokens_per_img,\r\n>     )\r\n> ```\r\n\r\nThanks, I tried this and it seems to work. Thanks for the help!\r\n\r\n",
        "created_at": "2024-09-12T19:28:47Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8361": {
    "issue_number": 8361,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8361",
    "title": "[Bug]: internvl2 multi-prompt input with one image each get RuntimeError",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.4\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Alibaba Group Enterprise Linux Server 7.2 (Paladin) (x86_64)\r\nGCC version: (GCC) 10.2.1 20200825 (Alibaba 10.2.1-3 2.17)\r\nClang version: Could not collect\r\nCMake version: version 3.22.1\r\nLibc version: glibc-2.32\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.10.112-005.ali5000.al8.x86_64-x86_64-with-glibc2.32\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L20\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Probably one of the following:\r\n/usr/lib64/libcudnn.so.9.0.0\r\n/usr/lib64/libcudnn_adv.so.9.0.0\r\n/usr/lib64/libcudnn_cnn.so.9.0.0\r\n/usr/lib64/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib64/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib64/libcudnn_graph.so.9.0.0\r\n/usr/lib64/libcudnn_heuristic.so.9.0.0\r\n/usr/lib64/libcudnn_ops.so.9.0.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                128\r\nOn-line CPU(s) list:   8-15,72-78\r\nOff-line CPU(s) list:  0-7,16-71,79-127\r\nThread(s) per core:    0\r\nCore(s) per socket:    32\r\nSocket(s):             2\r\nNUMA node(s):          2\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 143\r\nModel name:            Intel(R) Xeon(R) Gold 6462C\r\nStepping:              8\r\nCPU MHz:               3899.765\r\nCPU max MHz:           3900.0000\r\nCPU min MHz:           800.0000\r\nBogoMIPS:              6600.00\r\nVirtualization:        VT-x\r\nL1d cache:             48K\r\nL1i cache:             32K\r\nL2 cache:              2048K\r\nL3 cache:              61440K\r\nNUMA node0 CPU(s):     0-31,64-95\r\nNUMA node1 CPU(s):     32-63,96-127\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single intel_ppin cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm uintr md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-ml-py==12.560.30\r\n[pip3] nvidia-ml-py3==7.352.0\r\n[pip3] pynvml==11.5.0\r\n[pip3] pyzmq==26.2.0\r\n[pip3] torch==2.4.0\r\n[pip3] torchaudio==2.4.0a0+69d4077\r\n[pip3] torchmetrics==1.4.1\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.2\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.6.0@32e7db25365415841ebc7c4215851743fbb1bad1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \t8-15,72-78\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nI init the mode with \r\n``` python\r\nfrom vllm import LLM, SamplingParams\r\n\r\nload_dir=\"\"OpenGVLab/InternVL2-8B\"\"\r\nllm = LLM(\r\n    model=load_dir,\r\n    trust_remote_code=True,\r\n    max_num_seqs=5,\r\n    max_model_len=8192,\r\n)\r\n\r\ntokenizer = AutoTokenizer.from_pretrained(load_dir, trust_remote_code=True)\r\n\r\nstop_tokens = [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\", \"<|end|>\"]\r\nstop_token_ids = [tokenizer.convert_tokens_to_ids(i) for i in stop_tokens]\r\n\r\nsampling_params = SamplingParams(temperature=0.2,\r\n                                 max_tokens=8192,\r\n                                 top_p=0.5,\r\n                                 repetition_penalty=1.0,\r\n                                 stop_token_ids=stop_token_ids)\r\n```\r\n\r\nI run the inference with \r\n``` python\r\nllm.generate(prompt, sampling_params=sampling_params)\r\n```\r\n\r\nand the `prompt` is as below\r\n``` json\r\n[{'multi_modal_data': {'image': <PIL.Image.Image image mode=RGB size=800x800 at 0x7FE280267490>}, 'prompt': '<s><|im_start|>user\\n{prompt_text}<image><|im_end|>\\n<|im_start|>assistant\\n'}, {'multi_modal_data': {'image': <PIL.Image.Image image mode=RGB size=500x500 at 0x7FE280267430>}, 'prompt': '<s><|im_start|>user\\n{prompt_text}<image><|im_end|>\\n<|im_start|>assistant\\n'}, {'multi_modal_data': {'image': <PIL.Image.Image image mode=RGB size=800x800 at 0x7FE2982D40A0>}, 'prompt': '<s><|im_start|>user\\n{prompt_text}<image><|im_end|>\\n<|im_start|>assistant\\n'}]\r\n```\r\nwhere the `{prompt_text}` is some text.\r\n\r\nAnd I meet the \r\n\r\n<img width=\"1421\" alt=\"image\" src=\"https://github.com/user-attachments/assets/1d50f236-2731-487f-b329-c84ec31d37cf\">\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-11T08:48:37Z",
    "closed_at": "2024-09-12T17:10:36Z",
    "author": "guozhiyao",
    "comments_count": 2,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Does the problem occur when you only pass in one prompt at a time?",
        "created_at": "2024-09-11T09:57:16Z"
      },
      {
        "author": "guozhiyao",
        "body": "> Does the problem occur when you only pass in one prompt at a time?\r\n\r\n@DarkLight1337 \r\nI have tested it, and it works correctly this way.\r\n\r\n```python\r\nres = []\r\nfor p in prompt:\r\n       tmp = llm.generate(p, *args, **kwargs, sampling_params=sampling_params)[0]\r\n       res.append(tmp)\r\n```",
        "created_at": "2024-09-11T11:16:48Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/8321": {
    "issue_number": 8321,
    "issue_url": "https://github.com/vllm-project/vllm/issues/8321",
    "title": "[Bug]: ÊÆµÈîôËØØ (Ê†∏ÂøÉÂ∑≤ËΩ¨ÂÇ®)",
    "body": "### Your current environment\n\nimport os \r\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4,5,6,7\"  # 4090*4\r\n\r\n\r\n\r\nfrom transformers import AutoTokenizer\r\nfrom vllm import LLM, SamplingParams\r\nimport time\r\nimport uvicorn\r\nfrom fastapi import FastAPI,Body\r\nfrom pydantic import BaseModel\r\nimport asyncio\r\n\r\napps = FastAPI()\r\n\r\npath = \"/workspace/model/llm/Mistral/Mistral-Large-Instruct-2407/Mistral-Large-Instruct-2407-IQ1_M.gguf\"\r\nsampling_params = SamplingParams(temperature=1.0,repetition_penalty=1.0,max_tokens=512)\r\n\r\n# Create an LLM.\r\nllm = LLM(model=path,tokenizer=\"/workspace/model/llm/Mistral/Mistral-Large-Instruct-2407/\",trust_remote_code=True,gpu_memory_utilization=0.8,tensor_parallel_size=4,enforce_eager=True,disable_custom_all_reduce=True)\r\n\r\n\r\n\n\n### üêõ Describe the bug\n\nWARNING 09-10 15:07:35 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 72 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 09-10 15:07:35 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=3532163) INFO 09-10 15:07:35 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=3532163) INFO 09-10 15:07:35 utils.py:977] Found nccl from library libnccl.so.2\r\nINFO 09-10 15:07:35 utils.py:977] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=3532163) INFO 09-10 15:07:35 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 09-10 15:07:35 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 09-10 15:07:35 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fed8f24fee0>, local_subscribe_port=38065, remote_subscribe_port=None)\r\nINFO 09-10 15:07:36 model_runner.py:915] Starting to load model /workspace/model/llm/Mistral/Mistral-Large-Instruct-2407/Mistral-Large-Instruct-2407-IQ1_M.gguf...\r\n(VllmWorkerProcess pid=3532163) INFO 09-10 15:07:36 model_runner.py:915] Starting to load model /workspace/model/llm/Mistral/Mistral-Large-Instruct-2407/Mistral-Large-Instruct-2407-IQ1_M.gguf...\r\nINFO 09-10 15:07:58 model_runner.py:926] Loading model weights took 15.6715 GB\r\n(VllmWorkerProcess pid=3532163) INFO 09-10 15:07:59 model_runner.py:926] Loading model weights took 15.6715 GB\r\n/root/miniconda3/envs/vllm/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\nÊÆµÈîôËØØ (Ê†∏ÂøÉÂ∑≤ËΩ¨ÂÇ®)\r\n\n\n### Before submitting a new issue...\n\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-09-10T07:10:44Z",
    "closed_at": "2024-09-15T22:51:45Z",
    "author": "LIUKAI0815",
    "comments_count": 8,
    "comments": [
      {
        "author": "LIUKAI0815",
        "body": "vllm                              0.6.0",
        "created_at": "2024-09-10T07:11:06Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Does this segmentation fault occur when disabling tensor parallel?",
        "created_at": "2024-09-10T08:07:50Z"
      },
      {
        "author": "DarkLight1337",
        "body": "cc @Isotr0py since it may be related to GGUF loading",
        "created_at": "2024-09-10T08:09:23Z"
      },
      {
        "author": "LIUKAI0815",
        "body": "> Does this segmentation fault occur when disabling tensor parallel?\r\n\r\n[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 74.00 MiB. GPU 0 has a total capacity of 23.65 GiB of which 13.75 MiB is free. Process 2509972 has 23.63 GiB memory in use. Of the allocated memory 23.05 GiB is allocated by PyTorch, and 145.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
        "created_at": "2024-09-10T08:12:32Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Looks like the model is too big to load inside 1 GPU. Is there a smaller version that is easier to test with?",
        "created_at": "2024-09-10T08:14:03Z"
      },
      {
        "author": "LIUKAI0815",
        "body": "Mistral-Large-Instruct-2407-IQ1_M.gguf   1bit Already the smallest",
        "created_at": "2024-09-10T08:20:43Z"
      },
      {
        "author": "Isotr0py",
        "body": "Seems that the model has been loaded to GPU successfully: \r\n```\r\nINFO 09-10 15:07:58 model_runner.py:926] Loading model weights took 15.6715 GB\r\n(VllmWorkerProcess pid=3532163) INFO 09-10 15:07:59 model_runner.py:926] Loading model weights took 15.6715 GB\r\n```\r\nPerhaps related to problematic model forwarding due to gguf config extraction instead. (Maybe caused by calling kernel like `rotary_embeddings` or `page_attention`)",
        "created_at": "2024-09-10T08:35:04Z"
      },
      {
        "author": "Isotr0py",
        "body": "Oh, it's because the gguf kernel we ported is out of date which didn't include `IQ1_M` implementation. I will add it back.",
        "created_at": "2024-09-11T02:22:41Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7920": {
    "issue_number": 7920,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7920",
    "title": "[Bug]: OpenAI server errors out with \"ZMQError Too many open files\" under heavy load",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\n@robertgshaw2-neuralmagic, @njhill\r\n\r\nI am running vllm @ 665304092de6d56aaccaadacfa497a7836d88e7b which includes #7394.\r\n\r\nReproducer:\r\n```\r\n# vllm serve meta-llama/Meta-Llama-3-8B-Instruct  --disable-log-requests\r\n\r\nimport openai\r\nimport asyncio\r\n\r\nN = 800\r\n\r\nclient = openai.AsyncOpenAI(base_url=\"http://localhost:8000/v1\", api_key=\"EMPTY\")\r\n\r\nasync def generate_streaming(prompt: str):\r\n    async for req_output in await client.completions.create(\r\n      model=\"meta-llama/Meta-Llama-3-8B-Instruct\",\r\n      prompt=prompt,\r\n      stream=True,\r\n    ):\r\n        yield req_output.choices[0].text\r\n\r\nasync def generate_output(prompt: str):\r\n    async for output in generate_streaming(prompt):\r\n       final_output = output\r\n    return final_output\r\n\r\n\r\nasync def main():\r\n    prompts = [str(i) for i in range(N)]\r\n    async with asyncio.TaskGroup() as tg:\r\n        tasks = [tg.create_task(generate_output(prompt)) for prompt in prompts]\r\n\r\nasyncio.run(main())\r\n```\r\n\r\nError message:\r\n```\r\n    | Traceback (most recent call last):\r\n    |   File \".venv/lib/python3.11/site-packages/starlette/responses.py\", line 261, in wrap\r\n    |     await func()\r\n    |   File \".venv/lib/python3.11/site-packages/starlette/responses.py\", line 250, in stream_response\r\n    |     async for chunk in self.body_iterator:\r\n    |   File \"vllm/vllm/entrypoints/openai/serving_completion.py\", line 231, in completion_stream_generator\r\n    |     async for prompt_idx, res in result_generator:\r\n    |   File \"vllm/vllm/utils.py\", line 468, in merge_async_iterators\r\n    |     item = await d\r\n    |            ^^^^^^^\r\n    |   File \"vllm/vllm/entrypoints/openai/rpc/client.py\", line 424, in generate\r\n    |     await self.abort(request_id)\r\n    |   File \"vllm/vllm/entrypoints/openai/rpc/client.py\", line 350, in abort\r\n    |     await self._send_one_way_rpc_request(\r\n    |   File \"vllm/vllm/entrypoints/openai/rpc/client.py\", line 256, in _send_one_way_rpc_request\r\n    |     with self.to_proxy_socket() as socket:\r\n    |   File \"/usr/lib/python3.11/contextlib.py\", line 137, in __enter__\r\n    |     return next(self.gen)\r\n    |            ^^^^^^^^^^^^^^\r\n    |   File \"vllm/vllm/entrypoints/openai/rpc/client.py\", line 195, in to_proxy_socket\r\n    |     socket = self.context.socket(zmq.constants.DEALER)\r\n    |              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \".venv/lib/python3.11/site-packages/zmq/sugar/context.py\", line 354, in socket\r\n    |     socket_class(  # set PYTHONTRACEMALLOC=2 to get the calling frame\r\n    |   File \".venv/lib/python3.11/site-packages/zmq/_future.py\", line 218, in __init__\r\n    |     super().__init__(context, socket_type, **kwargs)  # type: ignore\r\n    |     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n    |   File \".venv/lib/python3.11/site-packages/zmq/sugar/socket.py\", line 156, in __init__\r\n    |     super().__init__(\r\n    |   File \"_zmq.py\", line 690, in zmq.backend.cython._zmq.Socket.__init__\r\n    | zmq.error.ZMQError: Too many open files\r\n\r\n```\r\n\r\nThis arguably is not normal online serving traffic. With that said, if `--disable-frontend-multiprocessing` is on, the server can handle `N=8192` with no issue.\r\n\r\nstrace shows lots of eventfd, which might be related to https://www.mail-archive.com/zeromq-dev@lists.zeromq.org/msg31244.html\r\n```\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 976\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 977\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 978\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 979\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 980\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 981\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 982\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 983\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 984\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 985\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 986\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 987\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 988\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 989\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 990\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 991\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 992\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 993\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 994\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 995\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 996\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 997\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 998\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 999\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1000\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1001\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1002\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1003\r\n730059 eventfd2(0, EFD_CLOEXEC <unfinished ...>\r\n730059 <... eventfd2 resumed>)          = 1004\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1005\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1006\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1007\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1008\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1009\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1010\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1011\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1012\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1013\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1014\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1015\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1016\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1017\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1018\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1019\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1020\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1021\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1022\r\n730059 eventfd2(0, EFD_CLOEXEC)         = 1023\r\n730059 eventfd2(0, EFD_CLOEXEC)         = -1 EMFILE (Too many open files)\r\n```\r\n\r\n### Before submitting a new issue...\r\n\r\n- [X] Make sure you already searched for relevant issues, and asked the chatbot living at the bottom right corner of the [documentation page](https://docs.vllm.ai/en/latest/), which can answer lots of frequently asked questions.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-27T17:45:10Z",
    "closed_at": "2024-09-18T13:57:00Z",
    "author": "zifeitong",
    "comments_count": 8,
    "comments": [
      {
        "author": "Br1tBreaker",
        "body": "Right, so it's a different issue then, bruv.  \"Too many open files\", eh? That's a classic.  Sounds like your system's getting overwhelmed with all those requests.\r\n\r\nFirst thing's first, check your system's open file limit.  It's probably set too low. You can bump it up with ulimit -n.  Give it a generous number, like 65536 or even higher.\r\n\r\nNext, have a look at how you're handling those asyncio tasks. Are you creating too many at once? Try limiting the number of concurrent requests with asyncio.Semaphore.\r\n\r\nIf that doesn't do the trick, you might need to get a bit more creative. Consider using a connection pool to manage your requests.  That way, you can reuse connections instead of opening new ones all the time.\r\n\r\nAnd if you're still hitting that limit, it might be time to look at your system architecture. Are you running vllm on a machine with enough resources?  Maybe it's time for an upgrade, mate.\r\n\r\nDon't let this get you down, bruv. We'll get those files under control.",
        "created_at": "2024-08-27T17:50:25Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "Can you share the value of `ulimit`?",
        "created_at": "2024-08-27T18:10:54Z"
      },
      {
        "author": "zifeitong",
        "body": "> Can you share the value of `ulimit`?\r\n\r\nIt's the Ubuntu default 1024.",
        "created_at": "2024-08-27T18:13:48Z"
      },
      {
        "author": "youkaichao",
        "body": "does this mean zmq still opens one file (fortunately it's not a socket) for every connection?",
        "created_at": "2024-08-27T21:39:15Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "@youkaichao i need to dig in more. There is only 1 ipc socket created. The rest are inproc, so I don‚Äôt get what‚Äôs going on. I‚Äôm going to get something up on an Ubuntu server to try to repro\r\n\r\nIn my testing, I manually verified that socket usage was not growing by monitoring system stats, so I‚Äôm not sure what is going on here",
        "created_at": "2024-08-27T21:42:07Z"
      },
      {
        "author": "me-v2",
        "body": "zmq.error.ZMQError: Too many open files  ,  ",
        "created_at": "2024-08-28T09:09:54Z"
      },
      {
        "author": "me-v2",
        "body": "+1",
        "created_at": "2024-08-28T09:10:00Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "We have a new design that should resolve this issue:\r\n- WIP PR: https://github.com/vllm-project/vllm/pull/8092",
        "created_at": "2024-09-02T21:48:50Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7718": {
    "issue_number": 7718,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7718",
    "title": "[Bug]: Error loading microsoft/Phi-3.5-vision-instruct",
    "body": "### Your current environment\n\nvllm version: `Version: 0.5.4`\n\n### üêõ Describe the bug\n\nRepro command:\r\n```\r\nvllm serve microsoft/Phi-3.5-vision-instruct --trust-remote-code --max-model-len 4096\r\n```\r\n\r\nError:\r\n```\r\nvllm serve microsoft/Phi-3.5-vision-instruct --trust-remote-code --max-model-len 4096\r\nINFO 08-21 04:43:37 api_server.py:339] vLLM API server version 0.5.4\r\nINFO 08-21 04:43:37 api_server.py:340] args: Namespace(model_tag='microsoft/Phi-3.5-vision-instruct', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='microsoft/Phi-3.5-vision-instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7206f7951750>)\r\nWARNING 08-21 04:43:37 config.py:1454] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-21 04:43:38 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/Phi-3.5-vision-instruct', speculative_config=None, tokenizer='microsoft/Phi-3.5-vision-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/Phi-3.5-vision-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\r\nINFO 08-21 04:43:38 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 08-21 04:43:38 selector.py:54] Using XFormers backend.\r\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n/usr/local/lib/python3.10/dist-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\nINFO 08-21 04:43:39 model_runner.py:720] Starting to load model microsoft/Phi-3.5-vision-instruct...\r\nINFO 08-21 04:43:39 selector.py:170] Cannot use FlashAttention-2 backend due to sliding window.\r\nINFO 08-21 04:43:39 selector.py:54] Using XFormers backend.\r\nINFO 08-21 04:43:40 weight_utils.py:225] Using model weights format ['*.safetensors']\r\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.84it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.29it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:01<00:00,  1.35it/s]\r\n\r\nINFO 08-21 04:43:42 model_runner.py:732] Loading model weights took 7.7498 GB\r\n/usr/local/lib/python3.10/dist-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n  warnings.warn(\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 263, in __init__\r\n    self._initialize_kv_caches()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 362, in _initialize_kv_caches\r\n    self.model_executor.determine_num_available_blocks())\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 94, in determine_num_available_blocks\r\n    return self.driver_worker.determine_num_available_blocks()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 179, in determine_num_available_blocks\r\n    self.model_runner.profile_run()\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 940, in profile_run\r\n    self.execute_model(model_input, kv_caches, intermediate_tensors)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1363, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 532, in forward\r\n    inputs_embeds = merge_vision_embeddings(input_ids, inputs_embeds,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 29, in merge_vision_embeddings\r\n    raise ValueError(\r\nValueError: Attempted to assign 1 x 781 = 781 image tokens to 2653 placeholders\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-21T04:45:15Z",
    "closed_at": "2024-08-22T01:36:25Z",
    "author": "BabyChouSr",
    "comments_count": 14,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Can you check out #7710 and see if it fixes your issue?",
        "created_at": "2024-08-21T04:50:31Z"
      },
      {
        "author": "berkecanrizai",
        "body": "@DarkLight1337 is this currently fixed?\r\nI am still getting the same error with the Dockerfile.cpu in [this tutorial](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html).",
        "created_at": "2024-08-26T16:43:40Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> @DarkLight1337 is this currently fixed?\n> I am still getting the same error with the Dockerfile.cpu in [this tutorial](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html).\n\nWhich version of vLLM are you using?",
        "created_at": "2024-08-27T01:19:11Z"
      },
      {
        "author": "berkecanrizai",
        "body": "> > @DarkLight1337 is this currently fixed?\r\n> > I am still getting the same error with the Dockerfile.cpu in [this tutorial](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html).\r\n> \r\n> Which version of vLLM are you using?\r\n\r\n`0.5.5`.\r\nI pulled from the source yesterday, so I assume that is the latest available version.\r\nI also tried adding a separate `RUN pip install vllm==0.5.5` into the Docker to make sure it also happens in latest release.\r\n\r\nText only inference works fine for me (just text messages without any image), but, still getting the following errors with the image inputs:\r\n```\r\nValueError: Attempted to assign 1921 = 1921 multimodal tokens to 0 placeholders\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 64, in _log_task_completion\r\n```\r\n\r\nThis also happens with the `microsoft/Phi-3-vision-128k-instruct`, not only `microsoft/Phi-3.5-vision-instruct`.",
        "created_at": "2024-08-27T08:03:03Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> > > @DarkLight1337 is this currently fixed?\r\n> > > I am still getting the same error with the Dockerfile.cpu in [this tutorial](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html).\r\n> > \r\n> > \r\n> > Which version of vLLM are you using?\r\n> \r\n> `0.5.5`. I pulled from the source yesterday, so I assume that is the latest available version. I also tried adding a separate `RUN pip install vllm==0.5.5` into the Docker to make sure it also happens in latest release.\r\n> \r\n> Text only inference works fine for me (just text messages without any image), but, still getting the following errors with the image inputs:\r\n> \r\n> ```\r\n> ValueError: Attempted to assign 1921 = 1921 multimodal tokens to 0 placeholders\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 64, in _log_task_completion\r\n> ```\r\n> \r\n> This also happens with the `microsoft/Phi-3-vision-128k-instruct`, not only `microsoft/Phi-3.5-vision-instruct`.\r\n\r\nYou may have to increase the `max_model_len` as multimodal tokens count towards the limit. Any excess tokens will be truncated.",
        "created_at": "2024-08-27T08:05:28Z"
      },
      {
        "author": "berkecanrizai",
        "body": "> > > > @DarkLight1337 is this currently fixed?\r\n> > > > I am still getting the same error with the Dockerfile.cpu in [this tutorial](https://docs.vllm.ai/en/latest/getting_started/cpu-installation.html).\r\n> > > \r\n> > > \r\n> > > Which version of vLLM are you using?\r\n> > \r\n> > \r\n> > `0.5.5`. I pulled from the source yesterday, so I assume that is the latest available version. I also tried adding a separate `RUN pip install vllm==0.5.5` into the Docker to make sure it also happens in latest release.\r\n> > Text only inference works fine for me (just text messages without any image), but, still getting the following errors with the image inputs:\r\n> > ```\r\n> > ValueError: Attempted to assign 1921 = 1921 multimodal tokens to 0 placeholders\r\n> > \r\n> > The above exception was the direct cause of the following exception:\r\n> > \r\n> > Traceback (most recent call last):\r\n> >   File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n> >   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 64, in _log_task_completion\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > This also happens with the `microsoft/Phi-3-vision-128k-instruct`, not only `microsoft/Phi-3.5-vision-instruct`.\r\n> \r\n> You may have to increase the `max_model_len` as multimodal tokens count towards the limit. Any excess tokens will be truncated.\r\n\r\nI tried with larger `max_model_len` (80.000) as well as without limiting it, still getting the same error.\r\nI get this error on a CPU only machine. I had been running it without any errors on another machine with a GPU.",
        "created_at": "2024-08-27T10:20:32Z"
      },
      {
        "author": "berkecanrizai",
        "body": "```\r\n(VllmWorkerProcess pid=234352) ERROR 08-27 10:30:28 multiproc_worker_utils.py:226]   File \"/home/{USER_NAME}/miniforge3/envs/vllm2/lib/python3.10/site-packages/vllm-0.5.5+cpu-py3.10-linux-x86_64.egg/vllm/model_executor/models/utils.py\", line 88, in merge_multimodal_embeddings\r\n(VllmWorkerProcess pid=234352) ERROR 08-27 10:30:28 multiproc_worker_utils.py:226]     raise ValueError(\r\n(VllmWorkerProcess pid=234352) ERROR 08-27 10:30:28 multiproc_worker_utils.py:226] ValueError: Attempted to assign 1921 = 1921 multimodal tokens to 0 placeholders\r\n```\r\n\r\n@DarkLight1337 this is the exact error I have. I get it in both the Docker and outside of the Docker.",
        "created_at": "2024-08-27T10:32:18Z"
      },
      {
        "author": "DarkLight1337",
        "body": "@Isotr0py since you have a CPU-only environment (and also implemented this model), can you help investigate this? Thanks!",
        "created_at": "2024-08-27T10:46:00Z"
      },
      {
        "author": "Isotr0py",
        "body": "Ok, I will investigate this tonight.",
        "created_at": "2024-08-27T11:02:17Z"
      },
      {
        "author": "berkecanrizai",
        "body": "Small addition @DarkLight1337 @Isotr0py ,\r\n```python\r\nfrom vllm import LLM\r\nfrom transformers import AutoTokenizer\r\nfrom vllm import LLM, SamplingParams\r\nfrom vllm.assets.image import ImageAsset\r\nfrom vllm.utils import FlexibleArgumentParser\r\n\r\nllm = LLM(\r\n     model=\"microsoft/Phi-3.5-vision-instruct\",\r\n     trust_remote_code=True\r\n )\r\n```\r\n\r\nImage inputs work without any issues when I use the LLM as above with `llm.generate...`, however, OpenAI mimicking (`python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3.5-vision-instruct --trust-remote-code`) still fails with the error above.",
        "created_at": "2024-08-27T11:04:16Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Please note that multi-image support is not supported yet for OpenAI-compatible server. Can you provide a minimum reproducible example?",
        "created_at": "2024-08-27T11:11:55Z"
      },
      {
        "author": "berkecanrizai",
        "body": "> Please note that multi-image support is not supported yet for OpenAI-compatible server. Can you provide a minimum reproducible example?\r\n\r\nSure, after running with the above instructions,\r\n\r\nrun the following:\r\n\r\n```python\r\nfrom openai import OpenAI\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8001/v1\". #### make sure this port is correct, I changed it to 8001 in server\r\nclient = OpenAI(\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nchat_response = client.chat.completions.create(\r\n    model=\"microsoft/Phi-3.5-vision-instruct\",\r\n    messages=[{\r\n        \"role\": \"user\",\r\n        \"content\": [\r\n            {\"type\": \"text\", \"text\": \"What's in this image?\"},\r\n            {\r\n                \"type\": \"image_url\",\r\n                \"image_url\": {\r\n                    \"url\": \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\",\r\n                },\r\n            },\r\n        ],\r\n    }],\r\n)\r\nprint(\"Chat response:\", chat_response)\r\n```",
        "created_at": "2024-08-27T11:17:32Z"
      },
      {
        "author": "Isotr0py",
        "body": "@berkecanrizai I have created #7916 to fix this. Please take a look at this :)",
        "created_at": "2024-08-27T15:42:26Z"
      },
      {
        "author": "berkecanrizai",
        "body": "> @berkecanrizai I have created #7916 to fix this. Please take a look at this :)\r\n\r\nThanks, that was fast :D",
        "created_at": "2024-08-27T17:11:53Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7516": {
    "issue_number": 7516,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7516",
    "title": "[Bug]: ImportError related to compressed tensors module",
    "body": "### Your current environment\n\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.134-007.ali5000.al8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 535.129.03\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nBIOS Vendor ID:                  Intel(R) Corporation\r\nModel name:                      Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\r\nBIOS Model name:                 Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        5800.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single intel_ppin ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    1\r\nNUMA node0 CPU(s):               0-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.16.0\r\n[pip3] optree==0.11.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pytorch-triton==3.0.0+989adb9a2\r\n[pip3] pyzmq==26.0.3\r\n[pip3] torch==2.4.0\r\n[pip3] torch-tensorrt==2.4.0a0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.44.0\r\n[pip3] triton==3.0.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: N/A\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    0-127   0               N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    0-127   0               N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    0-127   0               N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    0-127   0               N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    0-127   0               N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    0-127   0               N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    0-127   0               N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      0-127   0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n</details>\r\n\n\n### üêõ Describe the bug\n\nI'm building vllm from source with container `nvcr.io/nvidia/pytorch:24.05-py3`. After `pip install -e .`, I'm trying `import vllm` and I got\r\n<details>\r\n<summary>The error </summary>\r\n\r\n```text\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/host_home/vllm-fork/vllm/__init__.py\", line 3, in <module>\r\n    from vllm.engine.arg_utils import AsyncEngineArgs, EngineArgs\r\n  File \"/host_home/vllm-fork/vllm/engine/arg_utils.py\", line 7, in <module>\r\n    from vllm.config import (CacheConfig, DecodingConfig, DeviceConfig,\r\n  File \"/host_home/vllm-fork/vllm/config.py\", line 11, in <module>\r\n    from vllm.model_executor.layers.quantization import QUANTIZATION_METHODS\r\n  File \"/host_home/vllm-fork/vllm/model_executor/layers/quantization/__init__.py\", line 10, in <module>\r\n    from vllm.model_executor.layers.quantization.compressed_tensors.compressed_tensors import (  # noqa: E501\r\n  File \"/host_home/vllm-fork/vllm/model_executor/layers/quantization/compressed_tensors/compressed_tensors.py\", line 4, in <module>\r\n    from compressed_tensors.config import CompressionFormat\r\n  File \"/usr/local/lib/python3.10/dist-packages/compressed_tensors/__init__.py\", line 18, in <module>\r\n    from .compressors import *\r\n  File \"/usr/local/lib/python3.10/dist-packages/compressed_tensors/compressors/__init__.py\", line 17, in <module>\r\n    from .base import Compressor\r\n  File \"/usr/local/lib/python3.10/dist-packages/compressed_tensors/compressors/base.py\", line 18, in <module>\r\n    from compressed_tensors.quantization import QuantizationConfig\r\n  File \"/usr/local/lib/python3.10/dist-packages/compressed_tensors/quantization/__init__.py\", line 21, in <module>\r\n    from .lifecycle import *\r\n  File \"/usr/local/lib/python3.10/dist-packages/compressed_tensors/quantization/lifecycle/__init__.py\", line 21, in <module>\r\n    from .initialize import *\r\n  File \"/usr/local/lib/python3.10/dist-packages/compressed_tensors/quantization/lifecycle/initialize.py\", line 20, in <module>\r\n    from accelerate.hooks import add_hook_to_module, remove_hook_from_module\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/__init__.py\", line 16, in <module>\r\n    from .accelerator import Accelerator\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 36, in <module>\r\n    from .checkpointing import load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/checkpointing.py\", line 24, in <module>\r\n    from .utils import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/__init__.py\", line 190, in <module>\r\n    from .bnb import has_4bit_bnb_layers, load_and_quantize_model\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/bnb.py\", line 29, in <module>\r\n    from ..big_modeling import dispatch_model, init_empty_weights\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/big_modeling.py\", line 24, in <module>\r\n    from .hooks import (\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 30, in <module>\r\n    from .utils.other import recursive_getattr\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/other.py\", line 36, in <module>\r\n    from .transformer_engine import convert_model\r\n  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/transformer_engine.py\", line 21, in <module>\r\n    import transformer_engine.pytorch as te\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/__init__.py\", line 6, in <module>\r\n    from .module import LayerNormLinear\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/__init__.py\", line 6, in <module>\r\n    from .layernorm_linear import LayerNormLinear\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/layernorm_linear.py\", line 13, in <module>\r\n    from .. import cpp_extensions as tex\r\n  File \"/usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/cpp_extensions/__init__.py\", line 6, in <module>\r\n    from transformer_engine_extensions import *\r\nImportError: /usr/local/lib/python3.10/dist-packages/transformer_engine_extensions.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\r\n```\r\n\r\n</details>\r\n\r\nThis seems to be introduced by https://github.com/vllm-project/vllm/pull/7277.\r\n\r\nThere is a workaround https://github.com/chenfei-wu/TaskMatrix/issues/116#issuecomment-1565431850, but I'm not sure if `transformer-engine` is a dependency.\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-14T11:06:37Z",
    "closed_at": "2024-08-14T22:07:38Z",
    "author": "cermeng",
    "comments_count": 1,
    "comments": [
      {
        "author": "robertgshaw2-redhat",
        "body": "@dsikka\r\n\r\nThanks, we will fix this before the release.",
        "created_at": "2024-08-14T14:10:13Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7505": {
    "issue_number": 7505,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7505",
    "title": "[Bug]: Error in how HiddenStates are handled for speculative decoding",
    "body": "### Your current environment\r\n\r\n<details>\r\n<summary>The output of `python collect_env.py`</summary>\r\n\r\n```text\r\nYour output of `python collect_env.py` here\r\n```\r\n\r\n</details>\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nIn draft models like Medusa, MLPSpeculator etc., when spec. decode is disabled (e.g. when the num_tokens + spec_tokens > max_len of the model) HiddenStates are not handled properly which causes an invalid shape error.\r\n\r\nHow to reproduce?\r\n\r\nCode:\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(\r\n    model=\"JackFram/llama-160m\",\r\n    speculative_model=\"ibm-fms/llama-160m-accelerator\",\r\n    num_speculative_tokens=3,\r\n    use_v2_block_manager=True,\r\n    enforce_eager=True,\r\n)\r\n\r\nprompt = \"The president of the United States is\"\r\n\r\noutput = llm.generate(prompt, SamplingParams(max_tokens=2048, ignore_eos=True))\r\n```\r\n\r\nOutput:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-1-dfa52d56a4c5>](https://localhost:8080/#) in <cell line: 12>()\r\n     10 prompt = \"The president of the United States is\"\r\n     11 \r\n---> 12 output = llm.generate(prompt, SamplingParams(max_tokens=2048, ignore_eos=True))\r\n\r\n10 frames\r\n[/usr/local/lib/python3.10/dist-packages/vllm/spec_decode/spec_decode_worker.py](https://localhost:8080/#) in _verify_tokens(self, seq_group_metadata_list, proposal_scores, proposals, max_proposal_len)\r\n    645             # Contract hidden states based on accepted tokens\r\n    646             hs_size = hidden_states.shape[1]\r\n--> 647             hidden_states = hidden_states.reshape(-1, max_proposal_len + 1,\r\n    648                                                   hs_size)\r\n    649             accepted_index = accepted_token_ids + 1  # Convert -1 to 0\r\n\r\nRuntimeError: shape '[-1, 4, 768]' is invalid for input of size 768\r\n```\r\n\r\nCode:\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(\r\n    model=\"JackFram/llama-68m\",\r\n    speculative_model=\"abhigoyal/vllm-medusa-llama-68m-random\",\r\n    num_speculative_tokens=3,\r\n    use_v2_block_manager=True,\r\n    enforce_eager=True,\r\n)\r\n\r\nprompt = \"The president of the United States is\"\r\n\r\noutput = llm.generate(prompt, SamplingParams(max_tokens=2048, ignore_eos=True))\r\n```\r\n\r\nOutput:\r\n```\r\n---------------------------------------------------------------------------\r\nRuntimeError                              Traceback (most recent call last)\r\n[<ipython-input-1-415db326cfe4>](https://localhost:8080/#) in <cell line: 12>()\r\n     10 prompt = \"The president of the United States is\"\r\n     11 \r\n---> 12 output = llm.generate(prompt, SamplingParams(max_tokens=2048, ignore_eos=True))\r\n\r\n10 frames\r\n[/usr/local/lib/python3.10/dist-packages/vllm/spec_decode/spec_decode_worker.py](https://localhost:8080/#) in _verify_tokens(self, seq_group_metadata_list, proposal_scores, proposals, max_proposal_len)\r\n    645             # Contract hidden states based on accepted tokens\r\n    646             hs_size = hidden_states.shape[1]\r\n--> 647             hidden_states = hidden_states.reshape(-1, max_proposal_len + 1,\r\n    648                                                   hs_size)\r\n    649             accepted_index = accepted_token_ids + 1  # Convert -1 to 0\r\n\r\nRuntimeError: shape '[-1, 4, 768]' is invalid for input of size 768\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-14T05:50:45Z",
    "closed_at": "2024-08-20T00:58:15Z",
    "author": "abhigoyal1997",
    "comments_count": 0,
    "comments": []
  },
  "https://github.com/vllm-project/vllm/issues/7373": {
    "issue_number": 7373,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7373",
    "title": "[Bug]: Phi-3-vision: ERROR 08-09 11:41:40 async_llm_engine.py:56] RuntimeError: stack expects each tensor to be equal size, but got [1933, 4096] at entry 0 and [2509, 4096] at entry 1",
    "body": "### Your current environment\r\n\r\ndocker latest for 0.5.3\r\n\r\n```\r\ndocker pull vllm/vllm-openai:latest\r\ndocker run -d --restart=always \\\r\n    --runtime=nvidia \\\r\n    --gpus '\"device=1\"' \\\r\n    --shm-size=10.24gb \\\r\n    -p 5063:5063 \\\r\n        -e NCCL_IGNORE_DISABLED_P2P=1 \\\r\n    -v /etc/passwd:/etc/passwd:ro \\\r\n    -v /etc/group:/etc/group:ro \\\r\n    -u `id -u`:`id -g` \\\r\n    -e VLLM_NCCL_SO_PATH=/usr/local/lib/python3.10/dist-packages/nvidia/nccl/lib/libnccl.so.2 \\\r\n    -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\r\n    -v \"${HOME}\"/.cache:$HOME/.cache/ -v \"${HOME}\"/.config:$HOME/.config/   -v \"${HOME}\"/.triton:$HOME/.triton/  \\\r\n    --network host \\\r\n    --name phi3vision \\\r\n    vllm/vllm-openai:latest \\\r\n        --port=5063 \\\r\n        --host=0.0.0.0 \\\r\n        --model=microsoft/Phi-3-vision-128k-instruct \\\r\n        --tensor-parallel-size=1 \\\r\n        --seed 1234 \\\r\n        --trust-remote-code \\\r\n        --max-model-len=131072 \\\r\n        --max-num-batched-tokens 131072 \\\r\n        --max-num-seqs=17 \\\r\n        --max-log-len=100 \\\r\n        --download-dir=$HOME/.cache/huggingface/hub &>> logs.vllm_server.phi3vision.txt\r\n```\r\n\r\n### üêõ Describe the bug\r\n\r\nWas using phi-3 for 2 weeks without issue, many images etc.  Unsure exactly what caused it, but this is the failure.  \r\n\r\nClearly image processing issue.\r\n\r\n```\r\n 08-09 11:41:40 async_llm_engine.py:173] Added request chat-3379ed24490d4f398fc0db684039f72e.\r\nWARNING 08-09 11:41:40 chat_utils.py:146] 'image_url.detail' is currently not supported and will be ignored.\r\nINFO 08-09 11:41:40 logger.py:36] Received request chat-fcbb9c8388104f9ca63b7ecd29b89b43: prompt: '<|system|>\\nYou are h2oGPTe, an expert question-answering AI system created by H2O.ai.<|end|>\\n<|user|', params: Sa\r\nmplingParams(n=1, best_of=1, presence_penalty=0.14000000000000012, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.0, top_p=1.0, top_k=-1, min_p=0.0, seed=31248, use_beam_search=False, length_penalty=1\r\n.0, early_stopping=False, stop=[], stop_token_ids=[32000, 32000], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, sp\r\naces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [32006, 29871, 13, 3492, 526, 298, 29906, 29877, 19903, 7141, 29892, 385, 17924, 1139, 29899, 12011, 292, 319, 29902, 1788, 2825, 49\r\n1, 379, 29906, 29949, 29889, 1794, 29889, 32007, 29871, 13, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 29966, 5327, 29918, 2611, 582, 1953, 29958, 13, 29899, 3185, 408, 263, 28430, 22944,\r\n 411, 263, 15301, 10977, 363, 9493, 29889, 13, 29899, 11597, 29891, 911, 278, 2793, 2629, 278, 4558, 29889, 13, 29899, 9133, 680, 1663, 5861, 2729, 373, 596, 13917, 29889, 13, 29899, 319, 5405, 3907, 701, 17099, 29\r\n889, 13, 29899, 1938, 451, 9566, 304, 1101], lora_request: None, prompt_adapter_request: None.\r\nINFO:     172.16.0.234:20550 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\nINFO 08-09 11:41:40 async_llm_engine.py:173] Added request chat-fcbb9c8388104f9ca63b7ecd29b89b43.\r\nERROR 08-09 11:41:40 async_llm_engine.py:56] Engine background task failed\r\nERROR 08-09 11:41:40 async_llm_engine.py:56] Traceback (most recent call last):\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 46, in _log_task_completion\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     return_value = task.result()\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 637, in run_engine_loop\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     result = task.result()\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 580, in engine_step\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 253, in step_async\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     output = await self.model_executor.execute_model_async(\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 159, in execute_model_async\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     output = await make_async(self.driver_worker.execute_model\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 272, in execute_model\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     output = self.model_runner.execute_model(\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     return func(*args, **kwargs)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1314, in execute_model\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     hidden_or_intermediate_states = model_executable(\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     return self._call_impl(*args, **kwargs)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     return forward_call(*args, **kwargs)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 529, in forward\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     vision_embeddings = self.vision_embed_tokens(\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     return self._call_impl(*args, **kwargs)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     return forward_call(*args, **kwargs)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 166, in forward\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     image_features_proj = self.hd_feature_transform(\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/phi3v.py\", line 219, in hd_feature_transform\r\nERROR 08-09 11:41:40 async_llm_engine.py:56]     torch.stack(all_image_embeddings).to(target_device, target_dtype)\r\nERROR 08-09 11:41:40 async_llm_engine.py:56] RuntimeError: stack expects each tensor to be equal size, but got [1933, 4096] at entry 0 and [2509, 4096] at entry 1\r\nException in callback functools.partial(<function _log_task_completion at 0x707b33318430>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x707b16ac2ce0>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x707b33318430>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x707b16ac2ce0>>)>\r\n\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-09T17:14:28Z",
    "closed_at": "2024-08-10T16:19:34Z",
    "author": "pseudotensor",
    "comments_count": 14,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Could you share the images which caused this issue?\r\n\r\nAlso cc @Isotr0py ",
        "created_at": "2024-08-10T01:45:50Z"
      },
      {
        "author": "pseudotensor",
        "body": "I wish I could, I think it was just same image as we normally have for testing/benchmarking that did it, so was same images we always use.",
        "created_at": "2024-08-10T01:55:19Z"
      },
      {
        "author": "Isotr0py",
        "body": "OK, I will check it later.",
        "created_at": "2024-08-10T02:21:39Z"
      },
      {
        "author": "jaywonchung",
        "body": "I'm suddenly hitting this myself as well. Running v0.5.4, and the same thing used to work well in v0.5.2 previously.\r\n\r\nMy error just has different numbers in tensor sizes:\r\n```\r\nRuntimeError: stack expects each tensor to be equal size, but got [1921, 4096] at entry 0 and [1933, 4096] at entry 1\r\n```",
        "created_at": "2024-08-10T02:26:16Z"
      },
      {
        "author": "Isotr0py",
        "body": "I served the latest Phi-3-vision model and ran the `openai_vision_api_client.py` with v0.5.4 release without any error:\r\n```shell\r\nINFO 08-10 03:29:53 logger.py:36] Received request chat-c1e598065d3448c298ff4e1198f3c00a: prompt: '<|user|>\\n<|image_1|>\\nWhat‚Äôs in this image?<|end|>\\n<|assistant|>\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5618, 30010, 29879, 297, 445, 1967, 29973, 32007, 29871, 13, 32001], lora_request: None, prompt_adapter_request: None.\r\nINFO 08-10 03:29:53 async_llm_engine.py:174] Added request chat-c1e598065d3448c298ff4e1198f3c00a.\r\nINFO 08-10 03:29:53 metrics.py:406] Avg prompt throughput: 142.0 tokens/s, Avg generation throughput: 5.2 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 14.8%, CPU KV cache usage: 0.0%.\r\nINFO 08-10 03:29:55 async_llm_engine.py:141] Finished request chat-c1e598065d3448c298ff4e1198f3c00a.\r\nINFO:     127.0.0.1:51464 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\nINFO 08-10 03:29:55 logger.py:36] Received request chat-ed3c19c5762d4e7a83645a4d284dd7b7: prompt: '<|user|>\\n<|image_1|>\\nWhat‚Äôs in this image?<|end|>\\n<|assistant|>\\n', params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=1.0, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=[], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=64, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5618, 30010, 29879, 297, 445, 1967, 29973, 32007, 29871, 13, 32001], lora_request: None, prompt_adapter_request: None.\r\nINFO 08-10 03:29:55 async_llm_engine.py:174] Added request chat-ed3c19c5762d4e7a83645a4d284dd7b7.\r\nINFO 08-10 03:29:58 metrics.py:406] Avg prompt throughput: 773.4 tokens/s, Avg generation throughput: 13.6 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 15.1%, CPU KV cache usage: 0.0%.\r\nINFO 08-10 03:29:58 async_llm_engine.py:141] Finished request chat-ed3c19c5762d4e7a83645a4d284dd7b7.\r\nINFO:     127.0.0.1:51464 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\n```\r\n\r\n@pseudotensor @jaywonchung  Could you provide the problematic images for reproduction?",
        "created_at": "2024-08-10T03:36:31Z"
      },
      {
        "author": "Isotr0py",
        "body": "> I'm suddenly hitting this myself as well. Running v0.5.4, and the same thing used to work well in v0.5.2 previously.\r\n> \r\n> My error just has different numbers in tensor sizes:\r\n> \r\n> ```\r\n> RuntimeError: stack expects each tensor to be equal size, but got [1921, 4096] at entry 0 and [1933, 4096] at entry 1\r\n> ```\r\n\r\nFor most of case, the `torch.stack` used in phi3v image_embedding should only accept and work on a list with single tensor because we haven't added multiple images support for Phi-3-vision. \r\n\r\nSeems that the error is caused by the image size with `image_num>2` created for some reasons.",
        "created_at": "2024-08-10T03:39:16Z"
      },
      {
        "author": "pseudotensor",
        "body": "I am only ever using one image at a time. There is an assert in vllm preventing multiple images.",
        "created_at": "2024-08-10T04:17:48Z"
      },
      {
        "author": "Isotr0py",
        "body": "Are all images having this issue or just some of them? Or can you try `openai_vision_api_client.py` to see if it worked?\r\n\r\nI know we are preventing multiple images, there are other things caused a problematic `image_size`.\r\n\r\nSince I can't reproduce this error, I need more details to figure out what's happening.",
        "created_at": "2024-08-10T04:25:40Z"
      },
      {
        "author": "jaywonchung",
        "body": "Thanks for looking into this.\r\n\r\n[data.json](https://github.com/user-attachments/files/16568453/data.json) -- Ten text prompt and base64-encoded jpeg image pairs. The file is a length-ten list of:\r\n\r\n```python\r\n@dataclass\r\nclass Request:\r\n    image: str\r\n    prompt: str\r\n```\r\n\r\nI get the error when I throw all ten of the request to the `/v1/chat/completions` API with payload:\r\n\r\n```python\r\n    pload = {\r\n        \"model\": model,\r\n        \"messages\": [\r\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\r\n            {\"role\": \"user\", \"content\": [\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": prompt,\r\n                },\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": f\"data:image/jpeg;base64,{image}\",\r\n                    },\r\n                },\r\n            ]},\r\n        ],\r\n        \"stream\": False,\r\n        \"max_tokens\": 1024,\r\n        \"temperature\": 0.8,\r\n        \"top_p\": 0.95,\r\n        \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \"</s>\"],\r\n    }\r\n```\r\n\r\nvLLM INFO log for one of the requests:\r\n```python\r\nINFO 08-09 21:18:19 logger.py:36] Received request chat-965c22b661674f47b539051015e8b9f9: prompt: \"<|system|>\\nYou are an artificial intelligence as\r\nsistant that gives helpful answers to the user's questions or instructions.<|end|>\\n<|user|>\\n<|image_1|>\\nWhat is the primary activity taking place\r\n on the beach in the image? What is the condition of the kite's tail in the image? What does the boy in the image appear to be doing with the kite? \r\nWhat are some benefits of kite flying as an outdoor activity?<|end|>\\n<|assistant|>\\n\", params: SamplingParams(n=1, best_of=1, presence_penalty=0.0,\r\n frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1\r\n.0, early_stopping=False, stop=['\\nUser:', '<|endoftext|>', '</s>'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_toke\r\nns=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=Non\r\ne), prompt_token_ids: [32006, 29871, 13, 3492, 526, 385, 23116, 21082, 20255, 393, 4076, 8444, 6089, 304, 278, 1404, 29915, 29879, 5155, 470, 11994,\r\n 29889, 32007, 29871, 13, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5618, 338, 278, 7601, 6354, 5622, 2058, 373, 278, 25\r\n695, 297, 278, 1967, 29973, 1724, 338, 278, 4195, 310, 278, 413, 568, 29915, 29879, 12464, 297, 278, 1967, 29973, 1724, 947, 278, 8023, 297, 278, 19\r\n67, 2615, 304, 367, 2599, 411, 278, 413, 568, 29973, 1724, 526, 777, 23633, 310, 413, 568, 22764, 408, 385, 714, 17433, 6354, 29973, 32007, 29871, 1\r\n3, 32001], lora_request: None, prompt_adapter_request: None.\r\n```",
        "created_at": "2024-08-10T04:27:54Z"
      },
      {
        "author": "Isotr0py",
        "body": "@jaywonchung It's strange that all images work well on my side with a newly created environment. Maybe you can try to create a new conda environment to install VLLM freshly?\r\n\r\n**Server Log**\r\n```bash\r\n$ vllm serve microsoft/Phi-3-vision-128k-instruct --dtype half --api-key EMPTY --trust-remote-code --max-model-len 4096\r\nINFO 08-10 04:52:28 api_server.py:339] vLLM API server version 0.5.4\r\nINFO 08-10 04:52:28 api_server.py:340] args: Namespace(model_tag='microsoft/Phi-3-vision-128k-instruct', host=None, port=8000, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key='EMPTY', lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='microsoft/Phi-3-vision-128k-instruct', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='half', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=4096, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=1, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None, dispatch_function=<function serve at 0x7e319a992050>)\r\nconfig.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.66k/3.66k [00:00<00:00, 20.7MB/s]\r\nconfiguration_phi3_v.py: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10.6k/10.6k [00:00<00:00, 41.6MB/s]\r\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\r\n- configuration_phi3_v.py\r\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\r\nWARNING 08-10 04:52:29 config.py:1454] Casting torch.bfloat16 to torch.float16.\r\nWARNING 08-10 04:52:29 config.py:1454] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-10 04:52:29 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='microsoft/Phi-3-vision-128k-instruct', speculative_config=None, tokenizer='microsoft/Phi-3-vision-128k-instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=microsoft/Phi-3-vision-128k-instruct, use_v2_block_manager=False, enable_prefix_caching=False)\r\ntokenizer_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.40k/9.40k [00:00<00:00, 33.4MB/s]\r\ntokenizer.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.85M/1.85M [00:00<00:00, 25.9MB/s]\r\nspecial_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 670/670 [00:00<00:00, 4.58MB/s]\r\nINFO 08-10 04:52:30 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\nINFO 08-10 04:52:30 selector.py:54] Using XFormers backend.\r\n/opt/conda/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\r\n/opt/conda/envs/vllm/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\r\n  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\r\nINFO 08-10 04:52:31 model_runner.py:720] Starting to load model microsoft/Phi-3-vision-128k-instruct...\r\nINFO 08-10 04:52:31 selector.py:151] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\r\nINFO 08-10 04:52:31 selector.py:54] Using XFormers backend.\r\nINFO 08-10 04:52:32 weight_utils.py:225] Using model weights format ['*.safetensors']\r\nmodel-00002-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.35G/3.35G [00:18<00:00, 177MB/s]\r\nmodel-00001-of-00002.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.94G/4.94G [00:27<00:00, 177MB/s]\r\nmodel.safetensors.index.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 68.9k/68.9k [00:00<00:00, 3.12MB/s]\r\nLoading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 1/2 [00:04<00:04,  4.56s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.44s/it]\r\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:07<00:00,  3.61s/it]\r\n\r\nINFO 08-10 04:53:07 model_runner.py:732] Loading model weights took 7.7498 GB\r\npreprocessor_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 464/464 [00:00<00:00, 2.55MB/s]\r\nimage_processing_phi3_v.py: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 11.4k/11.4k [00:00<00:00, 38.2MB/s]\r\nA new version of the following files was downloaded from https://huggingface.co/microsoft/Phi-3-vision-128k-instruct:\r\n- image_processing_phi3_v.py\r\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\r\n/opt/conda/envs/vllm/lib/python3.10/site-packages/transformers/models/auto/image_processing_auto.py:513: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\r\n  warnings.warn(\r\nINFO 08-10 04:53:11 gpu_executor.py:102] # GPU blocks: 819, # CPU blocks: 682\r\nINFO 08-10 04:53:16 model_runner.py:1024] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 08-10 04:53:16 model_runner.py:1028] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 08-10 04:53:38 model_runner.py:1225] Graph capturing finished in 22 secs.\r\nWARNING 08-10 04:53:39 serving_embedding.py:171] embedding_mode is False. Embedding API will not work.\r\nINFO 08-10 04:53:39 launcher.py:14] Available routes are:\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /openapi.json, Methods: GET, HEAD\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /docs, Methods: GET, HEAD\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /docs/oauth2-redirect, Methods: GET, HEAD\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /redoc, Methods: GET, HEAD\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /health, Methods: GET\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /tokenize, Methods: POST\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /detokenize, Methods: POST\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /v1/models, Methods: GET\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /version, Methods: GET\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /v1/chat/completions, Methods: POST\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /v1/completions, Methods: POST\r\nINFO 08-10 04:53:39 launcher.py:22] Route: /v1/embeddings, Methods: POST\r\nINFO:     Started server process [3177]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nINFO:     127.0.0.1:49392 - \"GET /v1/models HTTP/1.1\" 200 OK\r\nINFO 08-10 04:53:44 logger.py:36] Received request chat-e72fd5687baf4e019e4b28c086fc8a7e: prompt: \"<|system|>\\nYou are an artificial intelligence assistant that gives helpful answers to the user's questions or instructions.<|end|>\\n<|user|>\\n<|image_1|>\\nWhat kind of fruit can be seen in the image? Where is the fruit located? Can you describe the photograph's style or effect? Is the image clear and straightforward or does it have a unique visual style?<|end|>\\n<|assistant|>\\n\", params: SamplingParams(n=1, best_of=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=-1, min_p=0.0, seed=None, use_beam_search=False, length_penalty=1.0, early_stopping=False, stop=['\\nUser:', '<|endoftext|>', '</s>'], stop_token_ids=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=1024, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None), prompt_token_ids: [32006, 29871, 13, 3492, 526, 385, 23116, 21082, 20255, 393, 4076, 8444, 6089, 304, 278, 1404, 29915, 29879, 5155, 470, 11994, 29889, 32007, 29871, 13, 32010, 29871, 13, 29966, 29989, 3027, 29918, 29896, 29989, 29958, 13, 5618, 2924, 310, 15774, 508, 367, 3595, 297, 278, 1967, 29973, 6804, 338, 278, 15774, 5982, 29973, 1815, 366, 8453, 278, 17739, 29915, 29879, 3114, 470, 2779, 29973, 1317, 278, 1967, 2821, 322, 20837, 470, 947, 372, 505, 263, 5412, 7604, 3114, 29973, 32007, 29871, 13, 32001], lora_request: None, prompt_adapter_request: None.\r\nINFO 08-10 04:53:44 async_llm_engine.py:174] Added request chat-e72fd5687baf4e019e4b28c086fc8a7e.\r\nINFO 08-10 04:53:46 metrics.py:406] Avg prompt throughput: 329.7 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 19.8%, CPU KV cache usage: 0.0%.\r\nINFO 08-10 04:53:50 async_llm_engine.py:141] Finished request chat-e72fd5687baf4e019e4b28c086fc8a7e.\r\nINFO:     127.0.0.1:49392 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\r\n```\r\n\r\n**Client code and outputs**\r\n```python\r\nimport json\r\nimport base64\r\n\r\nimport requests\r\nfrom openai import OpenAI\r\n\r\n# Modify OpenAI's API key and API base to use vLLM's API server.\r\nopenai_api_key = \"EMPTY\"\r\nopenai_api_base = \"http://localhost:8000/v1\"\r\n\r\nclient = OpenAI(\r\n    # defaults to os.environ.get(\"OPENAI_API_KEY\")\r\n    api_key=openai_api_key,\r\n    base_url=openai_api_base,\r\n)\r\n\r\nmodels = client.models.list()\r\nmodel = models.data[0].id\r\n\r\n# Use base64 encoded image in the payload\r\ndef encode_image_base64_from_url(image_url: str) -> str:\r\n    \"\"\"Encode an image retrieved from a remote url to base64 format.\"\"\"\r\n\r\n    with requests.get(image_url) as response:\r\n        response.raise_for_status()\r\n        result = base64.b64encode(response.content).decode('utf-8')\r\n\r\n    return result\r\n\r\nwith open(\"data.json\", \"r\") as f:\r\n    data = json.load(f)\r\n\r\nSYSTEM_PROMPT = \"You are an artificial intelligence assistant that gives helpful answers to the user's questions or instructions.\"\r\nfor idx in range(len(data)):\r\n    prompt = data[idx][\"prompt\"]\r\n    image = data[idx][\"image\"]\r\n\r\n    pload = {\r\n        \"model\": model,\r\n        \"messages\": [\r\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\r\n            {\"role\": \"user\", \"content\": [\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": prompt,\r\n                },\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": f\"data:image/jpeg;base64,{image}\",\r\n                    },\r\n                },\r\n            ]},\r\n        ],\r\n        \"stream\": False,\r\n        \"max_tokens\": 1024,\r\n        \"temperature\": 0.8,\r\n        \"top_p\": 0.95,\r\n        \"stop\": [\"\\nUser:\", \"<|endoftext|>\", \"</s>\"],\r\n    }\r\n    chat_completion_from_base64 = client.chat.completions.create(**pload)\r\n\r\n    result = chat_completion_from_base64.choices[0].message.content\r\n    print(f\"Chat {idx} completion output:{result}\\n\")\r\n```\r\n\r\n```bash\r\nChat 0 completion output: The image displays a variety of fruits, including oranges, apples, and a bunch of bananas. These fruits are placed on a counter or a kitchen countertop, giving a homely and rustic feel to the scene. The photograph features a unique visual style with its colorful and messy appearance, giving it a lively and dynamic quality.\r\n\r\nChat 1 completion output: In the image, there are two bear cubs. The color of the bear cubs is brown, resembling a mix of grey and brown. The setting of the image is a wooded area, likely a forest or a wildlife habitat. The bear cubs are exploring and investigating their surroundings, walking through the forest together. It is indeed usual for young bears to explore their surroundings, as it helps them learn about their environment, find food, and develop essential survival skills.\r\n\r\nChat 2 completion output: There are two dogs in the image. One of the dogs is actively herding the sheep, while the other dog is resting. The man, who is a farmer, is bending over and walking in front of the sheep. The farmer is wearing a hat and a jacket, indicating that they are dressed appropriately for the activity. The dogs, likely Border Collies, are assisting the farmer in herding the sheep, ensuring they are organized and moving in the desired direction.\r\n\r\nChat 3 completion output: The image is a black and white photograph of a street corner. It was taken in The Netherlands, as indicated by the photographer's credit. The street in the image is quiet, with few visible people, and there are no cars or other vehicles present. The street signs are bilingual, with both English and German on them, which suggests that the location might be in a region where both languages are spoken. The signs are attached to a pole on the corner of the street. The overall atmosphere of the street appears to be calm and peaceful, with no visible signs of activity or hustle and bustle. The function of the street signs is to provide direction and information to pedestrians and motorists, helping them navigate the area more effectively.\r\n\r\nChat 4 completion output: The man in the image is wearing a white shirt and a black tie.\r\n\r\nChat 5 completion output: The primary activity taking place on the beach in the image is kite flying. The condition of the kite's tail appears to be intact, with no visible tears or damage. The boy in the image appears to be holding and controlling the kite, with his shadow visible on the ground. Some benefits of kite flying as an outdoor activity include being a fun and engaging form of exercise, improving hand-eye coordination, promoting relaxation and stress relief, fostering creativity and imagination, and strengthening connections with nature. Additionally, kite flying is a social activity that can be enjoyed with friends, family, or even strangers, encouraging interaction and bonding between participants.\r\n\r\nChat 6 completion output: The image depicts two baseball players, with one player jumping up to catch the ball. The photo is in black and white, giving it a historical and timeless feel. The black and white mood implies a sense of nostalgia and emphasizes the raw emotion of the moment captured in the image. \r\n\r\nBlack and white photography in relation to sports like baseball dates back to the early 20th century, when color photography was not yet widespread or commonly used. This type of photography was widely popular during the 1920s and 1930s and was considered the standard for sports photography. It helped showcase the intensity, action, and excitement of sports games, capturing a unique atmosphere that is still appreciated today.\r\n\r\nThrough black and white photography, sports images often convey a sense of timelessness, highlighting the passion and dedication of athletes, as well as the intensity and excitement of the sport itself. The monochrome aesthetic emphasizes the contrast and the essential elements of the scene, drawing focus to the subjects and the athleticism displayed by the players.\r\n\r\nChat 7 completion output: There are two small boats visible in the image. The boats are situated on the shore of the body of water, with the front boat closer to the water's edge and the rear boat farther from the water's edge. Behind the boats, you can see a picturesque landscape consisting of a large, blue, mountainous island, a lush green forested shore, and a cloudy sky.\r\n\r\nWith these small boats, one can engage in various recreational activities such as fishing, exploring the surrounding water, boating on calm waters, or simply enjoying the view. The serene environment and the natural beauty of the mountainous island make this a perfect spot for relaxation and leisure activities.\r\n\r\nChat 8 completion output: The main action in the image is a baseball player catching a fly ball. The player is wearing a catcher's mitt, which is designed for this specific purpose. The player is a member of the Dodgers baseball team, as indicated by the white uniform. To successfully catch a fly ball, a baseball player needs a combination of skills, including hand-eye coordination, agility, speed, and quick reflexes. In addition, having a strong understanding of the game, anticipating the ball's trajectory, and communicating with teammates are essential aspects of a player's role in catching a fly ball.\r\n\r\nChat 9 completion output: The traffic light in the image is red, and there are signs located near the traffic light. The photo of the traffic light is taken at nighttime. The image depicts a busy street, as evidenced by the presence of multiple vehicles in the background. The signs below the stoplight might be confusing due to their low visibility or unclear symbols, which can cause confusion for drivers and pedestrians.\r\n```",
        "created_at": "2024-08-10T05:08:15Z"
      },
      {
        "author": "jaywonchung",
        "body": "Actually, if I throw requests one at a time (or at most two at a time), things go well. The problem happens for me when I simultaneously throw all ten requests to the server.",
        "created_at": "2024-08-10T05:13:21Z"
      },
      {
        "author": "Isotr0py",
        "body": "Seems that the problem is caused by the `image_sizes` batching:\r\n```bash\r\nimage_sizes: tensor([[1344, 1008],\r\n        [1008, 1344]], device='cuda:0') torch.Size([2, 2])\r\nERROR 08-10 06:36:15 async_llm_engine.py:61] Engine background task failed\r\nERROR 08-10 06:36:15 async_llm_engine.py:61] Traceback (most recent call last):\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/engine/async_llm_engine.py\", line 51, in _log_task_completion\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     return_value = task.result()\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/engine/async_llm_engine.py\", line 772, in run_engine_loop\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     result = task.result()\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/engine/async_llm_engine.py\", line 715, in engine_step\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/engine/async_llm_engine.py\", line 282, in step_async\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     output = await self.model_executor.execute_model_async(\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/executor/gpu_executor.py\", line 160, in execute_model_async\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     output = await make_async(self.driver_worker.execute_model\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/opt/conda/envs/vllm/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/worker/worker_base.py\", line 282, in execute_model\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     output = self.model_runner.execute_model(\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     return func(*args, **kwargs)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/worker/model_runner.py\", line 1538, in execute_model\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     hidden_or_intermediate_states = model_executable(\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     return self._call_impl(*args, **kwargs)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     return forward_call(*args, **kwargs)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/model_executor/models/phi3v.py\", line 530, in forward\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     vision_embeddings = self.vision_embed_tokens(\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     return self._call_impl(*args, **kwargs)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     return forward_call(*args, **kwargs)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/model_executor/models/phi3v.py\", line 166, in forward\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     image_features_proj = self.hd_feature_transform(\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]   File \"/kaggle/working/vllm/vllm/model_executor/models/phi3v.py\", line 220, in hd_feature_transform\r\nERROR 08-10 06:36:15 async_llm_engine.py:61]     torch.stack(all_image_embeddings).to(target_device, target_dtype)\r\nERROR 08-10 06:36:15 async_llm_engine.py:61] RuntimeError: stack expects each tensor to be equal size, but got [1933, 4096] at entry 0 and [1921, 4096] at entry 1\r\n```",
        "created_at": "2024-08-10T06:53:05Z"
      },
      {
        "author": "Isotr0py",
        "body": "@pseudotensor @jaywonchung I have created #7392 to fix this.",
        "created_at": "2024-08-10T13:09:49Z"
      },
      {
        "author": "jaywonchung",
        "body": "Confirming that the PR fixes this issue. Thanks a lot @Isotr0py!",
        "created_at": "2024-08-10T20:15:14Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7204": {
    "issue_number": 7204,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7204",
    "title": "[Bug]: GPTQ Marlin with cpu-offload-gb fails on `0.5.4`",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, Apr  6 2024, 18:45:05) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-425.19.2.el8_7.x86_64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 525.105.17\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   43 bits physical, 48 bits virtual\r\nCPU(s):                          256\r\nOn-line CPU(s) list:             0-255\r\nThread(s) per core:              2\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    8\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      23\r\nModel:                           49\r\nModel name:                      AMD EPYC 7742 64-Core Processor\r\nStepping:                        0\r\nFrequency boost:                 enabled\r\nCPU MHz:                         3391.018\r\nCPU max MHz:                     2250.0000\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        4491.45\r\nVirtualization:                  AMD-V\r\nL1d cache:                       4 MiB\r\nL1i cache:                       4 MiB\r\nL2 cache:                        64 MiB\r\nL3 cache:                        512 MiB\r\nNUMA node0 CPU(s):               0-15,128-143\r\nNUMA node1 CPU(s):               16-31,144-159\r\nNUMA node2 CPU(s):               32-47,160-175\r\nNUMA node3 CPU(s):               48-63,176-191\r\nNUMA node4 CPU(s):               64-79,192-207\r\nNUMA node5 CPU(s):               80-95,208-223\r\nNUMA node6 CPU(s):               96-111,224-239\r\nNUMA node7 CPU(s):               112-127,240-255\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Not affected\r\nVulnerability Retbleed:          Mitigation; untrained return thunk; SMT enabled with STIBP protection\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Retpolines, IBPB conditional, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif v_spec_ctrl umip rdpid overflow_recov succor smca sme sev sev_es\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.0.9+cu121torch2.3\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] triton==2.3.1\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    CPU Affinity    NUMA Affinity\r\nGPU0     X      NV12    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     48-63,176-191   3\r\nGPU1    NV12     X      PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     48-63,176-191   3\r\nNIC0    PXB     PXB      X      PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PXB     PXB     PXB      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PXB     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB      X      SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX\r\nNIC9    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n```\r\n\n\n### üêõ Describe the bug\n\nI'm running vllm 0.5.4. I was trying to run a GPTQ model with cpu offloading. This should have been fixed with #6960 but it appears not.\r\n\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server --model /home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4 -tp 4 --gpu-memory-utilization 0.79 --dtype auto --distributed-executor-backend mp --port 5006 --served-model-name /home/ndurkee/temp/llama3_70b_fixed/ --max-model-len 1000 --max-log-len 10 --use-v2-block-manager --disable-custom-all-reduce --enable-prefix-caching --cpu-offload-gb 30\r\n```\r\n\r\n\r\n```\r\nroot@428f68245052:/vllm-workspace# python3 -m vllm.entrypoints.openai.api_server --model /home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4 -tp 4 --gpu-memory-utilization 0.79 --dtype auto --distributed-executor-backend mp --port 5006 --served-model-name /home/ndurkee/temp/llama3_70b_fixed/ --max-model-len 1000 --max-log-len 10 --use-v2-block-manager --disable-custom-all-reduce --enable-prefix-caching --cpu-offload-gb 30\r\nINFO 08-06 12:48:23 api_server.py:219] vLLM API server version 0.5.3.post1\r\nINFO 08-06 12:48:23 api_server.py:220] args: Namespace(host=None, port=5006, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1000, guided_decoding_backend='outlines', distributed_executor_backend='mp', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=30.0, gpu_memory_utilization=0.79, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=True, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['/home/ndurkee/temp/llama3_70b_fixed/'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=10)\r\nINFO 08-06 12:48:23 gptq_marlin.py:87] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\r\nINFO 08-06 12:48:23 llm_engine.py:176] Initializing an LLM engine (v0.5.3.post1) with config: model='/home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4', speculative_config=None, tokenizer='/home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/home/ndurkee/temp/llama3_70b_fixed/, use_v2_block_manager=True, enable_prefix_caching=True)\r\nINFO 08-06 12:48:24 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=83) INFO 08-06 12:48:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=84) INFO 08-06 12:48:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=85) INFO 08-06 12:48:24 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\nINFO 08-06 12:48:26 utils.py:784] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=84) INFO 08-06 12:48:26 utils.py:784] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=85) INFO 08-06 12:48:26 utils.py:784] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=83) INFO 08-06 12:48:26 utils.py:784] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=84) INFO 08-06 12:48:26 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-06 12:48:26 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=85) INFO 08-06 12:48:26 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=83) INFO 08-06 12:48:26 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-06 12:48:27 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f198dd419f0>, local_subscribe_port=60591, local_sync_port=35463, remote_subscribe_port=None, remote_sync_port=None)\r\nINFO 08-06 12:48:27 model_runner.py:680] Starting to load model /home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4...\r\n(VllmWorkerProcess pid=84) INFO 08-06 12:48:27 model_runner.py:680] Starting to load model /home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4...\r\n(VllmWorkerProcess pid=85) INFO 08-06 12:48:27 model_runner.py:680] Starting to load model /home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4...\r\n(VllmWorkerProcess pid=83) INFO 08-06 12:48:27 model_runner.py:680] Starting to load model /home/ndurkee/ndurkee/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4...\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: Cannot copy out of meta tensor; no data!, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     self.model_runner.load_model()\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 682, in load_model\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 280, in load_model\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 111, in _initialize_model\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 384, in __init__\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 285, in __init__\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 144, in make_layers\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 145, in <listcomp>\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 102, in maybe_offload_to_cpu\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     cpu_data.copy_(p.data)\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 78, in __torch_function__\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226] NotImplementedError: Cannot copy out of meta tensor; no data!\r\n(VllmWorkerProcess pid=83) ERROR 08-06 12:48:27 multiproc_worker_utils.py:226]\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 317, in <module>\r\n[rank0]:     run_server(args)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 231, in run_server\r\n[rank0]:     if llm_engine is not None else AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 466, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 380, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 547, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 251, in __init__\r\n[rank0]:     self.model_executor = executor_class(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 201, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n[rank0]:     super().__init__(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n[rank0]:     self._init_executor()\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 124, in _init_executor\r\n[rank0]:     self._run_workers(\"load_model\",\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 178, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n[rank0]:     self.model_runner.load_model()\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 682, in load_model\r\n[rank0]:     self.model = get_model(model_config=self.model_config,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n[rank0]:     return loader.load_model(model_config=model_config,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 280, in load_model\r\n[rank0]:     model = _initialize_model(model_config, self.load_config,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 111, in _initialize_model\r\n[rank0]:     return model_class(config=model_config.hf_config,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 384, in __init__\r\n[rank0]:     self.model = LlamaModel(config,\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 285, in __init__\r\n[rank0]:     self.start_layer, self.end_layer, self.layers = make_layers(\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 144, in make_layers\r\n[rank0]:     [PPMissingLayer() for _ in range(start_layer)] + [\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 145, in <listcomp>\r\n[rank0]:     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 102, in maybe_offload_to_cpu\r\n[rank0]:     cpu_data.copy_(p.data)\r\n[rank0]:   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 78, in __torch_function__\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]: NotImplementedError: Cannot copy out of meta tensor; no data!\r\nERROR 08-06 12:48:27 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 83 died, exit code: -15\r\nINFO 08-06 12:48:27 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\n/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-06T12:52:06Z",
    "closed_at": "2024-08-07T01:34:27Z",
    "author": "w013nad",
    "comments_count": 11,
    "comments": [
      {
        "author": "w013nad",
        "body": "It appears it's broken for quantization in general even without the cpu offload.\r\n\r\n```\r\npython3 -m vllm.entrypoints.openai.api_server --model /home/ndurkee/ndurkee/Meta-Llama-3.1-70B-Instruct/ --max-model-len 90000 -tp 4 --gpu-memory-utilization 0.99 --dtype auto --distributed-executor-backend mp --port 15001 --served-model-name /home/ndurkee/temp/llama3_70b_fixed/  --max-log-len 10 --use-v2-block-manager --disable-custom-all-reduce --enable-prefix-caching --quantization='fp8'\r\n```\r\n\r\n\r\n```\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: shape '[-1, 32]' is invalid for input of size 1, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     self.model_runner.load_model()\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 344, in load_model\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     quant_method.process_weights_after_loading(module)\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/fp8.py\", line 212, in process_weights_after_loading\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     prepare_fp8_layer_for_marlin(layer)\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py\", line 80, in prepare_fp8_layer_for_marlin\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     marlin_scales = marlin_permute_scales(s=scales,\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 172, in marlin_permute_scales\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     s = s.reshape((-1, len(scale_perm_single)))[:, scale_perm_single]\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226] RuntimeError: shape '[-1, 32]' is invalid for input of size 1\r\n(VllmWorkerProcess pid=144) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: shape '[-1, 32]' is invalid for input of size 1, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     self.model_runner.load_model()\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 344, in load_model\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     quant_method.process_weights_after_loading(module)\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/fp8.py\", line 212, in process_weights_after_loading\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     prepare_fp8_layer_for_marlin(layer)\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py\", line 80, in prepare_fp8_layer_for_marlin\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     marlin_scales = marlin_permute_scales(s=scales,\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 172, in marlin_permute_scales\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226]     s = s.reshape((-1, len(scale_perm_single)))[:, scale_perm_single]\r\n(VllmWorkerProcess pid=143) ERROR 08-06 14:49:20 multiproc_worker_utils.py:226] RuntimeError: shape '[-1, 32]' is invalid for input of size 1\r\n```",
        "created_at": "2024-08-06T15:42:16Z"
      },
      {
        "author": "youkaichao",
        "body": "cc @mgoin for quantization and cpu offloading. I feel this is a quantization issue, and it might be related with your quantized model.\r\n\r\n@w013nad do you have a hf link for the model you try to use?",
        "created_at": "2024-08-06T16:40:16Z"
      },
      {
        "author": "w013nad",
        "body": "> cc @mgoin for quantization and cpu offloading. I feel this is a quantization issue, and it might be related with your quantized model.\r\n> \r\n> @w013nad do you have a hf link for the model you try to use?\r\n\r\nhttps://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
        "created_at": "2024-08-06T16:43:37Z"
      },
      {
        "author": "mgoin",
        "body": "I will look into this, but are you sure you are using 0.5.4? In your logs and collect env output, it mentions 0.5.3.post1\r\n```\r\nvLLM Version: 0.5.3.post1\r\n```\r\nand\r\n```\r\nINFO 08-06 12:48:23 api_server.py:219] vLLM API server version 0.5.3.post1\r\n```",
        "created_at": "2024-08-06T16:51:21Z"
      },
      {
        "author": "w013nad",
        "body": "Shoot, some of this was with a prerelease wheel. There seems to be 2 separate issues here:\r\n\r\n1. fp8 doesn't work at all\r\n```\r\nroot@96aed4dedb69:/home/ndurkee# python3 -m vllm.entrypoints.openai.api_server --model /home/ndurkee/Llama-3-8B-Instruct/ -tp 4 --gpu-memory-utilization 0.79 --dtype auto --distributed-executor-backend mp --port 5006 --served-model-name /home/ndurkee/temp/llama3_70b_fixed/ --max-model-len 1000 --max-log-len 10 --use-v2-block-manager --disable-custom-all-reduce --enable-prefix-caching --quantization='fp8'\r\nINFO 08-06 18:47:55 api_server.py:339] vLLM API server version 0.5.4\r\nINFO 08-06 18:47:55 api_server.py:340] args: Namespace(host=None, port=5006, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='/home/ndurkee/Llama-3-8B-Instruct/', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1000, guided_decoding_backend='outlines', distributed_executor_backend='mp', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.79, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization='fp8', rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=True, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['/home/ndurkee/temp/llama3_70b_fixed/'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=10)\r\nWARNING 08-06 18:47:56 config.py:1454] Casting torch.bfloat16 to torch.float16.\r\nINFO 08-06 18:47:56 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/home/ndurkee/Llama-3-8B-Instruct/', speculative_config=None, tokenizer='/home/ndurkee/Llama-3-8B-Instruct/', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=1000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=fp8, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/home/ndurkee/temp/llama3_70b_fixed/, use_v2_block_manager=True, enable_prefix_caching=True)\r\nWARNING 08-06 18:47:56 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 08-06 18:47:56 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=3133) INFO 08-06 18:47:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=3134) INFO 08-06 18:47:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=3135) INFO 08-06 18:47:56 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\nINFO 08-06 18:47:58 utils.py:841] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=3135) INFO 08-06 18:47:58 utils.py:841] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=3134) INFO 08-06 18:47:58 utils.py:841] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=3133) INFO 08-06 18:47:58 utils.py:841] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=3135) INFO 08-06 18:47:58 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-06 18:47:58 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=3134) INFO 08-06 18:47:58 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=3133) INFO 08-06 18:47:58 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-06 18:47:59 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f9612647eb0>, local_subscribe_port=47097, remote_subscribe_port=None)\r\nINFO 08-06 18:47:59 model_runner.py:720] Starting to load model /home/ndurkee/Llama-3-8B-Instruct/...\r\n(VllmWorkerProcess pid=3134) INFO 08-06 18:47:59 model_runner.py:720] Starting to load model /home/ndurkee/Llama-3-8B-Instruct/...\r\n(VllmWorkerProcess pid=3135) INFO 08-06 18:47:59 model_runner.py:720] Starting to load model /home/ndurkee/Llama-3-8B-Instruct/...\r\n(VllmWorkerProcess pid=3133) INFO 08-06 18:47:59 model_runner.py:720] Starting to load model /home/ndurkee/Llama-3-8B-Instruct/...\r\nLoading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:01,  2.62it/s]\r\nLoading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:00,  2.63it/s]\r\nLoading safetensors checkpoint shards:  75% Completed | 3/4 [00:00<00:00,  3.67it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.26it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 4/4 [00:01<00:00,  3.16it/s]\r\n\r\nWARNING 08-06 18:48:01 utils.py:578] Your GPU does not have native support for FP8 computation but FP8 quantization is being used. Weight-only FP8 compression will be used leveraging the Marlin kernel. This may degrade performance for compute-heavy workloads.\r\nERROR 08-06 18:48:01 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 3135 died, exit code: -15\r\nINFO 08-06 18:48:01 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 249, in __init__\r\n    self.model_executor = executor_class(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 215, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 138, in _init_executor\r\n    self._run_workers(\"load_model\",\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n    self.model = get_model(model_config=self.model_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 344, in load_model\r\n    quant_method.process_weights_after_loading(module)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/fp8.py\", line 212, in process_weights_after_loading\r\n    prepare_fp8_layer_for_marlin(layer)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/marlin_utils_fp8.py\", line 80, in prepare_fp8_layer_for_marlin\r\n    marlin_scales = marlin_permute_scales(s=scales,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 172, in marlin_permute_scales\r\n    s = s.reshape((-1, len(scale_perm_single)))[:, scale_perm_single]\r\nRuntimeError: shape '[-1, 32]' is invalid for input of size 1\r\n/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n^CTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 370, in <module>\r\n    asyncio.run(run_server(args))\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\r\n    self._run_once()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871, in _run_once\r\n    event_list = self._selector.select(timeout)\r\n  File \"/usr/lib/python3.10/selectors.py\", line 469, in select\r\n    fd_event_list = self._selector.poll(timeout, max_ev)\r\nKeyboardInterrupt\r\n```\r\n\r\n2. GPTQ cpu offload doesn't work\r\n```\r\nroot@96aed4dedb69:/home/ndurkee# python3 -m vllm.entrypoints.openai.api_server --model /home/ndurkee/temp/llama3_8b_gptq -tp 4 --gpu-memory-utilization 0.79 --dtype auto --distributed-executor-backend mp --port 5006 --served-model-name /home/ndurkee/temp/llama3_70b_fixed/ --max-model-len 1000 --max-log-len 10 --use-v2-block-manager --disable-custom-all-reduce --enable-prefix-caching --cpu-offload-gb 5\r\nINFO 08-06 18:45:29 api_server.py:339] vLLM API server version 0.5.4\r\nINFO 08-06 18:45:29 api_server.py:340] args: Namespace(host=None, port=5006, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, prompt_adapters=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], return_tokens_as_token_ids=False, disable_frontend_multiprocessing=False, model='/home/ndurkee/temp/llama3_8b_gptq', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=1000, guided_decoding_backend='outlines', distributed_executor_backend='mp', worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=True, disable_sliding_window=False, use_v2_block_manager=True, num_lookahead_slots=0, seed=0, swap_space=4, cpu_offload_gb=5.0, gpu_memory_utilization=0.79, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=True, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', scheduler_delay_factor=0.0, enable_chunked_prefill=None, speculative_model=None, num_speculative_tokens=None, speculative_draft_tensor_parallel_size=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, spec_decoding_acceptance_method='rejection_sampler', typical_acceptance_sampler_posterior_threshold=None, typical_acceptance_sampler_posterior_alpha=None, disable_logprobs_during_spec_decoding=None, model_loader_extra_config=None, ignore_patterns=[], preemption_mode=None, served_model_name=['/home/ndurkee/temp/llama3_70b_fixed/'], qlora_adapter_name_or_path=None, otlp_traces_endpoint=None, engine_use_ray=False, disable_log_requests=False, max_log_len=10)\r\nINFO 08-06 18:45:29 gptq_marlin.py:98] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\r\nINFO 08-06 18:45:29 gptq_marlin.py:98] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\r\nINFO 08-06 18:45:29 llm_engine.py:174] Initializing an LLM engine (v0.5.4) with config: model='/home/ndurkee/temp/llama3_8b_gptq', speculative_config=None, tokenizer='/home/ndurkee/temp/llama3_8b_gptq', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=1000, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=gptq_marlin, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=0, served_model_name=/home/ndurkee/temp/llama3_70b_fixed/, use_v2_block_manager=True, enable_prefix_caching=True)\r\nWARNING 08-06 18:45:29 multiproc_gpu_executor.py:59] Reducing Torch parallelism from 128 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\r\nINFO 08-06 18:45:29 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\r\n(VllmWorkerProcess pid=2602) INFO 08-06 18:45:30 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=2603) INFO 08-06 18:45:30 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=2604) INFO 08-06 18:45:30 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\n(VllmWorkerProcess pid=2602) INFO 08-06 18:45:31 utils.py:841] Found nccl from library libnccl.so.2\r\nINFO 08-06 18:45:31 utils.py:841] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=2603) INFO 08-06 18:45:31 utils.py:841] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=2602) INFO 08-06 18:45:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=2604) INFO 08-06 18:45:31 utils.py:841] Found nccl from library libnccl.so.2\r\nINFO 08-06 18:45:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=2603) INFO 08-06 18:45:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=2604) INFO 08-06 18:45:31 pynccl.py:63] vLLM is using nccl==2.20.5\r\nINFO 08-06 18:45:32 shm_broadcast.py:235] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7fbf8e91a590>, local_subscribe_port=57567, remote_subscribe_port=None)\r\nINFO 08-06 18:45:32 model_runner.py:720] Starting to load model /home/ndurkee/temp/llama3_8b_gptq...\r\n(VllmWorkerProcess pid=2602) INFO 08-06 18:45:32 model_runner.py:720] Starting to load model /home/ndurkee/temp/llama3_8b_gptq...\r\n(VllmWorkerProcess pid=2603) INFO 08-06 18:45:32 model_runner.py:720] Starting to load model /home/ndurkee/temp/llama3_8b_gptq...\r\n(VllmWorkerProcess pid=2604) INFO 08-06 18:45:32 model_runner.py:720] Starting to load model /home/ndurkee/temp/llama3_8b_gptq...\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: Cannot copy out of meta tensor; no data!, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.model_runner.load_model()\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 324, in load_model\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 154, in _initialize_model\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 384, in __init__\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 285, in __init__\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 146, in make_layers\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 147, in <listcomp>\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 104, in maybe_offload_to_cpu\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     cpu_data.copy_(p.data)\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 79, in __torch_function__\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226] NotImplementedError: Cannot copy out of meta tensor; no data!\r\n(VllmWorkerProcess pid=2604) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method load_model: Cannot copy out of meta tensor; no data!, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.model_runner.load_model()\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.model = get_model(model_config=self.model_config,\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     return loader.load_model(model_config=model_config,\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 324, in load_model\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     model = _initialize_model(model_config, self.load_config,\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 154, in _initialize_model\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     return model_class(config=model_config.hf_config,\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 384, in __init__\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.model = LlamaModel(config,\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 285, in __init__\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     self.start_layer, self.end_layer, self.layers = make_layers(\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 146, in make_layers\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     [PPMissingLayer() for _ in range(start_layer)] + [\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 147, in <listcomp>\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 104, in maybe_offload_to_cpu\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     cpu_data.copy_(p.data)\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 79, in __torch_function__\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226] NotImplementedError: Cannot copy out of meta tensor; no data!\r\n(VllmWorkerProcess pid=2603) ERROR 08-06 18:45:33 multiproc_worker_utils.py:226]\r\nERROR 08-06 18:45:33 multiproc_worker_utils.py:120] Worker VllmWorkerProcess pid 2602 died, exit code: -15\r\nINFO 08-06 18:45:33 multiproc_worker_utils.py:123] Killing local vLLM worker processes\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/usr/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server\r\n    server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 249, in __init__\r\n    self.model_executor = executor_class(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 215, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 138, in _init_executor\r\n    self._run_workers(\"load_model\",\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 139, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n    self.model = get_model(model_config=self.model_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 324, in load_model\r\n    model = _initialize_model(model_config, self.load_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/loader.py\", line 154, in _initialize_model\r\n    return model_class(config=model_config.hf_config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 384, in __init__\r\n    self.model = LlamaModel(config,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llama.py\", line 285, in __init__\r\n    self.start_layer, self.end_layer, self.layers = make_layers(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 146, in make_layers\r\n    [PPMissingLayer() for _ in range(start_layer)] + [\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 147, in <listcomp>\r\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 104, in maybe_offload_to_cpu\r\n    cpu_data.copy_(p.data)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_device.py\", line 79, in __torch_function__\r\n    return func(*args, **kwargs)\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 1 leaked shared_memory objects to clean up at shutdown\r\n  warnings.warn('resource_tracker: There appear to be %d '\r\n^CTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 370, in <module>\r\n    asyncio.run(run_server(args))\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n    return loop.run_until_complete(main)\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\r\n    self._run_once()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871, in _run_once\r\n    event_list = self._selector.select(timeout)\r\n  File \"/usr/lib/python3.10/selectors.py\", line 469, in select\r\n    fd_event_list = self._selector.poll(timeout, max_ev)\r\nKeyboardInterrupt\r\n```\r\n\r\nGPTQ does work by itself. Note that this is on A100s.",
        "created_at": "2024-08-06T18:53:41Z"
      },
      {
        "author": "mgoin",
        "body": "Okay I confirmed dynamic FP8 works fine on H100 but fails on A100. This is an issue with the dynamic FP8 Marlin backend.\r\n```\r\nvllm serve meta-llama/Meta-Llama-3-8B-Instruct --quantization=\"fp8\" --port 9000 \r\n...\r\n  File \"/home/mgoin/venvs/vllm-rel/lib/python3.10/site-packages/vllm/model_executor/layers/quantization/utils/marlin_utils.py\", line 172, in marlin_permute_scales\r\n    s = s.reshape((-1, len(scale_perm_single)))[:, scale_perm_single]\r\nRuntimeError: shape '[-1, 32]' is invalid for input of size 1\r\n```\r\n\r\nIt does work fine with models that are already quantized to FP8 on A100:\r\n```\r\nvllm serve neuralmagic/Meta-Llama-3-8B-Instruct-FP8 --quantization=\"fp8\" --port 9000\r\n...\r\nINFO:     Uvicorn running on http://0.0.0.0:9000 (Press CTRL+C to quit)\r\n```\r\n\r\nI opened a tracking issue here: https://github.com/vllm-project/vllm/issues/7216\r\nLooking into this first",
        "created_at": "2024-08-06T19:36:23Z"
      },
      {
        "author": "mgoin",
        "body": "@w013nad If you can build and test from source, please try my PR to fix dynamic FP8 Marlin https://github.com/vllm-project/vllm/pull/7219. It seems to fix the issue from my reproduction\r\n\r\nI will look into GPTQ cpu offloading now",
        "created_at": "2024-08-06T21:08:01Z"
      },
      {
        "author": "mgoin",
        "body": "Verified that forcing GPTQ with cpu offload works:\r\n```\r\nvllm serve Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4 --cpu-offload-gb 5 --quantization gptq\r\n...\r\nINFO 08-06 21:20:41 gptq_marlin.py:102] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\r\n...\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\n```\r\n\r\nThe issue is specifically with GPTQ Marlin:\r\n```\r\nvllm serve Qwen/Qwen2-0.5B-Instruct-GPTQ-Int4 --cpu-offload-gb 5  \r\n...\r\nINFO 08-06 21:21:46 gptq_marlin.py:98] The model is convertible to gptq_marlin during runtime. Using gptq_marlin kernel.\r\n...\r\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/utils.py\", line 195, in <listcomp>\r\n    maybe_offload_to_cpu(layer_fn(prefix=f\"{prefix}.{idx}\"))\r\n  File \"/home/mgoin/code/vllm/vllm/model_executor/models/utils.py\", line 152, in maybe_offload_to_cpu\r\n    cpu_data.copy_(p.data)\r\n  File \"/home/mgoin/venvs/vllm/lib/python3.10/site-packages/torch/utils/_device.py\", line 79, in __torch_function__\r\n    return func(*args, **kwargs)\r\nNotImplementedError: Cannot copy out of meta tensor; no data!\r\n```",
        "created_at": "2024-08-06T21:22:33Z"
      },
      {
        "author": "mgoin",
        "body": "@w013nad ditto for the GPTQ Marlin fix linked above ^\r\n\r\nThank you very much for reporting these issues and my apologies for letting them slip through this release. I added explicit tests for both of these cases so they will be caught in automation going forward.",
        "created_at": "2024-08-06T21:48:05Z"
      },
      {
        "author": "w013nad",
        "body": "Sorry I'm not able to build from source. I'm stuck using your nightly pypi packages or docker images due to it being a closed environment.",
        "created_at": "2024-08-06T22:49:02Z"
      },
      {
        "author": "fzyzcjy",
        "body": "Looking forward to seeing this fix be released! (I am seeing the same problem)",
        "created_at": "2024-08-23T05:54:29Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/7160": {
    "issue_number": 7160,
    "issue_url": "https://github.com/vllm-project/vllm/issues/7160",
    "title": "[Bug]: InternVL2 Mismatch in number of image tokens and image embedding size",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\n/opt/aritra.c/worktree/vllm-main/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nPyTorch version: 2.4.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Debian GNU/Linux 11 (bullseye) (x86_64)\r\nGCC version: (Debian 10.2.1-6) 10.2.1 20210110\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.10.0-30-cloud-amd64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nByte Order:                           Little Endian\r\nAddress sizes:                        52 bits physical, 57 bits virtual\r\nCPU(s):                               208\r\nOn-line CPU(s) list:                  0-207\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   52\r\nSocket(s):                            2\r\nNUMA node(s):                         2\r\nVendor ID:                            GenuineIntel\r\nCPU family:                           6\r\nModel:                                143\r\nModel name:                           Intel(R) Xeon(R) Platinum 8481C CPU @ 2.70GHz\r\nStepping:                             8\r\nCPU MHz:                              2699.998\r\nBogoMIPS:                             5399.99\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            4.9 MiB\r\nL1i cache:                            3.3 MiB\r\nL2 cache:                             208 MiB\r\nL3 cache:                             210 MiB\r\nNUMA node0 CPU(s):                    0-51,104-155\r\nNUMA node1 CPU(s):                    52-103,156-207\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_\r\nfreq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpc\r\nid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512_bf16 arat avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpop\r\ncntdq rdpid cldemote movdiri movdir64b fsrm md_clear serialize arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] pytorch-lightning==2.3.3\r\n[pip3] pyzmq==26.0.3\r\n[pip3] torch==2.4.0\r\n[pip3] torchmetrics==1.4.0.post0\r\n[pip3] torchvision==0.19.0\r\n[pip3] transformers==4.43.3\r\n[pip3] triton==3.0.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] pytorch-lightning         2.3.3                    pypi_0    pypi\r\n[conda] pyzmq                     26.0.3                   pypi_0    pypi\r\n[conda] torch                     2.4.0                    pypi_0    pypi\r\n[conda] torchmetrics              1.4.0.post0              pypi_0    pypi\r\n[conda] torchvision               0.19.0                   pypi_0    pypi\r\n[conda] transformers              4.43.3                   pypi_0    pypi\r\n[conda] triton                    3.0.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    0-51,104-155    0               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    52-103,156-207  1               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    52-103,156-207  1               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    52-103,156-207  1               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      52-103,156-207  1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nOffline inference for InternVL2 fails frequently due to mismatch in image tokens in the prompt and size of ViT embeddings.\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\nfrom PIL import Image\r\n\r\n\r\nllm = LLM(\r\n    model=\"OpenGVLab/InternVL2-26B\",\r\n    enforce_eager=True,\r\n    tensor_parallel_size=1,\r\n    seed=42,\r\n    max_model_len=8192,\r\n    trust_remote_code=True,\r\n)\r\nsampling_params = SamplingParams(temperature=0.0, max_tokens=256, stop=[\"<|im_end|>\"])\r\nimage = Image.open(\r\n    \"images/89874e-pale-yellow-fs-mini-klub-12-18-months-original-imaeph9vtzfhrnav.jpeg\"\r\n)\r\nprompt = llm.get_tokenizer().apply_chat_template(\r\n    [\r\n        {\"role\": \"system\", \"content\": \"Answer the question.\"},\r\n        {\"role\": \"user\", \"content\": \"<image>\\nWhat is shown in the image?\"},\r\n    ],\r\n    tokenize=False,\r\n    add_generation_prompt=True,\r\n)\r\ninputs = {\"prompt\": prompt, \"multi_modal_data\": {\"image\": image}}\r\noutputs = llm.generate(inputs, sampling_params=sampling_params)\r\n\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n```\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=2 python debug.py\r\n/opt/aritra.c/worktree/vllm-main/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\r\nNo module named 'vllm.commit_id'\r\n  from vllm.version import __version__ as VLLM_VERSION\r\nINFO 08-05 19:48:10 llm_engine.py:174] Initializing an LLM engine (v0.5.3.post1) with config: model='/opt/aritra.c/worktree/llava-finetune-v1/LLaVA/data/checkpoint/future/InternVL2-26B', speculative_config=None, tokenizer='/opt/aritra.c/worktree/llava-finetune-v1/LLaVA/data/checkpoint/future/InternVL2-26B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/opt/aritra.c/worktree/llava-finetune-v1/LLaVA/data/checkpoint/future/InternVL2-26B, use_v2_block_manager=False, enable_prefix_caching=False)\r\nWARNING 08-05 19:48:10 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\nWARNING 08-05 19:48:10 logger.py:146] VLLM_TRACE_FUNCTION is enabled. It will record every function executed by Python. This will slow down the code. It is suggested to be used for debugging hang or crashes only.\r\nINFO 08-05 19:48:10 logger.py:150] Trace frame log is saved to /tmp/vllm/vllm-instance-10c45dd2af9949f9b2e55d4a3b04579c/VLLM_TRACE_FUNCTION_for_process_2460720_thread_140321903204160_at_2024-08-05_19:48:10.741247.log\r\nDEBUG 08-05 19:48:15 parallel_state.py:845] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.146.32.17:35725 backend=nccl\r\nINFO 08-05 19:48:15 model_runner.py:720] Starting to load model /opt/aritra.c/worktree/llava-finetune-v1/LLaVA/data/checkpoint/future/InternVL2-26B...\r\nLoading safetensors checkpoint shards:   0% Completed | 0/11 [00:00<?, ?it/s]\r\nLoading safetensors checkpoint shards:  18% Completed | 2/11 [00:00<00:01,  8.51it/s]\r\nLoading safetensors checkpoint shards:  64% Completed | 7/11 [00:01<00:00,  5.34it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 11/11 [00:02<00:00,  4.55it/s]\r\nLoading safetensors checkpoint shards: 100% Completed | 11/11 [00:02<00:00,  4.79it/s]\r\n\r\nINFO 08-05 19:48:31 model_runner.py:732] Loading model weights took 47.5707 GB\r\nWARNING 08-05 19:48:31 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\nINFO 08-05 19:48:34 gpu_executor.py:102] # GPU blocks: 6654, # CPU blocks: 1365\r\nProcessed prompts:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/opt/aritra.c/worktree/llava-finetune-v1/LLaVA/scripts/debug.py\", line 362, in <module>\r\n[rank0]:     outputs = llm.generate(inputs, sampling_params=sampling_params)\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/utils.py\", line 895, in inner\r\n[rank0]:     return fn(*args, **kwargs)\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/entrypoints/llm.py\", line 330, in generate\r\n[rank0]:     outputs = self._run_engine(use_tqdm=use_tqdm)\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/entrypoints/llm.py\", line 611, in _run_engine\r\n[rank0]:     step_outputs = self.llm_engine.step()\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/engine/llm_engine.py\", line 919, in step\r\n[rank0]:     output = self.model_executor.execute_model(\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/executor/gpu_executor.py\", line 110, in execute_model\r\n[rank0]:     output = self.driver_worker.execute_model(execute_model_req)\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/worker/worker_base.py\", line 273, in execute_model\r\n[rank0]:     output = self.model_runner.execute_model(\r\n[rank0]:   File \"/opt/aritra.c/.venvs/vllm_infer_latest/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/worker/model_runner.py\", line 1363, in execute_model\r\n[rank0]:     hidden_or_intermediate_states = model_executable(\r\n[rank0]:   File \"/opt/aritra.c/.venvs/vllm_infer_latest/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/opt/aritra.c/.venvs/vllm_infer_latest/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/model_executor/models/internvl.py\", line 397, in forward\r\n[rank0]:     inputs_embeds = merge_vision_embeddings(\r\n[rank0]:   File \"/opt/aritra.c/worktree/vllm-main/vllm/model_executor/models/utils.py\", line 31, in merge_vision_embeddings\r\n[rank0]:     raise ValueError(\r\n[rank0]: ValueError: Attempted to assign 1 x 256 = 256 image tokens to 512 placeholders\r\nProcessed prompts:   0%|                                                                                                                                          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-08-05T14:21:38Z",
    "closed_at": "2024-08-07T16:32:08Z",
    "author": "GohioAC",
    "comments_count": 18,
    "comments": [
      {
        "author": "GohioAC",
        "body": "Image: [https://img.fkcdn.com/image/jacket/w/f/n/89874e-pale-yellow-fs-mini-klub-12-18-months-original-imaeph9vtzfhrnav.jpeg](https://img.fkcdn.com/image/jacket/w/f/n/89874e-pale-yellow-fs-mini-klub-12-18-months-original-imaeph9vtzfhrnav.jpeg)",
        "created_at": "2024-08-05T14:22:50Z"
      },
      {
        "author": "DarkLight1337",
        "body": "cc @Isotr0py",
        "created_at": "2024-08-05T14:40:54Z"
      },
      {
        "author": "Isotr0py",
        "body": "Seems that there is a trouble when calculating num_patch for small image. I will fix it soon.",
        "created_at": "2024-08-05T15:13:15Z"
      },
      {
        "author": "Isotr0py",
        "body": "@GohioAC #7164 should fix this bug. And it works on `InternVL2-2B` with this image after the fix:\n\n```bash\n$ python examples/bug_example.py\nINFO 08-05 23:27:38 llm_engine.py:174] Initializing an LLM engine (v0.5.3.post1) with config: model='/data/LLM-model/InternVL2-2B', speculative_config=None, tokenizer='/data/LLM-model/InternVL2-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/data/LLM-model/InternVL2-2B, use_v2_block_manager=False, enable_prefix_caching=False)\nWARNING 08-05 23:27:38 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\nWARNING 08-05 23:27:38 cpu_executor.py:345] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\nINFO 08-05 23:27:38 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\nINFO 08-05 23:27:38 selector.py:66] Using Torch SDPA backend.\nINFO 08-05 23:27:41 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\nINFO 08-05 23:27:41 selector.py:66] Using Torch SDPA backend.\nLoading safetensors checkpoint shards: 0% Completed | 0/2 [00:00\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.97it/s]\nLoading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.94it/s]\n \nINFO 08-05 23:27:42 cpu_executor.py:208] # CPU blocks: 2730\nWARNING 08-05 23:27:42 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\nProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:54<00:00, 54.82s/it, est. speed input: 5.22 toks/s, output: 4.67 toks/s]\nPrompt: '<|im_start|>system\\nAnswer the question.<|im_end|>\\n<|im_start|>user\\n\\nWhat is shown in the image?<|im_end|>\\n<|im_start|>assistant\\n', Generated text: \"The image shows a bright yellow jacket with a hood. The jacket has a colorful design on the front, including a green bow on the left chest area and a patch on the right side. The patch features a cartoonish design with a smiling face and some text. The jacket also has a pocket on the left side with a cartoon character and some text. The hood is up, and the jacket appears to be made of a soft, possibly fleece material.\\nIs there anything else I can help you with?{No, that's all!}\"\n```",
        "created_at": "2024-08-05T16:10:22Z"
      },
      {
        "author": "github-0-searcher",
        "body": "> @GohioAC #7164 should fix this bug. And it works on `InternVL2-2B` with this image after the fix:\r\n> \r\n> ```shell\r\n> $ python examples/bug_example.py\r\n> INFO 08-05 23:27:38 llm_engine.py:174] Initializing an LLM engine (v0.5.3.post1) with config: model='/data/LLM-model/InternVL2-2B', speculative_config=None, tokenizer='/data/LLM-model/InternVL2-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/data/LLM-model/InternVL2-2B, use_v2_block_manager=False, enable_prefix_caching=False)\r\n> WARNING 08-05 23:27:38 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n> WARNING 08-05 23:27:38 cpu_executor.py:345] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\n> INFO 08-05 23:27:38 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n> INFO 08-05 23:27:38 selector.py:66] Using Torch SDPA backend.\r\n> INFO 08-05 23:27:41 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n> INFO 08-05 23:27:41 selector.py:66] Using Torch SDPA backend.\r\n> Loading safetensors checkpoint shards: 0% Completed | 0/2 [00:00\r\n> Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.97it/s]\r\n> Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.94it/s]\r\n>  \r\n> INFO 08-05 23:27:42 cpu_executor.py:208] # CPU blocks: 2730\r\n> WARNING 08-05 23:27:42 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n> Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:54<00:00, 54.82s/it, est. speed input: 5.22 toks/s, output: 4.67 toks/s]\r\n> Prompt: '<|im_start|>system\\nAnswer the question.<|im_end|>\\n<|im_start|>user\\n\\nWhat is shown in the image?<|im_end|>\\n<|im_start|>assistant\\n', Generated text: \"The image shows a bright yellow jacket with a hood. The jacket has a colorful design on the front, including a green bow on the left chest area and a patch on the right side. The patch features a cartoonish design with a smiling face and some text. The jacket also has a pocket on the left side with a cartoon character and some text. The hood is up, and the jacket appears to be made of a soft, possibly fleece material.\\nIs there anything else I can help you with?{No, that's all!}\"\r\n> ```\r\n\r\nStill getting this error with vllm==0.5.4.\r\nI gather that this pr has not been approved to be merged into release version. So I manually did the correction by myself.\r\n\r\nThis pr seems to change vllm/model_executor/models/internvl.py only.\r\nI copied and pasted https://github.com/vllm-project/vllm/blob/2676f58c9e3f9d84399822c04657a83a4fae30dd/vllm/model_executor/models/internvl.py to my local path.\r\n\r\nThis does not works for me. I tried 2B 8B 28B. \r\nMy error looks like:\r\nValueError: Attempted to assign 7 x 256 = 1792 image tokens to 507 placeholders\r\n\r\nThanks for your work!",
        "created_at": "2024-08-06T03:40:55Z"
      },
      {
        "author": "Isotr0py",
        "body": "@github-0-searcher Did you set `max_model_len`? You can try to set a larger `max_model_len` like 4096.",
        "created_at": "2024-08-06T03:44:54Z"
      },
      {
        "author": "github-0-searcher",
        "body": "Thanks for your fast reply.\r\nSeems to work well now :-)",
        "created_at": "2024-08-06T06:19:37Z"
      },
      {
        "author": "github-0-searcher",
        "body": "btw i notice a weird situation. 26B model's output is just ok. But if 8B model or 2B model is used, the output will be generated in a repetitive manner, either repeating some punctuations or a short sentence.\r\n\r\nWhat could be wrong here?",
        "created_at": "2024-08-06T06:47:13Z"
      },
      {
        "author": "Isotr0py",
        "body": "@github-0-searcher Can you provide the prompt and image? So that I can figure it out.",
        "created_at": "2024-08-06T07:30:02Z"
      },
      {
        "author": "JUNJIE99",
        "body": "> Thanks for your fast reply. Seems to work well now :-)\r\n\r\nHello, have you tried multi-image inference? I want to know how to pass two images into the `inputs` of `llm.generate`.\r\n\r\nThanks!",
        "created_at": "2024-08-07T13:29:38Z"
      },
      {
        "author": "Isotr0py",
        "body": "@JUNJIE99 The `InternVL` implementation in vllm hasn't supported multi-image inference yet. But it has been in our roadmap (#4194) and will work for it soon! A PR for this feature is also welcomed!",
        "created_at": "2024-08-07T13:36:43Z"
      },
      {
        "author": "JUNJIE99",
        "body": "Thanks for your quick reply! Looking forward to the updated version with multiple image inference.",
        "created_at": "2024-08-07T14:17:31Z"
      },
      {
        "author": "JUNJIE99",
        "body": "> @JUNJIE99 The `InternVL` implementation in vllm hasn't supported multi-image inference yet. But it has been in our roadmap (#4194) and will work for it soon! A PR for this feature is also welcomed!\r\n\r\nApologies for the interruption once again, but I was wondering if there is a timeline for updates related to multiple images inference?",
        "created_at": "2024-08-07T15:06:27Z"
      },
      {
        "author": "Isotr0py",
        "body": "@JUNJIE99 As shown in #4194, I think we have better wait #7230 merged before adding multiple images inference for this model.\r\nOnce the PR merged, multiple images inference should be updated soon.",
        "created_at": "2024-08-07T15:50:53Z"
      },
      {
        "author": "JUNJIE99",
        "body": "Thank you for your response and for the great work you're doing. I look forward to your updates.",
        "created_at": "2024-08-07T15:59:01Z"
      },
      {
        "author": "Howe-Young",
        "body": "> > @GohioAC #7164 should fix this bug. And it works on `InternVL2-2B` with this image after the fix:\r\n> > ```shell\r\n> > $ python examples/bug_example.py\r\n> > INFO 08-05 23:27:38 llm_engine.py:174] Initializing an LLM engine (v0.5.3.post1) with config: model='/data/LLM-model/InternVL2-2B', speculative_config=None, tokenizer='/data/LLM-model/InternVL2-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/data/LLM-model/InternVL2-2B, use_v2_block_manager=False, enable_prefix_caching=False)\r\n> > WARNING 08-05 23:27:38 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n> > WARNING 08-05 23:27:38 cpu_executor.py:345] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\n> > INFO 08-05 23:27:38 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n> > INFO 08-05 23:27:38 selector.py:66] Using Torch SDPA backend.\r\n> > INFO 08-05 23:27:41 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n> > INFO 08-05 23:27:41 selector.py:66] Using Torch SDPA backend.\r\n> > Loading safetensors checkpoint shards: 0% Completed | 0/2 [00:00\r\n> > Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.97it/s]\r\n> > Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.94it/s]\r\n> >  \r\n> > INFO 08-05 23:27:42 cpu_executor.py:208] # CPU blocks: 2730\r\n> > WARNING 08-05 23:27:42 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n> > Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:54<00:00, 54.82s/it, est. speed input: 5.22 toks/s, output: 4.67 toks/s]\r\n> > Prompt: '<|im_start|>system\\nAnswer the question.<|im_end|>\\n<|im_start|>user\\n\\nWhat is shown in the image?<|im_end|>\\n<|im_start|>assistant\\n', Generated text: \"The image shows a bright yellow jacket with a hood. The jacket has a colorful design on the front, including a green bow on the left chest area and a patch on the right side. The patch features a cartoonish design with a smiling face and some text. The jacket also has a pocket on the left side with a cartoon character and some text. The hood is up, and the jacket appears to be made of a soft, possibly fleece material.\\nIs there anything else I can help you with?{No, that's all!}\"\r\n> > ```\r\n> \r\n> Still getting this error with vllm==0.5.4. I gather that this pr has not been approved to be merged into release version. So I manually did the correction by myself.\r\n> \r\n> This pr seems to change vllm/model_executor/models/internvl.py only. I copied and pasted https://github.com/vllm-project/vllm/blob/2676f58c9e3f9d84399822c04657a83a4fae30dd/vllm/model_executor/models/internvl.py to my local path.\r\n> \r\n> This does not works for me. I tried 2B 8B 28B. My error looks like: ValueError: Attempted to assign 7 x 256 = 1792 image tokens to 507 placeholders\r\n> \r\n> Thanks for your work!\r\n\r\n\r\n\r\n> > @GohioAC #7164 should fix this bug. And it works on `InternVL2-2B` with this image after the fix:\r\n> > ```shell\r\n> > $ python examples/bug_example.py\r\n> > INFO 08-05 23:27:38 llm_engine.py:174] Initializing an LLM engine (v0.5.3.post1) with config: model='/data/LLM-model/InternVL2-2B', speculative_config=None, tokenizer='/data/LLM-model/InternVL2-2B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=8192, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, quantization_param_path=None, device_config=cpu, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None), seed=42, served_model_name=/data/LLM-model/InternVL2-2B, use_v2_block_manager=False, enable_prefix_caching=False)\r\n> > WARNING 08-05 23:27:38 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n> > WARNING 08-05 23:27:38 cpu_executor.py:345] Environment variable VLLM_CPU_KVCACHE_SPACE (GB) for CPU backend is not set, using 4 by default.\r\n> > INFO 08-05 23:27:38 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n> > INFO 08-05 23:27:38 selector.py:66] Using Torch SDPA backend.\r\n> > INFO 08-05 23:27:41 selector.py:117] Cannot use _Backend.FLASH_ATTN backend on CPU.\r\n> > INFO 08-05 23:27:41 selector.py:66] Using Torch SDPA backend.\r\n> > Loading safetensors checkpoint shards: 0% Completed | 0/2 [00:00\r\n> > Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.97it/s]\r\n> > Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00, 14.94it/s]\r\n> >  \r\n> > INFO 08-05 23:27:42 cpu_executor.py:208] # CPU blocks: 2730\r\n> > WARNING 08-05 23:27:42 tokenizer.py:129] Using a slow tokenizer. This might cause a significant slowdown. Consider using a fast tokenizer instead.\r\n> > Processed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:54<00:00, 54.82s/it, est. speed input: 5.22 toks/s, output: 4.67 toks/s]\r\n> > Prompt: '<|im_start|>system\\nAnswer the question.<|im_end|>\\n<|im_start|>user\\n\\nWhat is shown in the image?<|im_end|>\\n<|im_start|>assistant\\n', Generated text: \"The image shows a bright yellow jacket with a hood. The jacket has a colorful design on the front, including a green bow on the left chest area and a patch on the right side. The patch features a cartoonish design with a smiling face and some text. The jacket also has a pocket on the left side with a cartoon character and some text. The hood is up, and the jacket appears to be made of a soft, possibly fleece material.\\nIs there anything else I can help you with?{No, that's all!}\"\r\n> > ```\r\n> \r\n> Still getting this error with vllm==0.5.4. I gather that this pr has not been approved to be merged into release version. So I manually did the correction by myself.\r\n> \r\n> This pr seems to change vllm/model_executor/models/internvl.py only. I copied and pasted https://github.com/vllm-project/vllm/blob/2676f58c9e3f9d84399822c04657a83a4fae30dd/vllm/model_executor/models/internvl.py to my local path.\r\n> \r\n> This does not works for me. I tried 2B 8B 28B. My error looks like: ValueError: Attempted to assign 7 x 256 = 1792 image tokens to 507 placeholders\r\n> \r\n> Thanks for your work!\r\n\r\nsame error, I have already set max_model_len=4096:\r\n```\r\n ValueError: Attempted to assign 7 x 256 = 1792 image tokens to 3328 placeholders\r\n```",
        "created_at": "2024-08-13T09:41:39Z"
      },
      {
        "author": "DarkLight1337",
        "body": "The fix is currently only available if you build vLLM from source (`main` branch) since there hasn't been a release since then.",
        "created_at": "2024-08-13T09:44:31Z"
      },
      {
        "author": "Howe-Young",
        "body": "> The fix is currently only available if you build vLLM from source (`main` branch) since there hasn't been a release since then.\r\n\r\nbuild vLLm from source works! thank you!",
        "created_at": "2024-08-15T03:08:19Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6756": {
    "issue_number": 6756,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6756",
    "title": "[Bug]: Unable to run meta-llama/Llama-Guard-3-8B-INT8",
    "body": "### Your current environment\n\nLatest Docker image, RTX 4090\r\n\n\n### üêõ Describe the bug\n\n```\r\ndocker run --gpus all vllm/vllm-openai:latest --model meta-llama/Llama-Guard-3-8B-INT8\r\n...\r\n[rank0]:     raise ValueError(f\"Cannot find any of {keys} in the model's \"\r\n[rank0]: ValueError: Cannot find any of ['adapter_name_or_path'] in the model's quantization config.\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-24T17:38:09Z",
    "closed_at": "2024-08-29T23:09:09Z",
    "author": "xfalcox",
    "comments_count": 5,
    "comments": [
      {
        "author": "mgoin",
        "body": "@thesues @chenqianfzh It looks like this is an 8bit BNB model. Would it be easy to add support for these checkpoints as well?",
        "created_at": "2024-07-24T18:49:31Z"
      },
      {
        "author": "chenqianfzh",
        "body": "> @thesues @chenqianfzh It looks like this is an 8bit BNB model. Would it be easy to add support for these checkpoints as well?\r\n\r\nIt won't be difficult. I will work on it with higher priority.",
        "created_at": "2024-07-24T20:44:02Z"
      },
      {
        "author": "meihui",
        "body": "seems version 0.5.4+cu124 is working with bnb 4bit model. \r\n\r\nbut it says \r\n\r\nWARNING 08-06 06:27:07 config.py:254] bitsandbytes quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n\r\nWill that be a easy fix/support too? ",
        "created_at": "2024-08-06T06:32:43Z"
      },
      {
        "author": "chenqianfzh",
        "body": "https://github.com/vllm-project/vllm/pull/7445\r\n\r\nwith this PR, meta-llama/Llama-Guard-3-8B-INT8 is supported.",
        "created_at": "2024-08-12T23:54:27Z"
      },
      {
        "author": "chenqianfzh",
        "body": "> The speed can be slower than\r\n\r\nA lot of quantizations here are not in the optimized method list yet. It is not our top priority now to optimize the speed yet, as we are working to support more quantization features of bnb.\r\n",
        "created_at": "2024-08-13T00:01:31Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6703": {
    "issue_number": 6703,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6703",
    "title": "[Bug]: Flash-attn on-GPU advance step optimization bug with spec decode on LLama 405B",
    "body": "### Your current environment\n\n```text\r\nPyTorch version: 2.3.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.1\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Mar 22 2024, 16:50:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-116-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA H100 80GB HBM3\r\nGPU 1: NVIDIA H100 80GB HBM3\r\nGPU 2: NVIDIA H100 80GB HBM3\r\nGPU 3: NVIDIA H100 80GB HBM3\r\nGPU 4: NVIDIA H100 80GB HBM3\r\nGPU 5: NVIDIA H100 80GB HBM3\r\nGPU 6: NVIDIA H100 80GB HBM3\r\nGPU 7: NVIDIA H100 80GB HBM3\r\n\r\nNvidia driver version: 550.90.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 57 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               104\r\nOn-line CPU(s) list:                  0-103\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) Platinum 8470\r\nCPU family:                           6\r\nModel:                                143\r\nThread(s) per core:                   1\r\nCore(s) per socket:                   52\r\nSocket(s):                            2\r\nStepping:                             8\r\nBogoMIPS:                             4000.00\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                       VT-x\r\nL1d cache:                            4.9 MiB (104 instances)\r\nL1i cache:                            3.3 MiB (104 instances)\r\nL2 cache:                             208 MiB (104 instances)\r\nL3 cache:                             210 MiB (2 instances)\r\nNUMA node(s):                         8\r\nNUMA node0 CPU(s):                    0-12\r\nNUMA node1 CPU(s):                    13-25\r\nNUMA node2 CPU(s):                    26-38\r\nNUMA node3 CPU(s):                    39-51\r\nNUMA node4 CPU(s):                    52-64\r\nNUMA node5 CPU(s):                    65-77\r\nNUMA node6 CPU(s):                    78-90\r\nNUMA node7 CPU(s):                    91-103\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.1\r\n[pip3] torchvision==0.18.1\r\n[pip3] transformers==4.43.1\r\n[pip3] triton==2.3.1\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.3.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    NIC9    NIC10   NIC11   CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV18    NV18    NV18    NV18    NV18    NV18    NV18    PIX     PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     0-12    0               N/A\r\nGPU1    NV18     X      NV18    NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     26-38   2               N/A\r\nGPU2    NV18    NV18     X      NV18    NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     39-51   3               N/A\r\nGPU3    NV18    NV18    NV18     X      NV18    NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     13-25   1               N/A\r\nGPU4    NV18    NV18    NV18    NV18     X      NV18    NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX     PIX     SYS     SYS     SYS     52-64   4               N/A\r\nGPU5    NV18    NV18    NV18    NV18    NV18     X      NV18    NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     78-90   6               N/A\r\nGPU6    NV18    NV18    NV18    NV18    NV18    NV18     X      NV18    SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     91-103  7               N/A\r\nGPU7    NV18    NV18    NV18    NV18    NV18    NV18    NV18     X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     65-77   5               N/A\r\nNIC0    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC1    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC2    PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC3    SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC4    SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC5    SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS     SYS     SYS     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      PIX     PIX     SYS     SYS     SYS\r\nNIC7    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX      X      PIX     SYS     SYS     SYS\r\nNIC8    SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     PIX      X      SYS     SYS     SYS\r\nNIC9    SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS     SYS\r\nNIC10   SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      SYS\r\nNIC11   SYS     SYS     SYS     SYS     SYS     SYS     SYS     PIX     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS     SYS      X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n  NIC9: mlx5_9\r\n  NIC10: mlx5_10\r\n  NIC11: mlx5_11\r\n```\r\n\n\n### üêõ Describe the bug\n\nCrash when running more than 1 concurrent requests with spec decode and LLama 405B. Single concurrent request works.\r\n\r\nStartup command:\r\n```\r\nvllm serve hugging-quants/Meta-Llama-3.1-405B-Instruct-GPTQ-INT4 --tensor-parallel-size 4 --use-v2-block-manager --speculative-model ModelCloud/Meta-Llama-3.1-8B-Instruct-gptq-4bit --speculative-draft-tensor-parallel-size 1 --num-speculative-tokens 5 --max-model-len 4096\r\n```\r\n\r\nAs suggested by @cadedaniel, disabling the on-GPU advance step optimization bypasses the issue:\r\n\r\n```python\r\nallow_gpu_advance_step = False\r\n```\r\n[Here](https://github.com/vllm-project/vllm/blob/58f53034add8767c9e5d92431220faa409fa3dc2/vllm/spec_decode/draft_model_runner.py#L30C1-L30C30)\r\n\r\nCrash:\r\n```\r\nINFO 07-23 18:58:27 async_llm_engine.py:173] Added request chat-cd12945076fe419b946e48f076471bb9.\r\nINFO 07-23 18:58:27 metrics.py:396] Avg prompt throughput: 3.9 tokens/s, Avg generation throughput: 0.1 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 0.2%, CPU KV cache usage: 0.0%.\r\nERROR 07-23 18:58:31 async_llm_engine.py:56] Engine background task failed\r\nERROR 07-23 18:58:31 async_llm_engine.py:56] Traceback (most recent call last):\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 46, in _log_task_completion\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return_value = task.result()\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 637, in run_engine_loop\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     result = task.result()\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 580, in engine_step\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 253, in step_async\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     output = await self.model_executor.execute_model_async(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 175, in execute_model_async\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 210, in _driver_execute_model_async\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return await self.driver_exec_model(execute_model_req)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return func(*args, **kwargs)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 375, in execute_model\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return self._run_speculative_decoding_step(execute_model_req,\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return func(*args, **kwds)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 526, in _run_speculative_decoding_step\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     proposals = self.proposer_worker.get_spec_proposals(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/smaller_tp_proposer_worker.py\", line 132, in get_spec_proposals\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return self._worker.get_spec_proposals(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/multi_step_worker.py\", line 207, in get_spec_proposals\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return self._proposer.get_spec_proposals(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/top1_proposer.py\", line 77, in get_spec_proposals\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     maybe_sampler_output, transposed = self._worker.sampler_output(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return func(*args, **kwargs)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/multi_step_worker.py\", line 79, in sampler_output\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     model_outputs = self.execute_model(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 272, in execute_model\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     output = self.model_runner.execute_model(\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     return func(*args, **kwargs)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/draft_model_runner.py\", line 342, in execute_model\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     model_input = self._gpu_advance_step(model_input, outputs[-1])\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/draft_model_runner.py\", line 150, in _gpu_advance_step\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     self._update_flash_attn_metadata(attn_metadata, num_seqs, num_queries)\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]   File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/draft_model_runner.py\", line 91, in _update_flash_attn_metadata\r\nERROR 07-23 18:58:31 async_llm_engine.py:56]     assert attn_metadata.num_decode_tokens == num_seqs\r\nERROR 07-23 18:58:31 async_llm_engine.py:56] AssertionError\r\nException in callback functools.partial(<function _log_task_completion at 0x7f84f45816c0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f84e0093b80>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7f84f45816c0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f84e0093b80>>)>\r\nTraceback (most recent call last):\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 46, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 637, in run_engine_loop\r\n    result = task.result()\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 580, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 253, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 175, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 210, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 375, in execute_model\r\n    return self._run_speculative_decoding_step(execute_model_req,\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\r\n    return func(*args, **kwds)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/spec_decode_worker.py\", line 526, in _run_speculative_decoding_step\r\n    proposals = self.proposer_worker.get_spec_proposals(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/smaller_tp_proposer_worker.py\", line 132, in get_spec_proposals\r\n    return self._worker.get_spec_proposals(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/multi_step_worker.py\", line 207, in get_spec_proposals\r\n    return self._proposer.get_spec_proposals(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/top1_proposer.py\", line 77, in get_spec_proposals\r\n    maybe_sampler_output, transposed = self._worker.sampler_output(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/multi_step_worker.py\", line 79, in sampler_output\r\n    model_outputs = self.execute_model(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/worker/worker_base.py\", line 272, in execute_model\r\n    output = self.model_runner.execute_model(\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/draft_model_runner.py\", line 342, in execute_model\r\n    model_input = self._gpu_advance_step(model_input, outputs[-1])\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/draft_model_runner.py\", line 150, in _gpu_advance_step\r\n    self._update_flash_attn_metadata(attn_metadata, num_seqs, num_queries)\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/spec_decode/draft_model_runner.py\", line 91, in _update_flash_attn_metadata\r\n    assert attn_metadata.num_decode_tokens == num_seqs\r\nAssertionError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/home/adam/405/venv/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 58, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\nINFO 07-23 18:58:31 async_llm_engine.py:180] Aborted request chat-93f7c7a915de49f692a1bd660fdca238.\r\nINFO 07-23 18:58:31 async_llm_engine.py:180] Aborted request chat-cd12945076fe419b946e48f076471bb9.\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-23T19:38:49Z",
    "closed_at": "2024-07-25T05:33:58Z",
    "author": "alugowski",
    "comments_count": 3,
    "comments": [
      {
        "author": "comaniac",
        "body": "`attn_metadata.num_decode_tokens` and `num_seqs` won't change in multi-step runner, these two values must be different originally. Since disabling advance step works, it's likely that the actual prepare input works fine but the advance step kernel doesn't cover this case. ~~For short term fix I could move the flash attn checkers to the guard so that we could fallback in this case.~~ Found that it's a bit tricky because we haven't prepared inputs when making the decision of using advance step or not. The immediate working solution I could think of is allowing users to disable this when needed.\r\n\r\ncc @alexm-neuralmagic ",
        "created_at": "2024-07-23T20:00:32Z"
      },
      {
        "author": "alugowski",
        "body": "Note `--enforce-eager` does NOT crash.",
        "created_at": "2024-07-23T20:38:44Z"
      },
      {
        "author": "cadedaniel",
        "body": "I hit this with 70B too",
        "created_at": "2024-07-24T00:47:17Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6607": {
    "issue_number": 6607,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6607",
    "title": "[Bug]: Phi-3-mini does not work when using Ray",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Fedora Linux 40 (Workstation Edition) (x86_64)\r\nGCC version: (GCC) 14.1.1 20240701 (Red Hat 14.1.1-7)\r\nClang version: 18.1.6 (Fedora 18.1.6-3.fc40)\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.39\r\n\r\nPython version: 3.11.9 (main, Apr 19 2024, 16:48:06) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.9.9-200.fc40.x86_64-x86_64-with-glibc2.39\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 1: NVIDIA GeForce RTX 4090\r\n\r\nNvidia driver version: 555.58.02\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        48 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               32\r\nOn-line CPU(s) list:                  0-31\r\nVendor ID:                            AuthenticAMD\r\nModel name:                           AMD Ryzen 9 7950X 16-Core Processor\r\nCPU family:                           25\r\nModel:                                97\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   16\r\nSocket(s):                            1\r\nStepping:                             2\r\nCPU(s) scaling MHz:                   51%\r\nCPU max MHz:                          5881.0000\r\nCPU min MHz:                          545.0000\r\nBogoMIPS:                             8999.97\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid overflow_recov succor smca fsrm flush_l1d amd_lbr_pmc_freeze\r\nVirtualization:                       AMD-V\r\nL1d cache:                            512 KiB (16 instances)\r\nL1i cache:                            512 KiB (16 instances)\r\nL2 cache:                             16 MiB (16 instances)\r\nL3 cache:                             64 MiB (2 instances)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-31\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Not affected\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Not affected\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Not affected\r\nVulnerability Spec rstack overflow:   Mitigation; Safe RET\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS; IBPB conditional; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.16.1\r\n[pip3] onnxruntime==1.18.1\r\n[pip3] onnxruntime-gpu==1.18.1\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.4\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] sentence-transformers     3.0.1                    pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.4                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-31    0               N/A\r\nGPU1    PHB      X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nFeel free to use [this gist](https://gist.github.com/baughmann/f4bcdbdef07e05b587aa04ccc7f0cf7c) with a minimal Jupyter notebook.\r\n\r\nWhen attempting to load any Phi-3 mini/small model using the `AsyncLLMEngine` and specifying `ray` as the distributed backend, Ray throws a:\r\n\r\n```text\r\nray.exceptions.RaySystemError: System error: No module named 'transformers_modules'\r\n```\r\n\r\nA `pip list` in my main project shows\r\n```text\r\nsentence-transformers           3.0.1\r\ntransformers                             4.42.4\r\n```\r\nalthough it sounds like this is likely not a bug with my project.\r\n\r\nI highly encourage you to look at the Jupyter notebook, but for completeness, here's how I'm trying to load the model:\r\n\r\n```python\r\nfrom vllm import AsyncEngineArgs, AsyncLLMEngine\r\n\r\n# model source: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\r\n\r\n# this config works\r\nmp_args = AsyncEngineArgs(\r\n    model=\"../../models/microsoft/Phi-3-mini-128k-instruct\",\r\n    trust_remote_code=True,\r\n    distributed_executor_backend=\"mp\",\r\n    max_model_len=8000, # limit mem utilization for this example\r\n    disable_sliding_window=True, # needed in order to use flash-attn\r\n)\r\n\r\n# this config does not work. it just sits at \r\n#   \"INFO worker.py:1779 -- Started a local Ray instance. View the dashboard at...\"\r\n# The actor dies with a `ray.exceptions.RaySystemError: System error: No module named 'transformers_modules'`\r\nray_args = AsyncEngineArgs(\r\n    model=\"../../models/microsoft/Phi-3-mini-128k-instruct\",\r\n    trust_remote_code=True,\r\n    max_model_len=8000,\r\n    engine_use_ray=True,\r\n    distributed_executor_backend=\"ray\",\r\n)\r\n\r\n# engine = AsyncLLMEngine.from_engine_args(mp_args)\r\nengine = AsyncLLMEngine.from_engine_args(ray_args)\r\n```\r\n\r\nAdditionally, here's the full System log from the dead actor:\r\n```text\r\n1[2024-07-20 10:54:21,234 I 193920 193920] core_worker_process.cc:107: Constructing CoreWorkerProcess. pid: 193920\r\n[2024-07-20 10:54:21,235 I 193920 193920] io_service_pool.cc:35: IOServicePool is running with 1 io_service.\r\n[2024-07-20 10:54:21,236 I 193920 193920] grpc_server.cc:134: worker server started, listening on port 32921.\r\n[2024-07-20 10:54:21,238 I 193920 193920] core_worker.cc:275: Initializing worker at address: 192.168.88.7:32921, worker ID 8251a92f00f8164eafa28810f113655f1ff265d396be3f6ab41f0ba5, raylet b0e273bf47f964aa3b48176dcb0fc921baec9887c10c232502e64270\r\n[2024-07-20 10:54:21,238 I 193920 193920] task_event_buffer.cc:177: Reporting task events to GCS every 1000ms.\r\n[2024-07-20 10:54:21,239 I 193920 193920] core_worker.cc:704: Adjusted worker niceness to 15\r\n[2024-07-20 10:54:21,239 I 193920 193957] core_worker.cc:643: Event stats:\r\n\r\n\r\nGlobal stats: 13 total (9 active)\r\nQueueing time: mean = 6.636 us, max = 45.069 us, min = 6.370 us, total = 86.269 us\r\nExecution time:  mean = 21.033 us, total = 273.426 us\r\nEvent stats:\r\n\tPeriodicalRunner.RunFnPeriodically - 7 total (5 active, 1 running), Execution time: mean = 1.971 us, total = 13.800 us, Queueing time: mean = 11.414 us, max = 45.069 us, min = 34.830 us, total = 79.899 us\r\n\tCoreWorker.ExitIfParentRayletDies - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\tray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberCommandBatch - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\tray::rpc::WorkerInfoGcsService.grpc_client.AddWorkerInfo - 1 total (0 active), Execution time: mean = 229.476 us, total = 229.476 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\tray::rpc::InternalPubSubGcsService.grpc_client.GcsSubscriberPoll - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\tray::rpc::WorkerInfoGcsService.grpc_client.AddWorkerInfo.OnReplyReceived - 1 total (0 active), Execution time: mean = 30.150 us, total = 30.150 us, Queueing time: mean = 6.370 us, max = 6.370 us, min = 6.370 us, total = 6.370 us\r\n\tPublisher.CheckDeadSubscribers - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\r\n-----------------\r\nTask Event stats:\r\n\r\nIO Service Stats:\r\n\r\nGlobal stats: 4 total (1 active)\r\nQueueing time: mean = 4.330 us, max = 12.240 us, min = 5.080 us, total = 17.320 us\r\nExecution time:  mean = 71.079 us, total = 284.315 us\r\nEvent stats:\r\n\tPeriodicalRunner.RunFnPeriodically - 1 total (0 active), Execution time: mean = 58.839 us, total = 58.839 us, Queueing time: mean = 12.240 us, max = 12.240 us, min = 12.240 us, total = 12.240 us\r\n\tray::rpc::TaskInfoGcsService.grpc_client.AddTaskEventData - 1 total (0 active), Execution time: mean = 218.797 us, total = 218.797 us, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\tCoreWorker.deadline_timer.flush_task_events - 1 total (1 active), Execution time: mean = 0.000 s, total = 0.000 s, Queueing time: mean = 0.000 s, max = -0.000 s, min = 9223372036.855 s, total = 0.000 s\r\n\tray::rpc::TaskInfoGcsService.grpc_client.AddTaskEventData.OnReplyReceived - 1 total (0 active), Execution time: mean = 6.679 us, total = 6.679 us, Queueing time: mean = 5.080 us, max = 5.080 us, min = 5.080 us, total = 5.080 us\r\nOther Stats:\r\n\tgrpc_in_progress:0\r\n\tcurrent number of task status events in buffer: 0\r\n\tcurrent number of profile events in buffer: 0\r\n\tcurrent number of dropped task attempts tracked: 0\r\n\ttotal task events sent: 0 MiB\r\n\ttotal number of task attempts sent: 0\r\n\ttotal number of task attempts dropped reported: 0\r\n\ttotal number of sent failure: 0\r\n\tnum status task events dropped: 0\r\n\tnum profile task events dropped: 0\r\n\r\n\r\n[2024-07-20 10:54:21,239 I 193920 193920] event.cc:234: Set ray event level to warning\r\n[2024-07-20 10:54:21,239 I 193920 193920] event.cc:342: Ray Event initialized for CORE_WORKER\r\n[2024-07-20 10:54:21,239 I 193920 193957] accessor.cc:668: Received notification for node id = b0e273bf47f964aa3b48176dcb0fc921baec9887c10c232502e64270, IsAlive = 1\r\n[2024-07-20 10:54:21,239 I 193920 193957] core_worker.cc:4735: Number of alive nodes:1\r\n[2024-07-20 10:54:21,240 I 193920 193920] direct_actor_task_submitter.cc:36: Set max pending calls to -1 for actor 926791420e82ba35c48a118601000000\r\n[2024-07-20 10:54:21,240 I 193920 193920] direct_actor_task_submitter.cc:237: Connecting to actor 926791420e82ba35c48a118601000000 at worker 8251a92f00f8164eafa28810f113655f1ff265d396be3f6ab41f0ba5\r\n[2024-07-20 10:54:21,240 I 193920 193920] core_worker.cc:3010: Creating actor: 926791420e82ba35c48a118601000000\r\n[2024-07-20 10:54:22,071 I 193920 193920] core_worker.cc:878: Exit signal received, this process will exit after all outstanding tasks have finished, exit_type=USER_ERROR, detail=Worker exits because there was an exception in the initialization method (e.g., __init__). Fix the exceptions from the initialization to resolve the issue. Exception raised from an actor init method. Traceback: The actor died because of an error raised in its creation task, \u001b[36mray::_AsyncLLMEngine.__init__()\u001b[39m (pid=193920, ip=192.168.88.7, actor_id=926791420e82ba35c48a118601000000, repr=<vllm.engine.async_llm_engine._AsyncLLMEngine object at 0x7fbb4bd12650>)\r\n  At least one of the input arguments for this task could not be computed:\r\nray.exceptions.RaySystemError: System error: No module named 'transformers_modules'\r\ntraceback: Traceback (most recent call last):\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'transformers_modules'\r\n[2024-07-20 10:54:22,071 W 193920 193920] direct_actor_transport.cc:189: Actor creation task finished with errors, task_id: ffffffffffffffff926791420e82ba35c48a118601000000, actor_id: 926791420e82ba35c48a118601000000, status: CreationTaskError: Exception raised from an actor init method. Traceback: The actor died because of an error raised in its creation task, \u001b[36mray::_AsyncLLMEngine.__init__()\u001b[39m (pid=193920, ip=192.168.88.7, actor_id=926791420e82ba35c48a118601000000, repr=<vllm.engine.async_llm_engine._AsyncLLMEngine object at 0x7fbb4bd12650>)\r\n  At least one of the input arguments for this task could not be computed:\r\nray.exceptions.RaySystemError: System error: No module named 'transformers_modules'\r\ntraceback: Traceback (most recent call last):\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'transformers_modules'\r\n[2024-07-20 10:54:22,079 I 193920 193920] core_worker.cc:856: Try killing all child processes of this worker as it exits. Child process pids: \r\n[2024-07-20 10:54:22,080 I 193920 193920] core_worker.cc:815: Disconnecting to the raylet.\r\n[2024-07-20 10:54:22,080 I 193920 193920] raylet_client.cc:161: RayletClient::Disconnect, exit_type=USER_ERROR, exit_detail=Worker exits because there was an exception in the initialization method (e.g., __init__). Fix the exceptions from the initialization to resolve the issue. Exception raised from an actor init method. Traceback: The actor died because of an error raised in its creation task, \u001b[36mray::_AsyncLLMEngine.__init__()\u001b[39m (pid=193920, ip=192.168.88.7, actor_id=926791420e82ba35c48a118601000000, repr=<vllm.engine.async_llm_engine._AsyncLLMEngine object at 0x7fbb4bd12650>)\r\n  At least one of the input arguments for this task could not be computed:\r\nray.exceptions.RaySystemError: System error: No module named 'transformers_modules'\r\ntraceback: Traceback (most recent call last):\r\n          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n          ^^^^^^^^^^^^^^^^^^^^^\r\nModuleNotFoundError: No module named 'transformers_modules', has creation_task_exception_pb_bytes=1\r\n[2024-07-20 10:54:22,080 I 193920 193920] core_worker.cc:723: Shutting down a core worker.\r\n[2024-07-20 10:54:22,080 I 193920 193920] task_event_buffer.cc:188: Shutting down TaskEventBuffer.\r\n[2024-07-20 10:54:22,080 I 193920 193978] task_event_buffer.cc:170: Task event buffer io service stopped.\r\n[2024-07-20 10:54:22,080 I 193920 193920] core_worker.cc:749: Disconnecting a GCS client.\r\n[2024-07-20 10:54:22,080 I 193920 193920] core_worker.cc:753: Waiting for joining a core worker io thread. If it hangs here, there might be deadlock or a high load in the core worker io service.\r\n[2024-07-20 10:54:22,080 I 193920 193957] core_worker.cc:986: Core worker main io service stopped.\r\n[2024-07-20 10:54:22,084 I 193920 193920] core_worker.cc:766: Core worker ready to be deallocated.\r\n[2024-07-20 10:54:22,084 I 193920 193920] core_worker_process.cc:245: Task execution loop terminated. Removing the global worker.\r\n[2024-07-20 10:54:22,084 I 193920 193920] core_worker.cc:714: Core worker is destructed\r\n[2024-07-20 10:54:22,084 I 193920 193920] task_event_buffer.cc:188: Shutting down TaskEventBuffer.\r\n[2024-07-20 10:54:22,084 I 193920 193920] core_worker_process.cc:148: Destructing CoreWorkerProcessImpl. pid: 193920\r\n[2024-07-20 10:54:22,084 I 193920 193920] io_service_pool.cc:47: IOServicePool is stopped.\r\n[2024-07-20 10:54:22,239 I 193920 193920] stats.h:120: Stats module has shutdown.\r\n```\r\n\r\nAlso, thank you guys for such a great library. Its very easy and fun to use and bugs like this are few and far between :smile: \r\n\r\nEdit: I've also tried this with 0.5.2 and the 0.5.3 prerelease per @rkooo567 's question",
    "state": "closed",
    "labels": [
      "bug",
      "ray"
    ],
    "created_at": "2024-07-20T14:51:24Z",
    "closed_at": "2024-10-22T02:46:25Z",
    "author": "baughmann",
    "comments_count": 21,
    "comments": [
      {
        "author": "youkaichao",
        "body": "does `distributed_executor_backend=\"mp\"` work?",
        "created_at": "2024-07-22T16:47:01Z"
      },
      {
        "author": "youkaichao",
        "body": "cc @rkooo567 @richardliaw for the ray error.",
        "created_at": "2024-07-22T16:47:04Z"
      },
      {
        "author": "rkooo567",
        "body": "I feel like I have seen this before, and it may have been fixed in the latest version. have you tried 0.5.2? ",
        "created_at": "2024-07-23T01:13:42Z"
      },
      {
        "author": "baughmann",
        "body": "> I feel like I have seen this before, and it may have been fixed in the latest version. have you tried 0.5.2? \r\n\r\nOh dang I didn't even realize it came out. Let me upgrade and report back.\r\n\r\nEdit:\r\nSame result with both 0.5.2 and 0.5.3 using the notebook posted in my OP. The actor dies immediately with `ModuleNotFoundError: No module named 'transformers_modules'`. Good catch, though @rkooo567 ",
        "created_at": "2024-07-23T13:14:02Z"
      },
      {
        "author": "baughmann",
        "body": "> does `distributed_executor_backend=\"mp\"` work?\r\n\r\nYes. I didn't explicitly set it in the original notebook because, as I understand it, `mp` is the default if there are enough GPUs on the local system to satisfy the tensor parallelization requirement. I'm updating the original notebook to include that in the working engine args. I'm also adding `disable_sliding_window` to the working args because otherwise it doesn't use `flash-attn`",
        "created_at": "2024-07-23T13:35:23Z"
      },
      {
        "author": "rkooo567",
        "body": "I assume it is the same issue as https://github.com/vllm-project/vllm/pull/4286. \r\n\r\n@baughmann is there a repro I can try? ",
        "created_at": "2024-07-23T21:00:47Z"
      },
      {
        "author": "rkooo567",
        "body": "maybe we need more fundamental solution for this case",
        "created_at": "2024-07-23T21:00:55Z"
      },
      {
        "author": "baughmann",
        "body": "@rkooo567 Thanks for looking into this.\r\n\r\nI put the content of the Jupyter notebook that I used to produce the error in the OP, but I'll also put it here for your convenience :)\r\n\r\nThe model used is just [the official one](https://huggingface.co/microsoft/Phi-3-mini-128k-instruct).\r\n\r\n```python\r\nfrom vllm import AsyncEngineArgs, AsyncLLMEngine\r\n\r\n# model source: https://huggingface.co/microsoft/Phi-3-mini-128k-instruct\r\n\r\n# this config works\r\nmp_args = AsyncEngineArgs(\r\n    model=\"../../models/microsoft/Phi-3-mini-128k-instruct\",\r\n    trust_remote_code=True,\r\n    distributed_executor_backend=\"mp\",\r\n    max_model_len=8000, # limit mem utilization for this example\r\n    disable_sliding_window=True, # needed in order to use flash-attn\r\n)\r\n\r\n# this config does not work. it just sits at \r\n#   \"INFO worker.py:1779 -- Started a local Ray instance. View the dashboard at...\"\r\n# The actor dies with a `ray.exceptions.RaySystemError: System error: No module named 'transformers_modules'`\r\nray_args = AsyncEngineArgs(\r\n    model=\"../../models/microsoft/Phi-3-mini-128k-instruct\",\r\n    trust_remote_code=True,\r\n    max_model_len=8000,\r\n    engine_use_ray=True,\r\n    distributed_executor_backend=\"ray\",\r\n)\r\n\r\n# engine = AsyncLLMEngine.from_engine_args(mp_args)\r\nengine = AsyncLLMEngine.from_engine_args(ray_args)\r\n```",
        "created_at": "2024-07-23T21:52:57Z"
      },
      {
        "author": "tjohnson31415",
        "body": "Hello. I've been investigating the same error but in the context of multi-node inference with Ray. I created https://github.com/vllm-project/vllm/pull/6751 which fixes the issue for me.\r\n\r\nPerhaps my fix will help in this scenario as well. I attempted to reproduce the error raised here using the code examples in this issue, but was unable to (using the latest vLLM code); the `AsyncLLMEngine` is created without error for me ü§î ",
        "created_at": "2024-07-24T15:31:00Z"
      },
      {
        "author": "baughmann",
        "body": "@tjohnson31415 In my case I'm running only on a single node.\r\n\r\nAnd oh, wow, the notebook didn't give you problems using the ray args? How strange",
        "created_at": "2024-07-24T16:43:13Z"
      },
      {
        "author": "tjohnson31415",
        "body": "> In my case I'm running only on a single node.\r\n\r\nYeah, that makes it interesting. In my understanding, https://github.com/vllm-project/vllm/pull/4286 should be the most recent fix for the single node case, but that fix looks like it was included in release v0.4.1...\r\nI just tested in my env with vllm==v0.5.1 and it worked with that too ü§î\r\n\r\nSome other thoughts/questions:\r\n- Does it work for you if you use `engine_use_ray=False`? Maybe that is a new piece of the puzzle.\r\n- You said you run this in a Jupyter notebook. If you run your code just as a python script, do you get the same error?\r\n    - I'm not running in Jupyter in my test... \r\n- Do the `transformers_modules` dynamic modules exist in `~/.cache/huggingface/modules/transformers_modules/` (assuming `HF_HOME` and `HF_MODULES_HOME` are unset)? And are they regenerated by running your code?\r\n    - I `rm -rf ~/.cache/huggingface/modules/` each time I try to run the code to confirm that the modules do get regenerated.",
        "created_at": "2024-07-24T17:23:00Z"
      },
      {
        "author": "rkooo567",
        "body": "Let me also take a look at it quickly. I am a little busy by other high priority task from our end. ",
        "created_at": "2024-07-25T06:49:10Z"
      },
      {
        "author": "baughmann",
        "body": "@tjohnson31415 What I'll do is make a minimal conda env with a minimal reqs etc. and then perform the troubleshooting on that.\n\nI will upload and post that repo when I'm able, but it may not be until later today ",
        "created_at": "2024-07-25T13:29:48Z"
      },
      {
        "author": "baughmann",
        "body": "@tjohnson31415 @rkooo567 Here is a [repo](https://github.com/baughmann/vllm-phi-3-ray-problem) with conda for you all. Also created a basic readme for your convenience.\r\n\r\n@tjohnson31415 Here's the update regarding your suggestions:\r\n\r\n> Does it work for you if you use engine_use_ray=False?\r\n\r\nYes, it does. As I understand it, with `emgine_use_ray=False`, it's not using Ray--it's using multiprocessing (I could be wrong about this though). I was able to use phi-3 with mp before, but I need to use it with Ray.\r\n\r\n> You said you run this in a Jupyter notebook. If you run your code just as a python script, do you get the same error?\r\n\r\nYes, I'm afraid so. That's not surprising though as I run Jupyter notebooks in the same exact development environment I'm writing my application in. For me, it's just a quick and easy way to experiment with specific parts of my application.\r\n\r\n> Do the transformers_modules dynamic modules exist in...\r\n\r\nYes, it does. I added a note about this in the readme of the repo I posted. I see a `Phi-3-mini-128k-instruct` directory in that folder. If I delete `.cache/huggingface/modules/transformers_modules/` it gets re-created the next time I try to run it with Ray.\r\n",
        "created_at": "2024-07-25T23:03:41Z"
      },
      {
        "author": "tjohnson31415",
        "body": "@baughmann Ah, thanks for creating the repro-repo! I didn't realize that `engine_use_ray=True` would not print out any logs from the main engine loop. I can see the `System error: No module named 'transformers_modules'` error by looking in the logs in the Ray dashboard, as you stated in your repro steps.\r\n\r\n> As I understand it, with engine_use_ray=False, it's not using Ray\r\n\r\nTo make the workers executing the model use Ray, `distributed_executor_backend=\"ray\"` is sufficient. `engine_use_ray` is a separate configuration that puts the engine execution itself into a Ray process separate from the server process.\r\n\r\nThe error occurs because the Ray worker spawned for the engine loop with `engine_use_ray` does not have its python path updated to include the dynamic modules generated for `trust_remote_code`. The failure occurs when communicating the `ModelConfig` from the main process to the Ray engine worker [here](https://github.com/vllm-project/vllm/blob/aa4867791ecd73a5f55b7bad4d9372954e661fe4/vllm/engine/async_llm_engine.py#L547).\r\n\r\nThe current way that the (non-engine) Ray workers handle this is that the `WorkerWrapperBase` is initialized where it runs [`init_cached_hf_modules`](https://github.com/vllm-project/vllm/blob/5e8ca973ebd5584582923b8ed1d3d823769a80a5/vllm/utils.py#L727-L732) before the args containing the `ModelConfig` are passed into `init_worker`. A wrapper/base like that could be added for the engine worker too... Or I think something similar to my fix in https://github.com/vllm-project/vllm/pull/6751 to pass the model configuration as a simple class instead of as an instance of a dynamic class generated in `transformers_modules` would work too.\r\n\r\nBut quickest fix is `engine_use_ray=False`.",
        "created_at": "2024-07-26T19:56:45Z"
      },
      {
        "author": "baughmann",
        "body": "@tjohnson31415 That most certainly did it! Thank you for the detailed explanation, that makes a lot of sense!\r\n\r\nHowever, I would still expect feature parity among the supported models. Should we leave this ticket open, even though there is that workaround?",
        "created_at": "2024-07-27T09:17:11Z"
      },
      {
        "author": "justinthelaw",
        "body": "I am having this issue as well, and the workaround works. I am also curious as to when this will be implemented into the engine? If there is an open branch or fork, can someone link it here?\r\n\r\nEDIT: NVM I found it! Thank you all!",
        "created_at": "2024-08-01T15:57:05Z"
      },
      {
        "author": "tjohnson31415",
        "body": "Just to note it here, there is a new RFC to remove `--engine-use-ray` altogether:\r\nhttps://github.com/vllm-project/vllm/issues/7045\r\n\r\nIf the RFC is accepted, a fix for this issue may not be relevant for very long.",
        "created_at": "2024-08-01T20:41:09Z"
      },
      {
        "author": "nightflight-dk",
        "body": "It would appear mp exec is now also affected in 0.6.1.post2\r\nthese two arguments to LLM don't help:\r\n\r\n                  distributed_executor_backend=\"mp\",\r\n                  worker_use_ray=False,\r\n\r\n```\r\nModuleNotFoundError: No module named 'transformers_modules.Phi-3'\r\nTraceback (most recent call last):\r\n  File \"<string>\", line 1, in <module>\r\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\r\n    exitcode = _main(fd, parent_sentinel)\r\n  File \"/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\r\n    self = reduction.pickle.load(from_parent)\r\nModuleNotFoundError: No module named 'transformers_modules.Phi-3'\r\n```",
        "created_at": "2024-09-16T21:55:15Z"
      },
      {
        "author": "nightflight-dk",
        "body": "workaround for me was to remove '.' from the model name (path), before instantiating the engine. e.g. \"weights/Phi3.5mini-instruct\" -> \"weights/Phi35mini-instruct\" (on top of disabling Ray)",
        "created_at": "2024-09-17T21:11:08Z"
      },
      {
        "author": "youkaichao",
        "body": "@nightflight-dk  it makes sense. the name might be used for import, and `.` has special meaning in python's import system.",
        "created_at": "2024-09-17T23:50:44Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/590": {
    "issue_number": 590,
    "issue_url": "https://github.com/vllm-project/vllm/issues/590",
    "title": "GPTJ output not consistent with that of transformers",
    "body": "Hey guys,\r\n\r\nI've been trying to get the base GPT-J model (EleutherAI/gpt-j-6b) served using vLLM, but I am running into issues with the quality of the output. Using the same offline inference code as in https://github.com/vllm-project/vllm/blob/main/examples/offline_inference.py, here is what I obtain from the model (running on an RTX A5000):\r\n\r\n\r\n``` python3\r\nfrom vllm import LLM, SamplingParams\r\n\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=128)\r\nllm = LLM(model=\"EleutherAI/gpt-j-6b\")\r\nprompts = [\"AI is going to\"] * 4\r\noutputs = [t.outputs[0].text for t in llm.generate(prompts, sampling_params)]\r\n\r\noutput = [\" get a lot of new stuff. A lot of other new stuff\\nthat's been added and things are going to come out.\\nthat you know, it's going to be implemented, that's coming out.\\nin the\\nand we've got like a lot of stuff coming out, and things in the like AI stuff. So we're going to be doing.\\nbefore the new stuff\\nand some of the AI, the future.\\nthe AI\\nis added in the next year and we're going to the next year.\\nthat's going to be in the next gen.\\nand it's going to be added\",\r\n \" change the way we work, not just the way we do business, but also how we work and we live. How we work, the way we live. In the way we do our everyday activities. It will be. We think, what we live.\\n\\n\\n\\n\\n\\n\\n\\nthe way we interact with each other, but our living.\\n\\nthe way we relate to each other, but how we live. AI.\\n\\n\\nit's not even our lives. The economy, live, and build our daily life.\\nhow we play, but also our lives and engage with each other, and how we\",\r\n \" be our friend.\\n\\n\\n\\n\\nHere's a good reason we're gonna do this\\nAnd you know, and that was interesting.\\nWe'll have to be, like,\\nour employees are gonna start.\\nOur\\nThey did you know, as you know.\\nTotally, we're gonna have like you know\\nAnd that's the oil.\\nSo that some other question, like, and that we're gonna\\nuse and and I'm going to be in the most of the middle of\\nThe boss and we're going to be on TV,\\nTogether. I'm gonna be\\nWe're gonna\",\r\n \" save us all. Some of them, anyway. It will. AI is almost certainly. It's just a utopianism is what we can see how much about as serious, but it will be a way out of the future. I mean.\\n\\n\\nsmart, it's going to out how it will be their business is the father than we're going to the ultimate evolution. I think we're going to be the in a perfect, the machine.\\n\\nnot be a bit more than 3Dot-it's just it.\\n\\n\\n\\nkill us we have long as we're going to have it will be making\"]\r\n ```\r\n\r\nFor comparison's sake, here is what I obtain using transformers:\r\n\r\n```python3\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nmodel = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6b\").half().eval().to(0)\r\ntokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6b\")\r\ninputs = tokenizer(\"AI is going to\", return_tensors=\"pt\").to(0)\r\nparams = {\"temperature\": 0.85, \"top_p\": 0.94, \"max_new_tokens\": 128, \"do_sample\": True}\r\noutputs = []\r\nfor _ in range(1,4):\r\n    out = model.generate(inputs[\"input_ids\"], **params)\r\n    outputs.append(tokenizer.decode(out[0]))\r\noutputs = [\r\n'AI is going to be our future and this is going to be a big part of that,\" said Peter Lee, vice president of Google\\'s self-driving cars and mapping division.\\n\\n\"It will allow us to make the most efficient use of public roads that are more congested and have more traffic,\" Lee told the BBC. \"It will allow us to better respond to traffic conditions and accidents.\"\\n\\nIt\\'s certainly a big deal, and one that Google will have to work its way through. The company is a pioneer in the area of driverless cars, but its software has been unable to operate as a taxi driver.\\n\\nIts self',\r\n'AI is going to make a comeback. It's already making a comeback. The best technology to understand AI is not the technology itself. It's human understanding of what the technology is. We're just starting to understand how human beings think, and the way that human beings think is going to revolutionize the way we think.\\n\\nIt's hard for us to imagine the future that AI brings. We're seeing AI in every form, and we're seeing it becoming a reality in our lives. You may not see it right away, but it's right around the corner.\\n\\n',\r\n'AI is going to be a game changer and a very important factor in the success of the entire Internet. It will not be just another technology, but a game changing technology. The way we do things has been changed.\\n\\nAnd we can't wait to help you succeed in the online world!\\n\\nOur Vision\\n\\nOur vision is to provide the best eCommerce platform that can help your business grow. We believe in the concept of \"Customer First\". We will work with you and your team to deliver the best online experience.\\n\\nWe will continue to innovate and develop new features, functionality and services to help you',\r\n'AI is going to change everything. The world of medicine and the world of business will be forever changed.\\n\\nThe world of medicine will change drastically. Doctors will be able to diagnose illnesses and provide immediate treatment without needing to send their patients to specialists. A patient's condition will be known almost instantly and he/she will be able to receive the best possible treatment.\\n\\nThe world of business will change drastically as well. With the help of AI, companies can easily increase their efficiency, improve their products, and ultimately increase profits.\\n\\nWhy is AI so important?\\n\\nAI has become incredibly important because it can help us solve many'\r\n]\r\n```\r\n\r\nI've noticed this behaviour independent of the choice of sampling parameters.\r\n\r\nAny help in resloving this is greatly appreciated!\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2023-07-26T16:46:01Z",
    "closed_at": "2023-09-06T02:54:35Z",
    "author": "alexandred",
    "comments_count": 9,
    "comments": [
      {
        "author": "zhuohan123",
        "body": "Hi! Can you try some deterministic sampling methods like greedy decoding and compare the results? To be honest I cannot easily tell the differences between the qualities of two sampling results.",
        "created_at": "2023-07-26T20:08:51Z"
      },
      {
        "author": "alexandred",
        "body": "Thanks for getting back to me!\r\n\r\n@AlekseyKorshuk and I ran the inference with deterministic sampling and got the following results:\r\n\r\n```python3\r\nsampling_params = SamplingParams(temperature=0, top_k=-1, max_tokens=128)\r\nllm = LLM(model=\"philschmid/gpt-j-6B-fp16-sharded\")\r\nprompts = [\"Obama was born in Honolulu, Hawaii. After graduating from Columbia University in 1983, he worked as a community organizer in Chicago. In 1988, he enrolled in Harvard Law School, where he was the first black president of the Harvard Law Review. After graduating,\"] * 4\r\noutputs = [t.outputs[0].text for t in llm.generate(prompts, sampling_params)]\r\noutputs = [\r\n' he was a year later, he was a year. He was a student of the University of Chicago, he was a community organizer for the University of Chicago, he was a community organizer, he was a year, and then he was a year later, he was a year.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\r\n ' he was a year later, he was a year. He was a student of the University of Chicago, he was a community organizer for the University of Chicago, he was a community organizer, he was a year, and then he was a year later, he was a year.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\r\n ' he was a year later, he was a year. He was a student of the University of Chicago, he was a community organizer for the University of Chicago, he was a community organizer, he was a year, and then he was a year later, he was a year.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n',\r\n ' he was a year later, he was a year. He was a student of the University of Chicago, he was a community organizer for the University of Chicago, he was a community organizer, he was a year, and then he was a year later, he was a year.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n']\r\n```\r\n\r\nUsing the same prompt and model on HF transformers (using do_sample=False), we got the following completion:\r\n`Obama served as an attorney for the Illinois State Senate and then taught constitutional law at the University of Chicago.\\n`\r\n",
        "created_at": "2023-07-26T22:39:57Z"
      },
      {
        "author": "leegohi04517",
        "body": "For us, VLLM has greatly improved in terms of inference speed, but the difference in the inference results due to the different sampling methods between VLLM and Transformer is quite confusing. Transformer has a good ecosystem and there is consensus on the sampling parameters. Can it support the option of choosing between VLLM sampling and Transformer sampling methods?",
        "created_at": "2023-07-27T02:09:50Z"
      },
      {
        "author": "syskn",
        "body": "VLLM's outputs are definitely (significantly) more repetitive than HF inference (as you can see in Alex's results in the first post and the greedy decoding result). I think sampling process itself has no significant delta; it might be something in cuda kernels or attention.\r\n\r\nRelated topic I posted in discussions:\r\nhttps://github.com/vllm-project/vllm/discussions/471",
        "created_at": "2023-07-27T03:35:15Z"
      },
      {
        "author": "ri938",
        "body": "yeah it looks to me like GPTJ responses are poor quality when I tried it out.\r\n\r\nOther models seem to have much better inference quality, although they can be more repetitive than HF so I worry they also have regression, albeit much more minor.",
        "created_at": "2023-07-27T09:06:46Z"
      },
      {
        "author": "juliensalinas",
        "body": "Same problem here with GPT-J.\r\nI am using the default sampling params from vLLM. I also tried playing with all the params availble, but it did not help.\r\n\r\nI am consistently getting some gibberish in the outputs.\r\nHere is one example.\r\n\r\nInput:\r\n> Let me tell you a story:\r\n\r\nOutput:\r\n> Let me tell you a story:\\n\\nIt‚Äôs midnight. Your eyes begin to fall right at noon, I came out of Scotland a summer, right at the dock where this story take in the sky blue.\\n\\nAM,\\nNever felt feverich I never...",
        "created_at": "2023-07-30T20:14:07Z"
      },
      {
        "author": "PanQiWei",
        "body": "May be it's because GPT-J's tensor rotate logic is different to other models, as described in #747",
        "created_at": "2023-08-12T17:32:38Z"
      },
      {
        "author": "chiragjn",
        "body": "+1\r\nNoticing similar weird issues, with both GPT-J 6B and Pygmalion 6B (finetuned on top of GPT-J)\r\n```python\r\nimport torch\r\nfrom transformers import pipeline\r\n\r\nMODEL_ID = \"EleutherAI/gpt-j-6b\"\r\nPROMPT = \"\"\"\r\nimport tensorflow\r\n# 4 layer CNN with a softmax output\r\n# test on MNIST data set\r\n\"\"\"\r\nprompts = [PROMPT]\r\np = pipeline(\r\n    \"text-generation\", model=MODEL_ID, tokenizer=MODEL_ID, torch_dtype=torch.float16, device=1\r\n)\r\nout = p(\r\n    prompts, temperature=0.8,  top_p=0.95,  max_new_tokens=100, top_k=10000\r\n)\r\nprint(out[0][0][\"generated_text\"])\r\n```\r\n\r\nwhich produces a reasonable output\r\n```\r\nSetting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\r\n\r\nimport tensorflow\r\n# 4 layer CNN with a softmax output\r\n# test on MNIST data set\r\n\r\nimport numpy as np\r\nimport matplotlib.pyplot as plt\r\n\r\nfrom sklearn.datasets import load_digits\r\nfrom sklearn.model_selection import train_test_split\r\n\r\nfrom keras.models import Sequential\r\nfrom keras.layers import Dense, Dropout, Activation\r\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten\r\nfrom keras.optimizers import SG\r\n```\r\n\r\nBut with vLLM, something is off\r\n\r\n```python\r\nfrom vllm import LLM, SamplingParams\r\nllm = LLM(model=MODEL_ID)\r\nsampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens=100, top_k=10000)\r\noutputs = llm.generate(prompts, sampling_params)\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\nprint(generated_text)\r\n```\r\n\r\n```\r\nimport tensorflow\r\nfrom tensorflow as TF-layers\r\nfrom tensorflow_model\r\nimport tensorflow\r\nimport time\r\nimport nn\r\nimport tensorflow as tf.nn.data\r\n\r\nfrom tensorflow as tf\r\nfrom tensorflow\r\nimport tensorflow as tf\r\nfrom tensorflow as nn\r\n\r\n# from Tensorflow\r\nfrom tensorflow as tf.image classification\r\nimport tf\r\nimport tensorflow as tf\r\n```",
        "created_at": "2023-09-02T10:13:53Z"
      },
      {
        "author": "WoosukKwon",
        "body": "Hi @alexandred @chiragjn @juliensalinas @ri938 , thanks for letting us know the bug. This is indeed due to my misunderstanding of the GPT-J RoPE, which is different from GPT-NeoX's RoPE. The bug is fixed by #941. I've checked that after the bug fix, the generated outputs by vLLM match the HF's outputs when using argmax sampling. Apologies for the confusion and inconvenience.",
        "created_at": "2023-09-04T15:56:27Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/2059": {
    "issue_number": 2059,
    "issue_url": "https://github.com/vllm-project/vllm/issues/2059",
    "title": "mixtral-8x7B-Instruct-v0.1 giving garbage output on long prompts",
    "body": "Having some trouble pinpointing if its my prompts that is causing this, but often on long-ish prompts (5k+ tokens), the output is either completely unrelated (as in the attached image), or just a word/pair-of-words/phrase being repeated endlessly.\r\n\r\n<img width=\"909\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/35597204/3e1f31b3-6f64-497d-beb3-2ae3b4dd9a30\">\r\n\r\nIs anybody else facing the same issue?",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2023-12-12T15:21:45Z",
    "closed_at": "2023-12-13T20:28:15Z",
    "author": "vibhuagrawal14",
    "comments_count": 3,
    "comments": [
      {
        "author": "arshadshk7",
        "body": "same issue here\r\n",
        "created_at": "2023-12-13T08:57:19Z"
      },
      {
        "author": "WoosukKwon",
        "body": "Hi @vibhuagrawal14 Thanks for reporting the bug. We just fixed it in #2088. Please [install vLLM from source](https://docs.vllm.ai/en/latest/getting_started/installation.html#build-from-source) until we publish a new release.",
        "created_at": "2023-12-13T20:29:39Z"
      },
      {
        "author": "vibhuagrawal14",
        "body": "Thank you! ",
        "created_at": "2023-12-13T20:30:03Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/2064": {
    "issue_number": 2064,
    "issue_url": "https://github.com/vllm-project/vllm/issues/2064",
    "title": "[BUG] Mistral/Mixtral generate nonsense past 4096 tokens in prompt",
    "body": "If the prompt contains more than 4k tokens, the model will begin generating nonsense. This seems to be true for both Mistral and Mixtral. I launched the engine at both 32k and 8k max model lengths for testing. Tested with the latest vllm release.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2023-12-12T19:06:48Z",
    "closed_at": "2023-12-13T20:28:15Z",
    "author": "AlpinDale",
    "comments_count": 15,
    "comments": [
      {
        "author": "AlpinDale",
        "body": "![image](https://github.com/vllm-project/vllm/assets/52078762/298db9fb-2e01-4837-93f0-b0d99664d6ee)\r\nTested using [SillyTavern](https://github.com/SillyTavern/SillyTavern). No issues with the prompt (I manually inspected, and it works perfectly fine with other models). \r\n\r\nCC @WoosukKwon @zhuohan123 because this seems like an important bug.",
        "created_at": "2023-12-12T19:09:29Z"
      },
      {
        "author": "AlpinDale",
        "body": "![image](https://github.com/vllm-project/vllm/assets/52078762/db54943d-a1de-4458-a946-486b01ba96e8)\r\n",
        "created_at": "2023-12-12T19:12:18Z"
      },
      {
        "author": "stefanobranco",
        "body": "I have noticed the same thing. I'm running from docker, and tried with both the latest and built from source, with no difference.\r\n\r\n`docker run -it --gpus all -e HUGGING_FACE_HUB_TOKEN=$token --ipc=host -p 8080:8080 -v $volume:/root/.cache/huggingface docker.io/vllm/vllm-openai --model $model --tensor-parallel-size 8 --port 8080 --load-format pt`\r\n\r\nAs far as I can tell I'm not doing anything special, and the same setup works with hf tgi, so it doesn't appear to be a model issue.",
        "created_at": "2023-12-13T08:06:34Z"
      },
      {
        "author": "mehdiataei",
        "body": "Same issue",
        "created_at": "2023-12-13T08:51:33Z"
      },
      {
        "author": "arshadshk7",
        "body": "I am having the same issue with Mistral 7b.",
        "created_at": "2023-12-13T08:54:54Z"
      },
      {
        "author": "dev-mush",
        "body": "I am having the same issue, was wondering if it has something to do with the sliding_window config, I'm running it using vLLM\r\n\r\nEdit: Yup, you have tu update the sliding_window attention in the config, it defaults to 4096, upping it up solves it.",
        "created_at": "2023-12-13T11:28:40Z"
      },
      {
        "author": "vibhuagrawal14",
        "body": "@dev-mush what value did you update it to?\r\n",
        "created_at": "2023-12-13T12:16:47Z"
      },
      {
        "author": "dev-mush",
        "body": "> @dev-mush what value did you update it to?\r\n\r\n32k, mistral team says that it should theoretically support up to 128k tokens, but the problem went away as soon as I got it to 8k...I'm having troubles understanding how the sliding_window affects the quality of the output to be honest, so any hint is well appreciated ‚ò∫Ô∏è",
        "created_at": "2023-12-13T12:51:38Z"
      },
      {
        "author": "vibhuagrawal14",
        "body": "with 32k, I am getting `rpc error: code = DeadlineExceeded desc = context deadline exceeded%`\r\n8k is working though, but the garbage is still there for me :/ \r\n\r\nfull traceback here:\r\n\r\n```\r\nTraceback (most recent call last):‚ñâ| 12.1G/12.1G [05:32<00:00, 40.6MB/s]\r\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main]\r\n    return _run_code(code, main_globals, None,\r\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 729, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 495, in from_engine_args\r\n    engine = cls(parallel_config.worker_use_ray,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 269, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 314, in _init_engine\r\n    return engine_class(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 107, in __init__\r\n    self._init_workers_ray(placement_group)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 194, in _init_workers_ray\r\n    self._run_workers(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 750, in _run_workers\r\n    self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 727, in _run_workers_in_batch\r\n    all_outputs = ray.get(all_outputs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2563, in get\r\n    raise value.as_instanceof_cause()\r\nray.exceptions.RayTaskError(ChunkedEncodingError): ray::RayWorkerVllm.execute_method() (pid=7598, ip=192.168.25.172, actor_id=9a702c3d2722e46a47e1444101000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1781fa3d90>)\r\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 833, in _raw_read\r\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\nurllib3.exceptions.IncompleteRead: IncompleteRead(4963331015 bytes read, 7171139841 more expected)\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nray::RayWorkerVllm.execute_method() (pid=7598, ip=192.168.25.172, actor_id=9a702c3d2722e46a47e1444101000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1781fa3d90>)\r\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 816, in generate\r\n    yield from self.raw.stream(chunk_size, decode_content=True)\r\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 934, in stream\r\n    data = self.read(amt=amt, decode_content=decode_content)\r\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 905, in read\r\n    data = self._raw_read(amt)\r\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 811, in _raw_read\r\n    with self._error_catcher():\r\n  File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n    self.gen.throw(typ, value, traceback)\r\n  File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 729, in _error_catcher\r\n    raise ProtocolError(f\"Connection broken: {e!r}\", e) from e\r\nurllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(4963331015 bytes read, 7171139841 more expected)', IncompleteRead(4963331015 bytes read, 7171139841 more expected))\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nray::RayWorkerVllm.execute_method() (pid=7598, ip=192.168.25.172, actor_id=9a702c3d2722e46a47e1444101000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1781fa3d90>)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/ray_utils.py\", line 32, in execute_method\r\n    return executor(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 72, in load_model\r\n    self.model_runner.load_model()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 36, in load_model\r\n    self.model = get_model(self.model_config)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader.py\", line 124, in get_model\r\n    model.load_weights(model_config.model, model_config.download_dir,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mixtral.py\", line 519, in load_weights\r\n    for name, loaded_weight in hf_model_weights_iterator(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/weight_utils.py\", line 201, in hf_model_weights_iterator\r\n    hf_folder, hf_weights_files, use_safetensors = prepare_hf_model_weights(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/weight_utils.py\", line 141, in prepare_hf_model_weights\r\n    hf_folder = snapshot_download(model_name_or_path,\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_snapshot_download.py\", line 238, in snapshot_download\r\n    thread_map(\r\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\r\n    return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\r\n    return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\r\n  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1170, in __iter__\r\n    for obj in iterable:\r\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\r\n    yield _result_or_cancel(fs.pop())\r\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\r\n    return fut.result(timeout)\r\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\r\n    return self.__get_result()\r\n  File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n    raise self._exception\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_snapshot_download.py\", line 213, in _inner_hf_hub_download\r\n    return hf_hub_download(\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n    return fn(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1461, in hf_hub_download\r\n    http_get(\r\n  File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 541, in http_get\r\n    for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\r\n  File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 818, in generate\r\n    raise ChunkedEncodingError(e)\r\nrequests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(4963331015 bytes read, 7171139841 more expected)', IncompleteRead(4963331015 bytes read, 7171139841 more expected))\r\nconsolidated.01.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.1G/12.1G [05:32<00:00, 36.5MB/s]\r\nrpc error: code = DeadlineExceeded desc = context deadline exceeded%\r\n```",
        "created_at": "2023-12-13T14:27:25Z"
      },
      {
        "author": "arshadshk",
        "body": " @dev-mush wouldn't it affect it if we increase the sliding window to 8k? Since it's trained with windows of 4k, the overall scales of attention output would change.\r\n\r\nI wonder why using a 4k sliding window would be a problem since it is a \"sliding window.\" Even if the context size is 8k, it would still only attend to the previous 4k tokens, hence having the same effect as with 4k.\r\n\r\nhere sliding_window is used : \r\n<img width=\"792\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/39850778/4671f07e-8ee9-405c-8c9a-38cda3b96ce0\">\r\nhttps://github.com/vllm-project/vllm/blob/main/vllm/model_executor/layers/attention.py#L135-L137\r\n\r\nDont know why this should be a problem. \r\nIs there something else, like some positional embedding stuff that affects this ?? \r\nthis might be useful: https://github.com/vllm-project/vllm/pull/1746 \r\n",
        "created_at": "2023-12-13T16:33:49Z"
      },
      {
        "author": "WoosukKwon",
        "body": "Hi @AlpinDale Thanks for reporting the bug. We just fixed it in #2088. Please [install vLLM from source](https://docs.vllm.ai/en/latest/getting_started/installation.html#build-from-source) until we publish a new release.",
        "created_at": "2023-12-13T20:30:09Z"
      },
      {
        "author": "AlpinDale",
        "body": "@WoosukKwon that seems to have fixed the issue, but not *quite*. Now, the model completely ignores any tokens past the 4096 context window. Perhaps it'd be best if the sliding window value was aligned with the max model length? As in, the `max_model_len` arg would also change the sliding window value. Manually setting it to 32k in the config.json fixes this issue.",
        "created_at": "2023-12-14T00:56:20Z"
      },
      {
        "author": "WoosukKwon",
        "body": "@AlpinDale Could you provide a reproducible example?",
        "created_at": "2023-12-14T00:58:31Z"
      },
      {
        "author": "exceedzhang",
        "body": "I ran Mixtral Model using vLLM! I found Error!\r\nRuntimeError: probability tensor contains either `inf`, `nan` or element < 0\r\n<img width=\"1512\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/4583537/5a866e57-4c6b-42bb-a8b5-75f6514bba49\">\r\n\r\n<img width=\"1512\" alt=\"image\" src=\"https://github.com/vllm-project/vllm/assets/4583537/062b8fcd-d741-4e43-9a5f-0d4658005f7f\">\r\n\r\nCan anyone know why?\r\n",
        "created_at": "2023-12-14T07:16:38Z"
      },
      {
        "author": "rakesgi2022",
        "body": "> with 32k, I am getting `rpc error: code = DeadlineExceeded desc = context deadline exceeded%` 8k is working though, but the garbage is still there for me :/\r\n> \r\n> full traceback here:\r\n> \r\n> ```\r\n> Traceback (most recent call last):‚ñâ| 12.1G/12.1G [05:32<00:00, 40.6MB/s]\r\n>   File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main]\r\n>     return _run_code(code, main_globals, None,\r\n>   File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\r\n>     exec(code, run_globals)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/entrypoints/openai/api_server.py\", line 729, in <module>\r\n>     engine = AsyncLLMEngine.from_engine_args(engine_args)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 495, in from_engine_args\r\n>     engine = cls(parallel_config.worker_use_ray,\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 269, in __init__\r\n>     self.engine = self._init_engine(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 314, in _init_engine\r\n>     return engine_class(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 107, in __init__\r\n>     self._init_workers_ray(placement_group)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 194, in _init_workers_ray\r\n>     self._run_workers(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 750, in _run_workers\r\n>     self._run_workers_in_batch(workers, method, *args, **kwargs))\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/llm_engine.py\", line 727, in _run_workers_in_batch\r\n>     all_outputs = ray.get(all_outputs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/auto_init_hook.py\", line 24, in auto_init_wrapper\r\n>     return fn(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\r\n>     return func(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/worker.py\", line 2563, in get\r\n>     raise value.as_instanceof_cause()\r\n> ray.exceptions.RayTaskError(ChunkedEncodingError): ray::RayWorkerVllm.execute_method() (pid=7598, ip=192.168.25.172, actor_id=9a702c3d2722e46a47e1444101000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1781fa3d90>)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 833, in _raw_read\r\n>     raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\r\n> urllib3.exceptions.IncompleteRead: IncompleteRead(4963331015 bytes read, 7171139841 more expected)\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> ray::RayWorkerVllm.execute_method() (pid=7598, ip=192.168.25.172, actor_id=9a702c3d2722e46a47e1444101000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1781fa3d90>)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 816, in generate\r\n>     yield from self.raw.stream(chunk_size, decode_content=True)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 934, in stream\r\n>     data = self.read(amt=amt, decode_content=decode_content)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 905, in read\r\n>     data = self._raw_read(amt)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 811, in _raw_read\r\n>     with self._error_catcher():\r\n>   File \"/usr/lib/python3.10/contextlib.py\", line 153, in __exit__\r\n>     self.gen.throw(typ, value, traceback)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/urllib3/response.py\", line 729, in _error_catcher\r\n>     raise ProtocolError(f\"Connection broken: {e!r}\", e) from e\r\n> urllib3.exceptions.ProtocolError: ('Connection broken: IncompleteRead(4963331015 bytes read, 7171139841 more expected)', IncompleteRead(4963331015 bytes read, 7171139841 more expected))\r\n> \r\n> During handling of the above exception, another exception occurred:\r\n> \r\n> ray::RayWorkerVllm.execute_method() (pid=7598, ip=192.168.25.172, actor_id=9a702c3d2722e46a47e1444101000000, repr=<vllm.engine.ray_utils.RayWorkerVllm object at 0x7f1781fa3d90>)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/ray_utils.py\", line 32, in execute_method\r\n>     return executor(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 72, in load_model\r\n>     self.model_runner.load_model()\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 36, in load_model\r\n>     self.model = get_model(self.model_config)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader.py\", line 124, in get_model\r\n>     model.load_weights(model_config.model, model_config.download_dir,\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/mixtral.py\", line 519, in load_weights\r\n>     for name, loaded_weight in hf_model_weights_iterator(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/weight_utils.py\", line 201, in hf_model_weights_iterator\r\n>     hf_folder, hf_weights_files, use_safetensors = prepare_hf_model_weights(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/weight_utils.py\", line 141, in prepare_hf_model_weights\r\n>     hf_folder = snapshot_download(model_name_or_path,\r\n>   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n>     return fn(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_snapshot_download.py\", line 238, in snapshot_download\r\n>     thread_map(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\", line 69, in thread_map\r\n>     return _executor_map(ThreadPoolExecutor, fn, *iterables, **tqdm_kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/tqdm/contrib/concurrent.py\", line 51, in _executor_map\r\n>     return list(tqdm_class(ex.map(fn, *iterables, chunksize=chunksize), **kwargs))\r\n>   File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1170, in __iter__\r\n>     for obj in iterable:\r\n>   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\r\n>     yield _result_or_cancel(fs.pop())\r\n>   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\r\n>     return fut.result(timeout)\r\n>   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 451, in result\r\n>     return self.__get_result()\r\n>   File \"/usr/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\r\n>     raise self._exception\r\n>   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n>     result = self.fn(*self.args, **self.kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/_snapshot_download.py\", line 213, in _inner_hf_hub_download\r\n>     return hf_hub_download(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_validators.py\", line 118, in _inner_fn\r\n>     return fn(*args, **kwargs)\r\n>   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 1461, in hf_hub_download\r\n>     http_get(\r\n>   File \"/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py\", line 541, in http_get\r\n>     for chunk in r.iter_content(chunk_size=DOWNLOAD_CHUNK_SIZE):\r\n>   File \"/usr/local/lib/python3.10/dist-packages/requests/models.py\", line 818, in generate\r\n>     raise ChunkedEncodingError(e)\r\n> requests.exceptions.ChunkedEncodingError: ('Connection broken: IncompleteRead(4963331015 bytes read, 7171139841 more expected)', IncompleteRead(4963331015 bytes read, 7171139841 more expected))\r\n> consolidated.01.pt: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 12.1G/12.1G [05:32<00:00, 36.5MB/s]\r\n> rpc error: code = DeadlineExceeded desc = context deadline exceeded%\r\n> ```\r\n\r\nHello !\r\nI have the same error and I still don't know how to solve the problem. I'm using 2 A100 GPUs...",
        "created_at": "2023-12-16T03:04:39Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/3572": {
    "issue_number": 3572,
    "issue_url": "https://github.com/vllm-project/vllm/issues/3572",
    "title": "[Bug]: Should check whether stop sequence appears anyway in the output text instead of just endswith",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nUsing mistral-7b-instruct model, and prompt `Here is the English alphabet: ABC`, temperature 0, stop sequence `DE`, model would still output `DEFGHIJKLMNOPQRSTUVWXYZ...` this is because `DEF` is a token, and stop sequence is only checked with `endswith` https://github.com/vllm-project/vllm/blob/main/vllm/engine/llm_engine.py#L717 I think instead of we should check string contain.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-03-22T23:30:47Z",
    "closed_at": "2024-04-11T22:34:13Z",
    "author": "yunfeng-scale",
    "comments_count": 1,
    "comments": [
      {
        "author": "njhill",
        "body": "@yunfeng-scale yes this is definitely a bug, I'm working on a fix.",
        "created_at": "2024-03-22T23:35:50Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/3574": {
    "issue_number": 3574,
    "issue_url": "https://github.com/vllm-project/vllm/issues/3574",
    "title": "[Bug]: During streaming tokens should not be emitted if they could form a stop sequence",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nUsing mistral 7b, prompt `Here is the English alphabet: ABC`, temperature 0, stop sequence `DEFGHIJ`, during streaming, `DEF`, `G`, `HI` tokens are output (`J` triggers the stop). Expected behavior is no output. Tokens should be buffered when they could form prefix of any stop sequences, and sent out when that condition is cleared.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-03-22T23:54:38Z",
    "closed_at": "2024-04-11T22:34:13Z",
    "author": "yunfeng-scale",
    "comments_count": 1,
    "comments": [
      {
        "author": "njhill",
        "body": "@yunfeng-scale I'm currently also already working on a fix for this as part of the same set of changes mentioned in #3572.",
        "created_at": "2024-03-23T02:25:33Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/3593": {
    "issue_number": 3593,
    "issue_url": "https://github.com/vllm-project/vllm/issues/3593",
    "title": "ModuleNotFoundError: No module named 'transformers_modules' with API serving using phi-2b",
    "body": "### Your current environment\n\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: Could not collect\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1062.18.1.el7.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 2080 Ti\r\nNvidia driver version: 535.154.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          64\r\nOn-line CPU(s) list:             0-63\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz\r\nCPU family:                      6\r\nModel:                           85\r\nThread(s) per core:              2\r\nCore(s) per socket:              16\r\nSocket(s):                       2\r\nStepping:                        7\r\nCPU max MHz:                     3200.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        4200.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop\r\n_tsc aperfmperf eagerfpu pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch epb cat_l3 cdp_l3 invpcid_single intel_ppin intel_pt ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm mpx rdt_a avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req pku ospke avx512_vnni md_clear spec_ctrl intel_stibp flush_l1d arch_capabilitiesVirtualization:                  VT-x\r\nL1d cache:                       1 MiB (32 instances)\r\nL1i cache:                       1 MiB (32 instances)\r\nL2 cache:                        32 MiB (32 instances)\r\nL3 cache:                        44 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-15,32-47\r\nNUMA node1 CPU(s):               16-31,48-63\r\nVulnerability Itlb multihit:     KVM: Mitigation: Split huge pages\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; Load fences, usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB\r\nVulnerability Tsx async abort:   Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.3.3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t16-31,48-63\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\n\n### üêõ Describe the bug\n\nI tried to deploy an API serving using phi-2b over a ray cluster which runs on 2 docker container instances, but there is an error:\r\n\r\n```\r\n python3 -m vllm.entrypoints.openai.api_server --host 0.0.0.0 --port 8080 --trust-remote-code --model=/data/models/phi-2b/ --tensor-parallel-size 2\r\n```\r\n```\r\nERROR worker.py:406 -- Unhandled error (suppress with 'RAY_IGNORE_UNHANDLED_ERRORS=1'): ray::RayWorkerVllm.init_worker() (pid=2374, ip=10.161.12.10, actor_id=a70ba90bc9c7da25d5d0824301000000, repr=<vllm.engine.ray_utils.RayWorke\r\nrVllm object at 0x7f9243af2230>)  At least one of the input arguments for this task could not be computed:\r\nray.exceptions.RaySystemError: System error: No module named 'transformers_modules'\r\ntraceback: Traceback (most recent call last):\r\nModuleNotFoundError: No module named 'transformers_modules'\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10) No module named 'transformers_modules'\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10) Traceback (most recent call last):\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\", line 404, in deserialize_objects\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)     obj = self._deserialize_object(data, metadata, object_ref)\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\", line 270, in _deserialize_object\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)     return self._deserialize_msgpack_data(data, metadata_fields)\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\", line 225, in _deserialize_msgpack_data\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)     python_objects = self._deserialize_pickle5_data(pickle5_data)\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)   File \"/usr/local/lib/python3.10/dist-packages/ray/_private/serialization.py\", line 215, in _deserialize_pickle5_data\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10)     obj = pickle.loads(in_band)\r\n(RayWorkerVllm pid=2374, ip=10.161.12.10) ModuleNotFoundError: No module named 'transformers_modules'\r\n```\r\nIt looks similar to the issue [572] (https://github.com/vllm-project/vllm/issues/572) however, I still got this blocker.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-03-24T09:52:35Z",
    "closed_at": "2024-10-22T02:46:25Z",
    "author": "haining78zhang",
    "comments_count": 9,
    "comments": [
      {
        "author": "youkaichao",
        "body": "> I tried to deploy an API serving using phi-2b over a ray cluster which runs on 2 docker container instances, but there is an error:\r\n\r\nDo you mean you use 2 docker instance with 1 GPU for each? You can try to have one docker instance with 2 GPUs. Not sure if it works, but it is worth a try.",
        "created_at": "2024-03-24T14:35:23Z"
      },
      {
        "author": "haining78zhang",
        "body": "> > I tried to deploy an API serving using phi-2b over a ray cluster which runs on 2 docker container instances, but there is an error:\r\n> \r\n> Do you mean you use 2 docker instance with 1 GPU for each? You can try to have one docker instance with 2 GPUs. Not sure if it works, but it is worth a try.\r\n\r\nYes, 2 docker instances with 1 GPU for each, each docker instance runs on a physical node, so I got two physical servers in the same network, each server has one GPU and one docker instance (the image was built on top of the official image [vllm/vllm-openai](https://hub.docker.com/r/vllm/vllm-openai/tags)).  the ray cluster seems working well, 2 GPUs are shown in the status report. Due to the power limitation, I can't have two GPUs running on the same machine, so hard to try the 2-GPU solution. ",
        "created_at": "2024-03-25T00:53:14Z"
      },
      {
        "author": "youkaichao",
        "body": "Sorry I'm not familiar with ray cluster. Maybe @simon-mo can help.",
        "created_at": "2024-03-25T04:54:30Z"
      },
      {
        "author": "haining78zhang",
        "body": "> Sorry I'm not familiar with ray cluster. Maybe @simon-mo can help.\r\n\r\nperhase [mklf](https://github.com/mklf)",
        "created_at": "2024-03-25T09:27:24Z"
      },
      {
        "author": "haining78zhang",
        "body": "@mklf can Yijia also take a look? ",
        "created_at": "2024-03-26T03:10:59Z"
      },
      {
        "author": "Yang-x-Zhao",
        "body": "I am facing the same bug for qwen and baichuan model. \r\n\r\nI am also on 2 docker instances on 2 nodes (each with 2 gpus). I tried `tensor_parallel=2` and `tensor_parallel=4`.  When `tensor_parallel=2` (running on 1 node only), qwen and baichuan are working well. When `tensor_parallel=3` (running on 2 nodes), qwen and baichuan shows this error. \r\n\r\nHowever, in this environment, I can run llama correctly when `tensor_parallel=4` and `tensor_parallel=2`. ",
        "created_at": "2024-04-17T01:42:17Z"
      },
      {
        "author": "DefTruth",
        "body": "same error for me",
        "created_at": "2024-04-23T03:59:21Z"
      },
      {
        "author": "zhenfenxiao",
        "body": "I encountered same error when I tried to deploy finetuned qwen (local storage) on two nodes.  ",
        "created_at": "2024-05-09T07:59:27Z"
      },
      {
        "author": "baughmann",
        "body": "Having this issue myself with Phi-3-small when using the `AsyncLLMEngine` directly",
        "created_at": "2024-07-11T04:19:04Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/3707": {
    "issue_number": 3707,
    "issue_url": "https://github.com/vllm-project/vllm/issues/3707",
    "title": "[Bug]: logprobs=0 behavior inconsistent with OpenAI spec",
    "body": "### Your current environment\n\nvLLM 0.3.3\n\n### üêõ Describe the bug\n\nI'm using AsyncLlmEngine with vLLM 0.3.3 to serve OpenAI-compatible chat and completions endpoints. I noticed that there are some discrepancies between vLLM's and OpenAI's logprobs behavior. For example, providing logprobs=0 to SamplingParams causes `generate` to return `None` in its output logprobs, whereas OpenAI returns logprobs for the sampled tokens:\r\n\r\n```\r\nresponse = openai.Completion.create(\r\n      engine=\"gpt-3.5-turbo-instruct\",\r\n      prompt=\"Translate the following English text to French: 'Hello, world!'\",\r\n      temperature=0.7,\r\n      max_tokens=60,\r\n      logprobs=0\r\n    )\r\n...\r\n{\r\n  \"id\": \"cmpl-97v6aQwrliFerfnhQKh1NwnO43Uup\",\r\n  \"object\": \"text_completion\",\r\n  \"created\": 1711674836,\r\n  \"model\": \"gpt-3.5-turbo-instruct\",\r\n  \"choices\": [\r\n    {\r\n      \"text\": \"\\n\\nBonjour, monde !\",\r\n      \"index\": 0,\r\n      \"logprobs\": {\r\n        \"tokens\": [\r\n          \"\\n\\n\",\r\n          \"Bonjour\",\r\n          \",\",\r\n          \" monde\",\r\n          \" !\"\r\n        ],\r\n        \"token_logprobs\": [\r\n          -0.16840062,\r\n          -0.5103194,\r\n          -0.010852911,\r\n          -0.19059794,\r\n          -0.48192957\r\n        ],\r\n        \"top_logprobs\": null,\r\n        \"text_offset\": [\r\n          63,\r\n          65,\r\n          72,\r\n          73,\r\n          79\r\n        ]\r\n      },\r\n      \"finish_reason\": \"stop\"\r\n    }\r\n  ],\r\n  \"usage\": {\r\n    \"prompt_tokens\": 13,\r\n    \"completion_tokens\": 5,\r\n    \"total_tokens\": 18\r\n  }\r\n}\r\n```\r\n\r\nThe vLLM documentation https://docs.vllm.ai/en/latest/dev/sampling_params.html claims to also have this behavior but it does not.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-03-29T01:24:02Z",
    "closed_at": "2024-03-29T16:38:23Z",
    "author": "maddiedawson",
    "comments_count": 2,
    "comments": [
      {
        "author": "esmeetu",
        "body": "Hi @maddiedawson. Could you try to apply changes in #3731 ?",
        "created_at": "2024-03-29T13:19:12Z"
      },
      {
        "author": "maddiedawson",
        "body": "We have a workaround for now (set logprobs to 1) but we can remove it once this makes it into a vLLM release :)",
        "created_at": "2024-03-29T14:48:44Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/3793": {
    "issue_number": 3793,
    "issue_url": "https://github.com/vllm-project/vllm/issues/3793",
    "title": "[Bug]: RuntimeError: No suitable kernel. h_in=16 h_out=3424 dtype=Float out_dtype=BFloat16",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.1.2+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, Mar 21 2024, 16:24:04) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-100-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 3090\r\nGPU 1: NVIDIA GeForce RTX 3090\r\nGPU 2: NVIDIA GeForce RTX 3090\r\nGPU 3: NVIDIA GeForce RTX 3090\r\n\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             88\r\nOn-line CPU(s) list:                0-87\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) CPU E5-2696 v4 @ 2.20GHz\r\nCPU family:                         6\r\nModel:                              79\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 22\r\nSocket(s):                          2\r\nStepping:                           1\r\nCPU max MHz:                        3700.0000\r\nCPU min MHz:                        1200.0000\r\nBogoMIPS:                           4399.70\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 cdp_l3 invpcid_single pti intel_ppin ssbd ibrs ibpb stibp tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a rdseed adx smap intel_pt xsaveopt cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local dtherm ida arat pln pts md_clear flush_l1d\r\nVirtualization:                     VT-x\r\nL1d cache:                          1.4 MiB (44 instances)\r\nL1i cache:                          1.4 MiB (44 instances)\r\nL2 cache:                           11 MiB (44 instances)\r\nL3 cache:                           110 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-21,44-65\r\nNUMA node1 CPU(s):                  22-43,66-87\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX disabled\r\nVulnerability L1tf:                 Mitigation; PTE Inversion; VMX conditional cache flushes, SMT vulnerable\r\nVulnerability Mds:                  Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Mitigation; Clear CPU buffers; SMT vulnerable\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] torch==2.1.2\r\n[pip3] triton==2.1.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] torch                     2.1.2                    pypi_0    pypi\r\n[conda] triton                    2.1.0                    pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     SYS     SYS     0-21,44-65      0               N/A\r\nGPU1    PHB      X      SYS     SYS     0-21,44-65      0               N/A\r\nGPU2    SYS     SYS      X      PHB     22-43,66-87     1               N/A\r\nGPU3    SYS     SYS     PHB      X      22-43,66-87     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\n```shell\r\nCUDA_VISIBLE_DEVICES=0,1,2,3 python -m vllm.entrypoints.openai.api_server \\\r\n    --model /home/greatwall/app/edison/models/Qwen1.5-14B-Chat \\\r\n    --served-model-name FUX14B2 \\\r\n    --enable-lora \\\r\n    --lora-modules lora_1=/home/greatwall/app/edison/output/qwen1half-14b-chat/v25-20240330-131708/checkpoint-300 lora_2=/home/greatwall/app/edison/output/qwen1half-14b-chat/v25-20240330-131708/checkpoint-270 lora_3=/home/greatwall/app/edison/output/qwen1half-14b-chat/v25-20240330-131708/checkpoint-240 \\\r\n    --gpu-memory-utilization 1 \\\r\n    --tensor-parallel-size 4 \\\r\n    --host 0.0.0.0 \\\r\n    --port 8001\r\n```\r\n```triple quotes blocks\r\nINFO 04-02 09:47:24 api_server.py:148] vLLM API server version 0.4.0\r\nINFO 04-02 09:47:24 api_server.py:149] args: Namespace(host='0.0.0.0', port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name='FUX14B2', lora_modules=[LoRA(name='lora_1', local_path='/home/greatwall/app/edison/output/qwen1half-14b-chat/v25-20240330-131708/checkpoint-300'), LoRA(name='lora_2', local_path='/home/greatwall/app/edison/output/qwen1half-14b-chat/v25-20240330-131708/checkpoint-270'), LoRA(name='lora_3', local_path='/home/greatwall/app/edison/output/qwen1half-14b-chat/v25-20240330-131708/checkpoint-240')], chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/home/greatwall/app/edison/models/Qwen1.5-14B-Chat', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', max_model_len=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=4, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=1.0, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization=None, enforce_eager=False, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=True, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\n2024-04-02 09:47:26,188 INFO worker.py:1752 -- Started a local Ray instance.\r\nINFO 04-02 09:47:27 llm_engine.py:75] Initializing an LLM engine (v0.4.0) with config: model='/home/greatwall/app/edison/models/Qwen1.5-14B-Chat', tokenizer='/home/greatwall/app/edison/models/Qwen1.5-14B-Chat', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 04-02 09:47:45 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\r\nINFO 04-02 09:47:45 selector.py:25] Using XFormers backend.\r\n(RayWorkerVllm pid=43202) INFO 04-02 09:47:45 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance.\r\n(RayWorkerVllm pid=43202) INFO 04-02 09:47:45 selector.py:25] Using XFormers backend.\r\n(RayWorkerVllm pid=43317) INFO 04-02 09:47:46 pynccl_utils.py:45] vLLM is using nccl==2.18.1\r\nINFO 04-02 09:47:46 pynccl_utils.py:45] vLLM is using nccl==2.18.1\r\nWARNING 04-02 09:47:48 custom_all_reduce.py:45] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\n(RayWorkerVllm pid=43202) WARNING 04-02 09:47:48 custom_all_reduce.py:45] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\r\nINFO 04-02 09:47:54 model_runner.py:104] Loading model weights took 6.6485 GB\r\n(RayWorkerVllm pid=43202) INFO 04-02 09:47:56 model_runner.py:104] Loading model weights took 6.6485 GB\r\n(RayWorkerVllm pid=43395) INFO 04-02 09:47:45 selector.py:51] Cannot use FlashAttention because the package is not found. Please install it for better performance. [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\r\n(RayWorkerVllm pid=43395) INFO 04-02 09:47:45 selector.py:25] Using XFormers backend. [repeated 2x across cluster]\r\n(RayWorkerVllm pid=43202) INFO 04-02 09:47:46 pynccl_utils.py:45] vLLM is using nccl==2.18.1 [repeated 2x across cluster]\r\n(RayWorkerVllm pid=43395) WARNING 04-02 09:47:48 custom_all_reduce.py:45] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly. [repeated 2x across cluster]\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44] Error executing method profile_num_available_blocks. This might cause deadlock in distributed execution.\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44] Traceback (most recent call last):\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/engine/ray_utils.py\", line 37, in execute_method\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return executor(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return func(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/worker/worker.py\", line 131, in profile_num_available_blocks\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     self.model_runner.profile_run()\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return func(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/worker/model_runner.py\", line 742, in profile_run\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     self.execute_model(seqs, kv_caches)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return func(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/worker/model_runner.py\", line 663, in execute_model\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     hidden_states = model_executable(**execute_model_kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/model_executor/models/qwen2.py\", line 317, in forward\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     hidden_states = self.model(input_ids, positions, kv_caches,\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/model_executor/models/qwen2.py\", line 254, in forward\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     hidden_states, residual = layer(\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/model_executor/models/qwen2.py\", line 217, in forward\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     hidden_states = self.mlp(hidden_states)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/model_executor/models/qwen2.py\", line 76, in forward\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     gate_up, _ = self.gate_up_proj(x)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return self._call_impl(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/opt/conda/envs/vllm/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     return forward_call(*args, **kwargs)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/lora/layers.py\", line 395, in forward\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     output_parallel = self.apply_weights(input_, bias)\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/lora/layers.py\", line 509, in apply_weights\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     _apply_lora_packed_nslice(\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/lora/layers.py\", line 97, in _apply_lora_packed_nslice\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     add_lora_slice(output, x, lora_a_stacked[slice_idx],\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]   File \"/home/greatwall/app/edison/vllm/vllm/lora/punica.py\", line 160, in add_lora_slice\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44]     punica_kernels.dispatch_bgmv_low_level(\r\n(RayWorkerVllm pid=43395) ERROR 04-02 09:47:57 ray_utils.py:44] RuntimeError: No suitable kernel. h_in=16 h_out=3424 dtype=Float out_dtype=BFloat16\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-02T09:55:23Z",
    "closed_at": "2024-08-02T15:27:07Z",
    "author": "Edisonwei54",
    "comments_count": 35,
    "comments": [
      {
        "author": "Edisonwei54",
        "body": "@WoosukKwon How can I solve this problem",
        "created_at": "2024-04-02T09:58:20Z"
      },
      {
        "author": "jeejeelee",
        "body": "Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error",
        "created_at": "2024-04-02T10:23:18Z"
      },
      {
        "author": "Edisonwei54",
        "body": "> Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n\r\nThanks, It can work now, but I still want to use all gpu, because the memory is not enough...",
        "created_at": "2024-04-03T02:35:11Z"
      },
      {
        "author": "nlp-learner",
        "body": "\r\nÊÇ®Â•ΩÔºåÊàëÂú®Âä†ËΩΩbaichuan2-13bÊó∂ÂÄôÈÅáÂà∞Áõ∏‰ººÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4ÁâàÊú¨ÂùáÂ≠òÂú®ÔºåRuntimeError: No suitable kernel. h_in=32 h_out=15360 dtype=Float out_dtype=BFloat16",
        "created_at": "2024-04-10T10:06:18Z"
      },
      {
        "author": "jeejeelee",
        "body": "> ÊÇ®Â•ΩÔºåÊàëÂú®Âä†ËΩΩbaichuan2-13bÊó∂ÂÄôÈÅáÂà∞Áõ∏‰ººÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4ÁâàÊú¨ÂùáÂ≠òÂú®ÔºåRuntimeError: No suitable kernel. h_in=32 h_out=15360 dtype=Float out_dtype=BFloat16\r\n\r\nÂΩìÂâçÁöÑvllmÁâàÊú¨‰∏≠ÔºåpunicaÁöÑÁÆóÂ≠ê‰∏çÊîØÊåÅ15360ÔºåÊàë‰πãÂâçÁöÑPRÊ≤°ÊúâÊ≥®ÊÑèÂà∞ËøôÁÇπÔºå‰∏çÂ•ΩÊÑèÊÄù„ÄÇ\r\n ÊÇ®ÂèØ‰ª•Âú® https://github.com/vllm-project/vllm/blob/main/csrc/punica/bgmv/bgmv_config.h#L48 Ê∑ªÂä† \r\n```C++\r\nf(in_T, out_T, W_T, narrow, 15360) \\\r\n```\r\nÁÑ∂ÂêéÈáçÊñ∞ÁºñËØëvllm(0.4.0ÁöÑÁâàÊú¨)\r\nÂ¶ÇÊûúÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÁöÑËØùÔºåÊÇ®‰πüÂèØ‰ª•Êèê‰∏™PRÊù•Ëß£ÂÜ≥Ëøô‰∏™BUG\r\n",
        "created_at": "2024-04-10T15:00:39Z"
      },
      {
        "author": "nlp-learner",
        "body": "ÊòØÁöÑÔºåÊàë‰πüÊòØËøôÊ†∑Ê∑ªÂä†ÁöÑÔºåÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4‰∏≠ÂùáÊ≤°ÊúâÈóÆÈ¢ò\r\n",
        "created_at": "2024-04-11T01:55:30Z"
      },
      {
        "author": "jeejeelee",
        "body": "> ÊòØÁöÑÔºåÊàë‰πüÊòØËøôÊ†∑Ê∑ªÂä†ÁöÑÔºåÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4‰∏≠ÂùáÊ≤°ÊúâÈóÆÈ¢ò\r\n\r\nhiÔºåÊÇ®ÂèØ‰ª•Êèê‰∏™PRËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÂêó",
        "created_at": "2024-04-11T02:17:36Z"
      },
      {
        "author": "nlp-learner",
        "body": "ÊàëÁúãÊÇ®Êúâ‰∏çÊñ≠Êèê‰∫§prÔºåÊÇ®ÂèØ‰ª•Â∏ÆÂøôÂú®‰∏ãÊ¨°Êèê‰∫§prÊääËøôÈÉ®ÂàÜÂêàÂπ∂‰∏äÂéªÔºåÊàëÂ∞±‰∏ç‰∏ìÈó®Êèêpr‰∫ÜÔºÅÂè¶Â§ñÈóÆ‰∏ãÔºåÂú®0.4ÁâàÊú¨‰∏éÊÇ®‰πãÂâçÊèê‰∫§qkvÁöÑprÂêàÂπ∂‰∫Ü‰πãÂ§ñÔºåËøòÂÅö‰∫ÜÂì™‰∫õÊîπÂä®ÔºåÊØîÂ¶ÇÊàëÁúãÂà∞‰∏ãÈù¢ËøôÈÉ®ÂàÜÔºå‰ª•‰æøÊàëÈÄâÊã©ÊòØÂê¶ÈúÄË¶ÅÊõ¥Êñ∞Âà∞0.4ÁâàÊú¨ÔºåÂõ†‰∏∫ÊàëÂü∫‰∫é0.3.3ÈáçÂÜô‰∫ÜModelRunner,workerÂà∞LLMEngineÈÉ®ÂàÜ\r\n`    @classmethod\r\n    def can_replace_layer(cls, source_layer: nn.Module,\r\n                          lora_config: LoRAConfig, packed_modules_list: List,\r\n                          model_config: Optional[PretrainedConfig]) -> bool:`",
        "created_at": "2024-04-11T02:26:14Z"
      },
      {
        "author": "jeejeelee",
        "body": "@nlp-learner  Â•ΩÁöÑ„ÄÇ\r\nÊ≠§Â§ñÔºåÊü•ÁúãÁâàÊú¨Èó¥ÁöÑÊîπÂä®ÂèäÂ∑ÆÂºÇÔºåÂèØ‰ª•Âú®dÂØπÂ∫î‰ªìÂ∫ìÁΩëÂùÄÂêéÊ∑ªÂä†`compare`ÂÅöÊØîËæÉ: https://github.com/vllm-project/vllm/compare",
        "created_at": "2024-04-11T05:13:04Z"
      },
      {
        "author": "kingljl",
        "body": "@jeejeelee Âì•ÔºåÊàëÈÅáÂà∞‰∫Ü‰∏Ä‰∏™bug„ÄÇ\r\n![image](https://github.com/vllm-project/vllm/assets/5028386/597a2760-fba2-4c5c-b4c1-5556572dad85)\r\n‰∏çÁü•ÈÅìÊÄé‰πàËß£ÂÜ≥‰∫Ü\r\nÁéØÂ¢ÉÔºö\r\nA10ÁöÑGPUÂç°\r\nnvcc: NVIDIA (R) Cuda compiler driver\r\nCopyright (c) 2005-2023 NVIDIA Corporation\r\nBuilt on Mon_Apr__3_17:16:06_PDT_2023\r\nCuda compilation tools, release 12.1, V12.1.105\r\nBuild cuda_12.1.r12.1/compiler.32688072_0",
        "created_at": "2024-04-11T09:25:27Z"
      },
      {
        "author": "kingljl",
        "body": "> @jeejeelee Âì•ÔºåÊàëÈÅáÂà∞‰∫Ü‰∏Ä‰∏™bug„ÄÇ ![image](https://private-user-images.githubusercontent.com/5028386/321577982-597a2760-fba2-4c5c-b4c1-5556572dad85.png?jwt=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3MTI4MzY0MTcsIm5iZiI6MTcxMjgzNjExNywicGF0aCI6Ii81MDI4Mzg2LzMyMTU3Nzk4Mi01OTdhMjc2MC1mYmEyLTRjNWMtYjRjMS01NTU2NTcyZGFkODUucG5nP1gtQW16LUFsZ29yaXRobT1BV1M0LUhNQUMtU0hBMjU2JlgtQW16LUNyZWRlbnRpYWw9QUtJQVZDT0RZTFNBNTNQUUs0WkElMkYyMDI0MDQxMSUyRnVzLWVhc3QtMSUyRnMzJTJGYXdzNF9yZXF1ZXN0JlgtQW16LURhdGU9MjAyNDA0MTFUMTE0ODM3WiZYLUFtei1FeHBpcmVzPTMwMCZYLUFtei1TaWduYXR1cmU9ZmQxMmE2NDcwYjhjZTU0NzI4MjJiYjI2NDAzNmU3NjVlZjU4MmJiODIxNDkwMTgxNmNmNWU4YTkzYzE2YzVkYyZYLUFtei1TaWduZWRIZWFkZXJzPWhvc3QmYWN0b3JfaWQ9MCZrZXlfaWQ9MCZyZXBvX2lkPTAifQ.vNOg_8xkGn3-HeftCmec-MqMmbYOo84zpX37WjV-FgA) ‰∏çÁü•ÈÅìÊÄé‰πàËß£ÂÜ≥‰∫Ü ÁéØÂ¢ÉÔºö A10ÁöÑGPUÂç° nvcc: NVIDIA (R) Cuda compiler driver Copyright (c) 2005-2023 NVIDIA Corporation Built on Mon_Apr__3_17:16:06_PDT_2023 Cuda compilation tools, release 12.1, V12.1.105 Build cuda_12.1.r12.1/compiler.32688072_0\r\n\r\n@jeejeelee ÊêûÂÆö‰∫ÜÔºåÊàëÂ¢ûÂä†‰∫Ü640ÁöÑÁÆóÂ≠ê",
        "created_at": "2024-04-11T11:56:03Z"
      },
      {
        "author": "AJAXLONG",
        "body": "ÊàëÂú®chinese-alpaca-llama2-7BÈÅáÂà∞Ê≠§ÈóÆÈ¢òÔºåÊòØÂê¶Êúâ‰∏çÈáçÊñ∞ÁºñËØëÁöÑÊñπÊ≥ï",
        "created_at": "2024-05-07T01:41:20Z"
      },
      {
        "author": "Xingxiangrui",
        "body": "h_in=16 h_out=3424\r\n+1 for this .   Qwen-14B with lora rank=16.   \r\nvllm  0.4.2 version not fixed.",
        "created_at": "2024-05-15T06:32:03Z"
      },
      {
        "author": "heurainbow",
        "body": "> Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n\r\n@jeejeelee can you support this on 8 gpus?",
        "created_at": "2024-05-16T11:09:57Z"
      },
      {
        "author": "AJAXLONG",
        "body": "@jeejeelee \r\nËÉΩÂú®‰∏ã‰∏Ä‰∏™ÁâàÊú¨‰∏≠ÔºåÈÄÇÈÖç‰∏Ä‰∏ãÁÆóÂ≠êÔºü\r\nNo suitable kernel.h_in=16 h_out=76032 dtype=Float out _dtype=Half\r\nNo suitable kernel. h_in=16 h_out=55552 dtype=Float out_dtype=BFloat16\r\nf(in_T, out_T, W_T, narrow, 55552) \\\r\nf(in_T, out_T, W_T, narrow, 76032 ) \\\r\n\r\n\r\n",
        "created_at": "2024-05-21T08:37:04Z"
      },
      {
        "author": "jeejeelee",
        "body": "> @jeejeelee ËÉΩÂú®‰∏ã‰∏Ä‰∏™ÁâàÊú¨‰∏≠ÔºåÈÄÇÈÖç‰∏Ä‰∏ãÁÆóÂ≠êÔºü No suitable kernel.h_in=16 h_out=76032 dtype=Float out _dtype=Half No suitable kernel. h_in=16 h_out=55552 dtype=Float out_dtype=BFloat16 f(in_T, out_T, W_T, narrow, 55552) f(in_T, out_T, W_T, narrow, 76032 ) \\\r\n\r\n\r\nËøô‰∏™ÊòØ‰ªÄ‰πàÊ®°ÂûãÁöÑÂ∞∫ÂØ∏Âë¢ÔºåÊ≠§Â§ñ‰Ω†ÂèØ‰ª•Ëá™Â∑±Êèê‰∏™PRÊù•Ëß£ÂÜ≥Ëøô‰∏™size",
        "created_at": "2024-05-21T09:29:30Z"
      },
      {
        "author": "jeejeelee",
        "body": "> > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n> \r\n> @jeejeelee can you support this on 8 gpus?\r\n\r\nI'm trying to solve this issue",
        "created_at": "2024-05-21T09:30:16Z"
      },
      {
        "author": "AJAXLONG",
        "body": "@jeejeelee \r\nÂì•Ê®°ÂûãÁöÑÂ∞∫ÂØ∏ÊòØ:\r\n(in_T, out_T, W_T, narrow, 55552) \\  ÊòØchinese-alpaca-llama2-7b\r\nf(in_T, out_T, W_T, narrow, 76032 ) \\ ÊòØËá™È¢ÑËÆ≠ÁªÉÁöÑ34BÊ®°ÂûãÔºå\r\nËÉΩÂê¶Â∏ÆÂøôÈÄÇÈÖçÂë¢ÔºåÂ¶ÇÊûúËá™Â∑±ÊèêPRÔºåÈúÄË¶ÅÂú®Âì™‰∏Ä‰∏™ÂºÄÂèëÂàÜÊîØ‰∏ä‰øÆÊîπ",
        "created_at": "2024-05-21T12:35:30Z"
      },
      {
        "author": "jeejeelee",
        "body": "> ËÉΩÂê¶Â∏ÆÂøôÈÄÇÈÖçÂë¢ÔºåÂ¶ÇÊûúËá™Â∑±ÊèêPRÔºåÈúÄË¶ÅÂú®Âì™‰∏Ä‰∏™ÂºÄÂèëÂàÜÊîØ‰∏ä‰øÆÊîπ\r\n\r\nÂèØ‰ª•ÂéªÊü•‰∏ãÂ¶Ç‰ΩïÂêëgithubÁöÑÂ∑•Á®ãÊèê‰∫§PR",
        "created_at": "2024-05-21T13:28:39Z"
      },
      {
        "author": "ghost",
        "body": "@Edisonwei54 Áúã‰∫Ü‰Ω†ÁöÑ‰ª£Á†ÅÊòØË∑ëÁöÑqwen1.5-14b+2‰∏™loraÔºåÂèØ‰ª•ËØ∑ÈóÆÊúâÂá∫Áé∞cannot access local variable 'lora_b_k' where it is not associated with a value Ëøô‰∏™ÈîôËØØÂêó",
        "created_at": "2024-05-22T07:46:06Z"
      },
      {
        "author": "1149722739",
        "body": "> > > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n> > \r\n> > \r\n> > @jeejeelee can you support this on 8 gpus?\r\n> \r\n> I'm trying to solve this issue\r\n\r\nËØ∑ÈóÆËß£ÂÜ≥‰∫ÜÂêóÔºü",
        "created_at": "2024-05-31T07:11:42Z"
      },
      {
        "author": "jeejeelee",
        "body": "> > > > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n> > > \r\n> > > \r\n> > > @jeejeelee can you support this on 8 gpus?\r\n> > \r\n> > \r\n> > I'm trying to solve this issue\r\n> \r\n> ËØ∑ÈóÆËß£ÂÜ≥‰∫ÜÂêóÔºü\r\n\r\nÊÇ®Â•ΩÔºåÁõÆÂâçÊàëÂ∑≤Áªè‰∫ÜÁõ∏ÂÖ≥ÁöÑPRÔºå ÂèÇËÄÉÔºöhttps://github.com/vllm-project/vllm/pull/5036  ‰ΩÜÊòØÁõÆÂâçËøòÂ§Ñ‰∫éÂºÄÂèë‰∏≠",
        "created_at": "2024-05-31T09:07:25Z"
      },
      {
        "author": "1149722739",
        "body": "> > > > > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n> > > > \r\n> > > > \r\n> > > > @jeejeelee can you support this on 8 gpus?\r\n> > > \r\n> > > \r\n> > > I'm trying to solve this issue\r\n> > \r\n> > \r\n> > ËØ∑ÈóÆËß£ÂÜ≥‰∫ÜÂêóÔºü\r\n> \r\n> ÊÇ®Â•ΩÔºåÁõÆÂâçÊàëÂ∑≤Áªè‰∫ÜÁõ∏ÂÖ≥ÁöÑPRÔºå ÂèÇËÄÉÔºö#5036 ‰ΩÜÊòØÁõÆÂâçËøòÂ§Ñ‰∫éÂºÄÂèë‰∏≠\r\n\r\nÂ•ΩÁöÑ",
        "created_at": "2024-05-31T12:34:03Z"
      },
      {
        "author": "zhanghanweii",
        "body": "> > ÊÇ®Â•ΩÔºåÊàëÂú®Âä†ËΩΩbaichuan2-13bÊó∂ÂÄôÈÅáÂà∞Áõ∏‰ººÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4ÁâàÊú¨ÂùáÂ≠òÂú®ÔºåRuntimeError: No suitable kernel. h_in=32 h_out=15360 dtype=Float out_dtype=BFloat16\r\n> \r\n> ÂΩìÂâçÁöÑvllmÁâàÊú¨‰∏≠ÔºåpunicaÁöÑÁÆóÂ≠ê‰∏çÊîØÊåÅ15360ÔºåÊàë‰πãÂâçÁöÑPRÊ≤°ÊúâÊ≥®ÊÑèÂà∞ËøôÁÇπÔºå‰∏çÂ•ΩÊÑèÊÄù„ÄÇ ÊÇ®ÂèØ‰ª•Âú® https://github.com/vllm-project/vllm/blob/main/csrc/punica/bgmv/bgmv_config.h#L48 Ê∑ªÂä†\r\n> \r\n> ```c++\r\n> f(in_T, out_T, W_T, narrow, 15360) \\\r\n> ```\r\n> \r\n> ÁÑ∂ÂêéÈáçÊñ∞ÁºñËØëvllm(0.4.0ÁöÑÁâàÊú¨) Â¶ÇÊûúÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÁöÑËØùÔºåÊÇ®‰πüÂèØ‰ª•Êèê‰∏™PRÊù•Ëß£ÂÜ≥Ëøô‰∏™BUG\r\n\r\n‰Ω†Â•ΩÔºåÊàëÊòØ‰ΩøÁî®pip install vllmÂÆâË£ÖÁöÑvllmÂåÖÔºå‰Ω†ËØ¥ÁöÑvllm/blob/main/csrc/punica/bgmv/bgmv_config.hÊ≤°ÊúâÊâæÂà∞Âú®Âì™",
        "created_at": "2024-06-05T06:43:13Z"
      },
      {
        "author": "jeejeelee",
        "body": "> > > ÊÇ®Â•ΩÔºåÊàëÂú®Âä†ËΩΩbaichuan2-13bÊó∂ÂÄôÈÅáÂà∞Áõ∏‰ººÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4ÁâàÊú¨ÂùáÂ≠òÂú®ÔºåRuntimeError: No suitable kernel. h_in=32 h_out=15360 dtype=Float out_dtype=BFloat16\r\n> > \r\n> > \r\n> > ÂΩìÂâçÁöÑvllmÁâàÊú¨‰∏≠ÔºåpunicaÁöÑÁÆóÂ≠ê‰∏çÊîØÊåÅ15360ÔºåÊàë‰πãÂâçÁöÑPRÊ≤°ÊúâÊ≥®ÊÑèÂà∞ËøôÁÇπÔºå‰∏çÂ•ΩÊÑèÊÄù„ÄÇ ÊÇ®ÂèØ‰ª•Âú® https://github.com/vllm-project/vllm/blob/main/csrc/punica/bgmv/bgmv_config.h#L48 Ê∑ªÂä†\r\n> > ```c++\r\n> > f(in_T, out_T, W_T, narrow, 15360) \\\r\n> > ```\r\n> > \r\n> > \r\n> >     \r\n> >       \r\n> >     \r\n> > \r\n> >       \r\n> >     \r\n> > \r\n> >     \r\n> >   \r\n> > ÁÑ∂ÂêéÈáçÊñ∞ÁºñËØëvllm(0.4.0ÁöÑÁâàÊú¨) Â¶ÇÊûúÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÁöÑËØùÔºåÊÇ®‰πüÂèØ‰ª•Êèê‰∏™PRÊù•Ëß£ÂÜ≥Ëøô‰∏™BUG\r\n> \r\n> ‰Ω†Â•ΩÔºåÊàëÊòØ‰ΩøÁî®pip install vllmÂÆâË£ÖÁöÑvllmÂåÖÔºå‰Ω†ËØ¥ÁöÑvllm/blob/main/csrc/punica/bgmv/bgmv_config.hÊ≤°ÊúâÊâæÂà∞Âú®Âì™\r\n\r\n‰Ω†Â•ΩÔºåclone‰∏ãÊ∫êÁ†ÅÔºåÂç≥ÂèØÊâæÂà∞ÔºåÂú®vllm/csrc/punica/bgmv/bgmv_config.h‰∏≠\r\nËã•ÊòØ‰Ω†ÊòØÈíàÂØπ15360ÁöÑÈóÆÈ¢òÔºåÂÆâË£ÖÊúÄÊñ∞ÁâàÊú¨Âç≥ÂèØÔºåÂ∑≤ÁªèËß£ÂÜ≥‰∫Ü",
        "created_at": "2024-06-05T07:57:09Z"
      },
      {
        "author": "liangxiao777",
        "body": "ÊÇ®Â•ΩÔºåÊàëÂú®ÂæÆË∞ÉQwen2-7BÂêéÈÉ®ÁΩ≤Êó∂ÈÅáÂà∞‰∫ÜÂêåÊ†∑ÁöÑÈóÆÈ¢òÔºö\r\n`[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 168, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 366, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 324, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 442, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 172, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 249, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 106, in determine_num_available_blocks\r\n[rank0]:     return self.driver_worker.determine_num_available_blocks()\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 139, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 888, in profile_run\r\n[rank0]:     self.execute_model(seqs, kv_caches)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 808, in execute_model\r\n[rank0]:     hidden_states = model_executable(**execute_model_kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 316, in forward\r\n[rank0]:     hidden_states = self.model(input_ids, positions, kv_caches,\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 253, in forward\r\n[rank0]:     hidden_states, residual = layer(\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 216, in forward\r\n[rank0]:     hidden_states = self.mlp(hidden_states)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 75, in forward\r\n[rank0]:     gate_up, _ = self.gate_up_proj(x)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n[rank0]:     return self._call_impl(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n[rank0]:     return forward_call(*args, **kwargs)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/lora/layers.py\", line 461, in forward\r\n[rank0]:     output_parallel = self.apply(input_, bias)\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/lora/layers.py\", line 585, in apply\r\n[rank0]:     _apply_lora_packed_nslice(\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/lora/layers.py\", line 127, in _apply_lora_packed_nslice\r\n[rank0]:     add_lora_slice(output, x, lora_a_stacked[slice_idx],\r\n[rank0]:   File \"/usr/local/lib/python3.10/site-packages/vllm/lora/punica.py\", line 203, in add_lora_slice\r\n[rank0]:     punica_kernels.dispatch_bgmv_low_level(\r\n[rank0]: RuntimeError: No suitable kernel. h_in=16 h_out=18944 dtype=Float out_dtype=BFloat16`\r\n\r\n> > > > ÊÇ®Â•ΩÔºåÊàëÂú®Âä†ËΩΩbaichuan2-13bÊó∂ÂÄôÈÅáÂà∞Áõ∏‰ººÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4ÁâàÊú¨ÂùáÂ≠òÂú®ÔºåRuntimeError: No suitable kernel. h_in=32 h_out=15360 dtype=Float out_dtype=BFloat16\r\n> > > \r\n> > > \r\n> > > ÂΩìÂâçÁöÑvllmÁâàÊú¨‰∏≠ÔºåpunicaÁöÑÁÆóÂ≠ê‰∏çÊîØÊåÅ15360ÔºåÊàë‰πãÂâçÁöÑPRÊ≤°ÊúâÊ≥®ÊÑèÂà∞ËøôÁÇπÔºå‰∏çÂ•ΩÊÑèÊÄù„ÄÇ ÊÇ®ÂèØ‰ª•Âú® https://github.com/vllm-project/vllm/blob/main/csrc/punica/bgmv/bgmv_config.h#L48 Ê∑ªÂä†\r\n> > > ```c++\r\n> > > f(in_T, out_T, W_T, narrow, 15360) \\\r\n> > > ```\r\n> > > \r\n> > > \r\n> > >     \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >       \r\n> > >     \r\n> > > \r\n> > >     \r\n> > >   \r\n> > > ÁÑ∂ÂêéÈáçÊñ∞ÁºñËØëvllm(0.4.0ÁöÑÁâàÊú¨) Â¶ÇÊûúÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÁöÑËØùÔºåÊÇ®‰πüÂèØ‰ª•Êèê‰∏™PRÊù•Ëß£ÂÜ≥Ëøô‰∏™BUG\r\n> > \r\n> > \r\n> > ‰Ω†Â•ΩÔºåÊàëÊòØ‰ΩøÁî®pip install vllmÂÆâË£ÖÁöÑvllmÂåÖÔºå‰Ω†ËØ¥ÁöÑvllm/blob/main/csrc/punica/bgmv/bgmv_config.hÊ≤°ÊúâÊâæÂà∞Âú®Âì™\r\n> \r\n> ‰Ω†Â•ΩÔºåclone‰∏ãÊ∫êÁ†ÅÔºåÂç≥ÂèØÊâæÂà∞ÔºåÂú®vllm/csrc/punica/bgmv/bgmv_config.h‰∏≠ Ëã•ÊòØ‰Ω†ÊòØÈíàÂØπ15360ÁöÑÈóÆÈ¢òÔºåÂÆâË£ÖÊúÄÊñ∞ÁâàÊú¨Âç≥ÂèØÔºåÂ∑≤ÁªèËß£ÂÜ≥‰∫Ü\r\n\r\n",
        "created_at": "2024-06-12T07:26:11Z"
      },
      {
        "author": "jeejeelee",
        "body": "> ÊÇ®Â•ΩÔºåÊàëÂú®ÂæÆË∞ÉQwen2-7BÂêéÈÉ®ÁΩ≤Êó∂ÈÅáÂà∞‰∫ÜÂêåÊ†∑ÁöÑÈóÆÈ¢òÔºö `[rank0]: Traceback (most recent call last): [rank0]: File \"/usr/local/lib/python3.10/runpy.py\", line 196, in _run_module_as_main [rank0]: return _run_code(code, main_globals, None, [rank0]: File \"/usr/local/lib/python3.10/runpy.py\", line 86, in _run_code [rank0]: exec(code, run_globals) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 168, in <module> [rank0]: engine = AsyncLLMEngine.from_engine_args( [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 366, in from_engine_args [rank0]: engine = cls( [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 324, in __init__ [rank0]: self.engine = self._init_engine(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 442, in _init_engine [rank0]: return engine_class(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 172, in __init__ [rank0]: self._initialize_kv_caches() [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 249, in _initialize_kv_caches [rank0]: self.model_executor.determine_num_available_blocks()) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 106, in determine_num_available_blocks [rank0]: return self.driver_worker.determine_num_available_blocks() [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context [rank0]: return func(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/worker/worker.py\", line 139, in determine_num_available_blocks [rank0]: self.model_runner.profile_run() [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context [rank0]: return func(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 888, in profile_run [rank0]: self.execute_model(seqs, kv_caches) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context [rank0]: return func(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 808, in execute_model [rank0]: hidden_states = model_executable(**execute_model_kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 316, in forward [rank0]: hidden_states = self.model(input_ids, positions, kv_caches, [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 253, in forward [rank0]: hidden_states, residual = layer( [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 216, in forward [rank0]: hidden_states = self.mlp(hidden_states) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/model_executor/models/qwen2.py\", line 75, in forward [rank0]: gate_up, _ = self.gate_up_proj(x) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl [rank0]: return self._call_impl(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl [rank0]: return forward_call(*args, **kwargs) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/lora/layers.py\", line 461, in forward [rank0]: output_parallel = self.apply(input_, bias) [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/lora/layers.py\", line 585, in apply [rank0]: _apply_lora_packed_nslice( [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/lora/layers.py\", line 127, in _apply_lora_packed_nslice [rank0]: add_lora_slice(output, x, lora_a_stacked[slice_idx], [rank0]: File \"/usr/local/lib/python3.10/site-packages/vllm/lora/punica.py\", line 203, in add_lora_slice [rank0]: punica_kernels.dispatch_bgmv_low_level( [rank0]: RuntimeError: No suitable kernel. h_in=16 h_out=18944 dtype=Float out_dtype=BFloat16`\r\n> \r\n> > > > > ÊÇ®Â•ΩÔºåÊàëÂú®Âä†ËΩΩbaichuan2-13bÊó∂ÂÄôÈÅáÂà∞Áõ∏‰ººÈóÆÈ¢òÔºåÂú®0.3.3‰∏é0.4ÁâàÊú¨ÂùáÂ≠òÂú®ÔºåRuntimeError: No suitable kernel. h_in=32 h_out=15360 dtype=Float out_dtype=BFloat16\r\n> > > > \r\n> > > > \r\n> > > > ÂΩìÂâçÁöÑvllmÁâàÊú¨‰∏≠ÔºåpunicaÁöÑÁÆóÂ≠ê‰∏çÊîØÊåÅ15360ÔºåÊàë‰πãÂâçÁöÑPRÊ≤°ÊúâÊ≥®ÊÑèÂà∞ËøôÁÇπÔºå‰∏çÂ•ΩÊÑèÊÄù„ÄÇ ÊÇ®ÂèØ‰ª•Âú® https://github.com/vllm-project/vllm/blob/main/csrc/punica/bgmv/bgmv_config.h#L48 Ê∑ªÂä†\r\n> > > > ```c++\r\n> > > > f(in_T, out_T, W_T, narrow, 15360) \\\r\n> > > > ```\r\n> > > > \r\n> > > > \r\n> > > >     \r\n> > > >       \r\n> > > >     \r\n> > > > \r\n> > > >       \r\n> > > >     \r\n> > > > \r\n> > > >     \r\n> > > >   \r\n> > > > ÁÑ∂ÂêéÈáçÊñ∞ÁºñËØëvllm(0.4.0ÁöÑÁâàÊú¨) Â¶ÇÊûúÊµãËØïÊ≤°ÊúâÈóÆÈ¢òÁöÑËØùÔºåÊÇ®‰πüÂèØ‰ª•Êèê‰∏™PRÊù•Ëß£ÂÜ≥Ëøô‰∏™BUG\r\n> > > \r\n> > > \r\n> > > ‰Ω†Â•ΩÔºåÊàëÊòØ‰ΩøÁî®pip install vllmÂÆâË£ÖÁöÑvllmÂåÖÔºå‰Ω†ËØ¥ÁöÑvllm/blob/main/csrc/punica/bgmv/bgmv_config.hÊ≤°ÊúâÊâæÂà∞Âú®Âì™\r\n> > \r\n> > \r\n> > ‰Ω†Â•ΩÔºåclone‰∏ãÊ∫êÁ†ÅÔºåÂç≥ÂèØÊâæÂà∞ÔºåÂú®vllm/csrc/punica/bgmv/bgmv_config.h‰∏≠ Ëã•ÊòØ‰Ω†ÊòØÈíàÂØπ15360ÁöÑÈóÆÈ¢òÔºåÂÆâË£ÖÊúÄÊñ∞ÁâàÊú¨Âç≥ÂèØÔºåÂ∑≤ÁªèËß£ÂÜ≥‰∫Ü\r\n\r\n18944ÁöÑÂ∞∫ÂØ∏‰∏çÊîØÊåÅ,ÂèØ‰ª•ÂèÇËÄÉ15360ÁöÑËß£ÂÜ≥ÊñπÂºèËß£ÂÜ≥",
        "created_at": "2024-06-12T07:41:18Z"
      },
      {
        "author": "jeejeelee",
        "body": "@liangxiao777  FYI https://github.com/vllm-project/vllm/pull/5441",
        "created_at": "2024-06-12T07:51:09Z"
      },
      {
        "author": "NiuBlibing",
        "body": "Same error with Qwen-72B-Instruct lora:\r\n```\r\nRuntimeError: No suitable kernel. h_in=16 h_out=3696 dtype=Float out_dtype=BFloat16\r\n```",
        "created_at": "2024-06-13T09:45:04Z"
      },
      {
        "author": "gtpgg1013",
        "body": "\r\n\r\nHi, Thanks for your help. I encountered the similar situation while fine tuning.\r\nMy error :\r\nRuntimeError: No suitable kernel. h_in=16 h_out=19200 dtype=Float out_dtype=BFloat16\r\n\r\nCould you say how can I treat this? And how '-tensor-parallel-size' option can avoid this error?\r\nThanks a lot in advance!\r\n\r\n> Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n\r\n",
        "created_at": "2024-06-20T06:07:21Z"
      },
      {
        "author": "jeejeelee",
        "body": "> Hi, Thanks for your help. I encountered the similar situation while fine tuning. My error : RuntimeError: No suitable kernel. h_in=16 h_out=19200 dtype=Float out_dtype=BFloat16\r\n> \r\n> Could you say how can I treat this? And how '-tensor-parallel-size' option can avoid this error? Thanks a lot in advance!\r\n> \r\n> > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n\r\nHiÔºåalthough the error messages are similar, your situation are not the same. You can refer to https://github.com/vllm-project/vllm/pull/5441 to add support for `19200`.\r\n\r\n3424 is not divisible by 64, so the Punica kernel can't process h_out=3424. However, 19200 is divisible by 64 and can be processed.",
        "created_at": "2024-06-20T06:34:11Z"
      },
      {
        "author": "gtpgg1013",
        "body": "> > Hi, Thanks for your help. I encountered the similar situation while fine tuning. My error : RuntimeError: No suitable kernel. h_in=16 h_out=19200 dtype=Float out_dtype=BFloat16\r\n> > Could you say how can I treat this? And how '-tensor-parallel-size' option can avoid this error? Thanks a lot in advance!\r\n> > > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n> \r\n> HiÔºåalthough the error messages are similar, your situation are not the same. You can refer to #5441 to add support for `19200`.\r\n> \r\n> 3424 is not divisible by 64, so the Punica kernel can't process h_out=3424. However, 19200 is divisible by 64 and can be processed.\r\n\r\nThanks for your help! You mean referring to #5441 and changing 2 files (csrc/punica/bgmv/bgmv_config.h, tests/lora/test_punica.py). And build and install again. I am going to try that! Thanks!",
        "created_at": "2024-06-20T07:30:29Z"
      },
      {
        "author": "jeejeelee",
        "body": "> > > Hi, Thanks for your help. I encountered the similar situation while fine tuning. My error : RuntimeError: No suitable kernel. h_in=16 h_out=19200 dtype=Float out_dtype=BFloat16\r\n> > > Could you say how can I treat this? And how '-tensor-parallel-size' option can avoid this error? Thanks a lot in advance!\r\n> > > > Current punica kernel can't process ` h_out=3424` , you can set `-tensor-parallel-size 2` to avoid this error\r\n> > \r\n> > \r\n> > HiÔºåalthough the error messages are similar, your situation are not the same. You can refer to #5441 to add support for `19200`.\r\n> > 3424 is not divisible by 64, so the Punica kernel can't process h_out=3424. However, 19200 is divisible by 64 and can be processed.\r\n> \r\n> Thanks for your help! You mean referring to #5441 and changing 2 files (csrc/punica/bgmv/bgmv_config.h, tests/lora/test_punica.py). And build and install again. I am going to try that! Thanks!\r\n\r\nYes, you can try it by:\r\n```python\r\n\r\nexport VLLM_INSTALL_PUNICA_KERNELS=1 #  build for multi-LoRA capability\r\npip install -e .  # This may take 5-10 minutes.\r\n```",
        "created_at": "2024-06-20T07:43:01Z"
      },
      {
        "author": "mgoin",
        "body": "This should be resolved with the new landed Triton kernels https://github.com/vllm-project/vllm/pull/5036",
        "created_at": "2024-08-02T15:27:07Z"
      },
      {
        "author": "4daJKong",
        "body": "Dear all,\r\n\r\nI use 0.4.3 version vllm face similar error: `RuntimeError: No suitable kernel. h_in=32 h_out=65280 dtype=Float out_dtype=Half`\r\nI install vllm by `pip install vllm==0.4.3` I know the latest version have solve this problem but it is difficult for me to upgrade the new version because there is some compatibility issues in my server(e.g. transformer and pytorch)\r\n\r\nBesides, I didn't find the `vllm/csrc/punica/bgmv/bgmv_config.h` after ` git clone https://github.com/vllm-project/vllm.git` and still don't know how to complie it after correcting.\r\n\r\nNow, I have to merge lora with raw model to infer, but it is so large in disk,  so is there any choice I can do to solve this issue?\r\n\r\n@zhanghanweii @jeejeelee \r\n ",
        "created_at": "2024-08-16T09:09:02Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4054": {
    "issue_number": 4054,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4054",
    "title": "[Bug]: Incorrect Data Type Conversion for MultiModalData",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nI've encountered an issue with the code in the vllm project on GitHub. Specifically, at line 170 of the llm.py file https://github.com/vllm-project/vllm/blob/main/vllm/entrypoints/llm.py#L170, there is a conversion of multi_modal_data.data to torch.float16 using the statement multi_modal_data.data = multi_modal_data.data.to(torch.float16). This automatic conversion may not be suitable for all use cases, especially if the model is designed to operate with bfloat16 or other numerical precisions. If this is an issue that needs to be addressed, I would be happy to submit a pull request with a fix",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-13T08:23:35Z",
    "closed_at": "2024-06-03T05:56:42Z",
    "author": "caiqi",
    "comments_count": 1,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "I have refactored the processing logic in #3978 which removes the assumption of `float16` data type. Could you suggest a test case to determine whether the conversion problem still exists or not?",
        "created_at": "2024-04-16T05:47:55Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4100": {
    "issue_number": 4100,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4100",
    "title": "[Bug]: --engine-use-ray is broken.",
    "body": "### Your current environment\r\n\r\nN/A (happens in all env)\r\n\r\n### üêõ Describe the bug\r\n\r\n```\r\npython -m vllm.entrypoints.openai.api_server --model \"facebook/opt-125m\" --dtype auto --api-key token-abc123 --engine-use-ray\r\n```\r\n\r\nWhen I start an openai server with engine-use-ray, I found it crashes with the following error\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/home/ray/anaconda3/lib/python3.9/runpy.py\", line 197, in _run_module_as_main\r\n    return _run_code(code, main_globals, None,\r\n  File \"/home/ray/anaconda3/lib/python3.9/runpy.py\", line 87, in _run_code\r\n    exec(code, run_globals)\r\n  File \"/home/ray/default/vllm/vllm/entrypoints/openai/api_server.py\", line 157, in <module>\r\n    engine = AsyncLLMEngine.from_engine_args(\r\n  File \"/home/ray/default/vllm/vllm/engine/async_llm_engine.py\", line 347, in from_engine_args\r\n    engine = cls(\r\n  File \"/home/ray/default/vllm/vllm/engine/async_llm_engine.py\", line 311, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)\r\n  File \"/home/ray/default/vllm/vllm/engine/async_llm_engine.py\", line 413, in _init_engine\r\n    cache_config = args[1]\r\nIndexError: tuple index out of range\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-15T22:44:55Z",
    "closed_at": "2024-04-16T05:24:55Z",
    "author": "rkooo567",
    "comments_count": 13,
    "comments": [
      {
        "author": "jdinalt",
        "body": "I suspect an \"off-by-one\" index error in the worker.\r\n\r\nI am seeing what looks like a related failure when passing \"--tensor-parallel-size 4\"\r\n\r\nI added some instrumentation to the failure point:\r\n\r\nvllm/worker/worker.py\r\n```\r\n...\r\n309 def _check_if_gpu_supports_dtype(torch_dtype: torch.dtype):                                                             \r\n310     # Check if the GPU supports the dtype.                                                                              \r\n311     if torch_dtype == torch.bfloat16:                                                                                   \r\n312         print(\"Current Device:\")                                                                                        \r\n313         print(torch.cuda.current_device(), \"of\", torch.cuda.device_count())                                             \r\n314         compute_capability = torch.cuda.get_device_capability()\r\n...\r\n```\r\n\r\n```\r\n(RayWorkerVllm pid=2098) INFO 04-16 01:10:45 selector.py:28] Using FlashAttention backend.\r\nCurrent Device:\r\n0 of 4\r\n(RayWorkerVllm pid=2098) Current Device:\r\n(RayWorkerVllm pid=2098) 1 of 1\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50] Error executing method init_device. This might cause deadlock in distributed execution.\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50] Traceback (most recent call last):\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]   File \"/home/dinalt/ai_assets/code/vllm/vllm/engine/ray_utils.py\", line 43, in execute_method\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]     return executor(*args, **kwargs)\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]   File \"/home/dinalt/ai_assets/code/vllm/vllm/worker/worker.py\", line 99, in init_device\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]     _check_if_gpu_supports_dtype(self.model_config.dtype)\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]   File \"/home/dinalt/ai_assets/code/vllm/vllm/worker/worker.py\", line 314, in _check_if_gpu_supports_dtype\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]     compute_capability = torch.cuda.get_device_capability()\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]   File \"/home/dinalt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 439, in get_device_capability\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]     prop = get_device_properties(device)\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]   File \"/home/dinalt/.local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 456, in get_device_properties\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50]     raise AssertionError(\"Invalid device id\")\r\n(RayWorkerVllm pid=2098) ERROR 04-16 01:10:45 ray_utils.py:50] AssertionError: Invalid device id\r\n```\r\n\r\nYou can see that the device index assigned to the worker is \"1 of 1,\" where the only valid index is '0' for a single assigned GPU.\r\n\r\nI'll see if I can root-cause it...",
        "created_at": "2024-04-16T01:21:01Z"
      },
      {
        "author": "jdinalt",
        "body": "I have located the commit which caused the breakage via binary search. It's large enough that it may take a bit of time to parse the details of the change and identify where things went wrong.\r\n\r\nhttps://github.com/vllm-project/vllm/commit/09473ee41c0a22c4d18936ea7eb2328071c19308\r\n\r\nIf you roll back to the commit immediately preceding the above:\r\n```\r\ngit reset --hard d4ec9ffb9574988132d927fd1615180522877262\r\n```\r\n\r\n...Ray works again.\r\n\r\nAnd moving forward to the identified commit breaks it.",
        "created_at": "2024-04-16T02:04:53Z"
      },
      {
        "author": "jdinalt",
        "body": "I was able to trace the issue to the modification of \"ray_utils.py\"\r\n\r\nThe bug does not appear to be in the changes made to this file but a side effect of importing the \"Worker\" module.\r\n\r\nThe following diff is sufficient to eliminate the problem:\r\n```\r\ndiff --git a/vllm/engine/ray_utils.py b/vllm/engine/ray_utils.py\r\nindex 04d4ed8..175fa6f 100644\r\n--- a/vllm/engine/ray_utils.py\r\n+++ b/vllm/engine/ray_utils.py\r\n@@ -4,7 +4,7 @@ from typing import Callable, List, Optional, Tuple\r\n from vllm.config import ParallelConfig\r\n from vllm.logger import init_logger\r\n from vllm.utils import get_ip, is_hip, set_cuda_visible_devices\r\n-from vllm.worker.worker import Worker\r\n+#from vllm.worker.worker import Worker\r\n \r\n logger = init_logger(__name__)\r\n \r\n@@ -19,18 +19,18 @@ try:\r\n             if init_cached_hf_modules:\r\n                 from transformers.dynamic_module_utils import init_hf_modules\r\n                 init_hf_modules()\r\n-            self._worker: Optional[Worker] = None\r\n+            self._worker = None\r\n             # Since the compiled DAG runs a main execution\r\n             # in a different thread that calls cuda.set_device.\r\n             # The flag indicates is set_device is called on\r\n             # that thread.\r\n             self.compiled_dag_cuda_device_set = False\r\n \r\n-        def init_worker(self, worker_init_fn: Callable[[], Worker]):\r\n+        def init_worker(self, worker_init_fn):\r\n             self._worker = worker_init_fn()\r\n \r\n         @property\r\n-        def worker(self) -> Worker:\r\n+        def worker(self):\r\n             assert self._worker is not None\r\n             return self._worker\r\n```\r\n\r\nOf the changes to this file, the problem persists until the line importing \"Worker\" is removed. This implicates a fragile initialization order dependency. I did not see anything obviously wrong in \"worker.py,\" so it's likely caused by a direct or indirect import of that module. My money would be on one of the \"distributed\" related imports.\r\n\r\nAnyhow, the above patch should be sufficient to at least make the library usable again with multiple GPUs. The initialization order dependency has yet to be identified.",
        "created_at": "2024-04-16T03:29:23Z"
      },
      {
        "author": "rkooo567",
        "body": "@jdinalt thanks for the investigation! do you plan to create a PR? I think we should also make sure to have unit tests for this code path ",
        "created_at": "2024-04-16T03:46:15Z"
      },
      {
        "author": "jdinalt",
        "body": "With respect to the original test case:\r\n```\r\npython -m vllm.entrypoints.openai.api_server --model \"facebook/opt-125m\" --dtype auto --api-key token-abc123 --engine-use-ray\r\n```\r\nI can reproduce the bug with and without my change, but it is not present if I explicitly set the tensor parallel size:\r\n```\r\npython -m vllm.entrypoints.openai.api_server --tensor-parallel-size 4 --model \"facebook/opt-125m\" --dtype auto --api-key token-abc123 --engine-use-ray\r\n```\r\nMy suspicion is that it relates to having 6 GPUs on my system, which is not a power-of-two.\r\n\r\nIt still crashes and burns without my patch above, but with both the patch and specifying the size, it works fine. ",
        "created_at": "2024-04-16T03:48:38Z"
      },
      {
        "author": "jdinalt",
        "body": "@rkooo567 Sure. I'll create a PR. I would really like to know what the real issue is, as that has not been solved, but this is at least good enough to make it usable again.\r\n\r\nAnd if you would like another hand with this project, I'm happy to help.",
        "created_at": "2024-04-16T03:50:20Z"
      },
      {
        "author": "rkooo567",
        "body": "actually I may have identified the root cause... making a PR now",
        "created_at": "2024-04-16T04:02:18Z"
      },
      {
        "author": "rkooo567",
        "body": "https://github.com/vllm-project/vllm/pull/4105/files lmk if this makes sense @jdinalt ",
        "created_at": "2024-04-16T04:07:56Z"
      },
      {
        "author": "jdinalt",
        "body": "Re: Pull 4105\r\n\r\nThe test looks like it should do what is desired. At present, there appears to be something broken with respect to having to manually specify \"--tensor-parallel-size.\"\r\n\r\nI suspect that this is a separate bug related to automatically determining the tensor-parallel size when not explicitly specified. It does not appear to be related to my six GPU setup, as the following command still fails:\r\n\r\n```\r\nCUDA_VISIBLE_DEVICES=\"0,1,2,3\" python -m vllm.entrypoints.openai.api_server --model \"facebook/opt-125m\" --dtype auto --api-key token-abc123 --engine-use-ray\r\n```\r\n\r\nBefore the unit test is checked in, the root cause of the above needs to be determined and fixed.\r\n\r\nI'll dig in a little deeper, as it is obviously not setting that parameter automatically to the number of GPUs.",
        "created_at": "2024-04-16T05:09:37Z"
      },
      {
        "author": "jdinalt",
        "body": "Sorry. I somehow missed the second half of your PR.\r\n\r\nJust came to the same conclusion, so I think your change will work.",
        "created_at": "2024-04-16T05:17:33Z"
      },
      {
        "author": "jdinalt",
        "body": "I ran my test with Pull 4105, both on its own and with my PR.\r\n\r\n```\r\npython -m vllm.entrypoints.openai.api_server --model \"facebook/opt-125m\" --dtype auto --api-key token-abc123 --engine-use-ray\r\n```\r\nPR4105: Pass\r\nPR4105 + PR4106: Pass\r\n\r\nI noted that the tensor-parallel level was \"1\" for this test, with only a single GPU being utilized.\r\n\r\n```\r\npython -m vllm.entrypoints.openai.api_server --tensor-parallel-size 4 --model \"facebook/opt-125m\" --dtype auto --api-key token-abc123 --engine-use-ray\r\n```\r\nPR4105: Fail\r\nPR4105 + PR4106: Pass\r\n\r\nWhen the tensor-parallel value is greater than one, the test fails with only PR4105, hitting the bug I came across.\r\n\r\nYou may want to add an explicit parallel configuration to the tests to catch this type of failure.",
        "created_at": "2024-04-16T05:42:05Z"
      },
      {
        "author": "rkooo567",
        "body": "ah interesting higher tp size breaks the test... let me try that out tmrw. Your PR is supposed fix the issue? (it is surprising given your PR only changes the typing annotation...?)",
        "created_at": "2024-04-16T13:52:21Z"
      },
      {
        "author": "jdinalt",
        "body": "The PR is a work-around. The actual root-cause is still TBD. The failure appears to be a side effect of importing \"Worker,\" where https://github.com/vllm-project/vllm/commit/09473ee41c0a22c4d18936ea7eb2328071c19308 has exposed a pre-existing bug.\r\n\r\nI did not have time to dig much deeper the other night, but I did not see anything obvious directly in \"vllm.worker.worker.\" This suggests that the cause may be an direct or indirect import of that module. My suspicion is that it is either a module initialization ordering dependency, or worse, an initialization race-condition.\r\n\r\nMy PR just let's one use tensor-parallel again -- until the root cause can be identified. After that, restoring the original change should be fine.\r\n\r\nLet me know if you can't reproduce it. If that's the case, it's more likely timing dependent. The kind of bug which can be very difficult to pinpoint.",
        "created_at": "2024-04-18T08:03:57Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4171": {
    "issue_number": 4171,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4171",
    "title": "[Bug]: Server crash for bloom-3b while use prefix_caching, `AssertionError assert Lk in {16, 32, 64, 128}`",
    "body": "### Your current environment\n\n```text\r\nvllm v0.4.0.post1 CUDA 12.2\r\n```\r\n\n\n### üêõ Describe the bug\n\n```text\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/ops/paged_attn.py\", line 178, in forward_prefix\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>>     context_attention_fwd(\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>>   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>>     return func(*args, **kwargs)\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>>   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/ops/prefix_prefill.py\", line 639, in context_attention_fwd\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>>     assert Lk in {16, 32, 64, 128}\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>> AssertionError\r\n[2024-04-17 20:07:13,727] [ERROR] [MainThread] [asyncio] >>> \r\n[2024-04-17 20:07:13\r\n```\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-18T09:52:30Z",
    "closed_at": "2024-05-08T16:19:59Z",
    "author": "DefTruth",
    "comments_count": 9,
    "comments": [
      {
        "author": "DefTruth",
        "body": "\"n_head\": 32\r\n\"hidden_size\": 2560",
        "created_at": "2024-04-18T10:10:20Z"
      },
      {
        "author": "Guy-Shapira",
        "body": "Hey, I had a similar problem and solved it by changing the `enable_prefix_caching` to `False` while initalizing the model\n\nHope it will help",
        "created_at": "2024-04-18T16:09:03Z"
      },
      {
        "author": "DefTruth",
        "body": "@youkaichao for the latest vllm, i still hit this error for bloom-3b\r\n```bash\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>     return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/attention/backends/flash_attn.py\", line 240, in forward\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>     output[:num_prefill_tokens] = PagedAttention.forward_prefix(\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/attention/ops/paged_attn.py\", line 177, in forward_prefix\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>     context_attention_fwd(\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>     return func(*args, **kwargs)\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>   File \"/root/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/attention/ops/prefix_prefill.py\", line 657, in context_attention_fwd\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>>     assert Lk == Lk_padded\r\n[2024-04-30 13:05:15,529] [ERROR] [MainThread] [vllm.engine.async_llm_engine] >>> AssertionError\r\n```",
        "created_at": "2024-04-30T05:08:00Z"
      },
      {
        "author": "youkaichao",
        "body": "Hi, I'm not familiar with this kernel yet. cc @WoosukKwon  and @zhuohan123 may help.",
        "created_at": "2024-04-30T05:12:58Z"
      },
      {
        "author": "zhuohan123",
        "body": "I believe this is because the context attention triton kernel does not support the attention key size in bloom-3b (which is `80`). This should be able to be solved once we fully port flashattention kernels or flashinfer kernels. (#4353)",
        "created_at": "2024-04-30T05:17:39Z"
      },
      {
        "author": "DefTruth",
        "body": "@zhuohan123  this pr https://github.com/vllm-project/vllm/pull/4128/files seems solve non-power-of-two head sizes in prefix prefill, but also add an `assert` while alibi slopes is not None, bloom-3b use alibi and then hit this assert.",
        "created_at": "2024-04-30T05:23:24Z"
      },
      {
        "author": "DefTruth",
        "body": "@youkaichao i have make a fix pr https://github.com/vllm-project/vllm/pull/4489/ to this issue, allow non-power-of-two head sizes in prefix prefill with alibi, this is a small fix based on https://github.com/vllm-project/vllm/pull/4128.\r\nit works for me, an bloom-3b did not hit this crash again with `--enable-prefix-caching`",
        "created_at": "2024-04-30T06:06:23Z"
      },
      {
        "author": "youkaichao",
        "body": "Do you mention the wrong person? It is @zhuohan123 above, but you mentioned @zhouyuan .",
        "created_at": "2024-04-30T06:13:12Z"
      },
      {
        "author": "DefTruth",
        "body": "> Do you mention the wrong person? It is @zhuohan123 above, but you mentioned @zhouyuan .\r\n\r\noh, sorry ~",
        "created_at": "2024-04-30T06:16:56Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4293": {
    "issue_number": 4293,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4293",
    "title": "[Bug]: Engine iteration timed out. This should never happen occurred when vllm 0.4.1 deployed llama3. ",
    "body": "### Your current environment\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-4.18.0-193.14.2.el8_2.x86_64-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.140\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A800-SXM4-80GB\r\nGPU 1: NVIDIA A800-SXM4-80GB\r\nGPU 2: NVIDIA A800-SXM4-80GB\r\nGPU 3: NVIDIA A800-SXM4-80GB\r\n\r\nNvidia driver version: 535.104.12\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.4.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.4.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   46 bits physical, 57 bits virtual\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       GenuineIntel\r\nCPU family:                      6\r\nModel:                           106\r\nModel name:                      Intel(R) Xeon(R) Platinum 8378A CPU @ 3.00GHz\r\nStepping:                        6\r\nFrequency boost:                 enabled\r\nCPU MHz:                         3500.000\r\nCPU max MHz:                     3500.0000\r\nCPU min MHz:                     800.0000\r\nBogoMIPS:                        6000.00\r\nVirtualization:                  VT-x\r\nL1d cache:                       3 MiB\r\nL1i cache:                       2 MiB\r\nL2 cache:                        80 MiB\r\nL3 cache:                        96 MiB\r\nNUMA node0 CPU(s):               0-31,64-95\r\nNUMA node1 CPU(s):               32-63,96-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Vulnerable\r\nVulnerability Spectre v1:        Vulnerable: __user pointer sanitization and usercopy barriers only; no swapgs barriers\r\nVulnerability Spectre v2:        Vulnerable, IBPB: disabled, STIBP: disabled\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid md_clear pconfig flush_l1d arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.2.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.2.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    NIC0    NIC1    NIC2    NIC3    NIC4    NIC5    NIC6    NIC7    NIC8    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV8     NV8     NV8     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     NODE    32-63,96-127    1               N/A\r\nGPU1    NV8      X      NV8     NV8     SYS     SYS     SYS     SYS     SYS     SYS     SYS     PXB     NODE    32-63,96-127    1               N/A\r\nGPU2    NV8     NV8      X      NV8     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PXB     32-63,96-127    1               N/A\r\nGPU3    NV8     NV8     NV8      X      SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE    PXB     32-63,96-127    1               N/A\r\nNIC0    SYS     SYS     SYS     SYS      X      PIX     PIX     NODE    NODE    NODE    NODE    SYS     SYS\r\nNIC1    SYS     SYS     SYS     SYS     PIX      X      PIX     NODE    NODE    NODE    NODE    SYS     SYS\r\nNIC2    SYS     SYS     SYS     SYS     PIX     PIX      X      NODE    NODE    NODE    NODE    SYS     SYS\r\nNIC3    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      PIX     NODE    NODE    SYS     SYS\r\nNIC4    SYS     SYS     SYS     SYS     NODE    NODE    NODE    PIX      X      NODE    NODE    SYS     SYS\r\nNIC5    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE     X      PIX     SYS     SYS\r\nNIC6    SYS     SYS     SYS     SYS     NODE    NODE    NODE    NODE    NODE    PIX      X      SYS     SYS\r\nNIC7    PXB     PXB     NODE    NODE    SYS     SYS     SYS     SYS     SYS     SYS     SYS      X      NODE\r\nNIC8    NODE    NODE    PXB     PXB     SYS     SYS     SYS     SYS     SYS     SYS     SYS     NODE     X\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n  NIC2: mlx5_2\r\n  NIC3: mlx5_3\r\n  NIC4: mlx5_4\r\n  NIC5: mlx5_5\r\n  NIC6: mlx5_6\r\n  NIC7: mlx5_7\r\n  NIC8: mlx5_8\r\n\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nI'm doing this via python -m vllm.entrypoints.openai.api_server --port 7801 --host 0.0.0.0 --model /mnt/model/llama3-70B-instruct --served-model-name vllm_llama3_70B_instruct --tensor-parallel- size 4 --trust-remote-code After deploying llama3, we started to perform performance tests on the model concurrently. The model could reply normally at the beginning, but about 10 minutes into the test, vllm reported the following error (I installed vllm through the source code 0.4.1):\r\n\r\n`\r\nRROR 04-23 16:19:04 async_llm_engine.py:499] Engine iteration timed out. This should never happen!\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] Engine background task failed\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] Traceback (most recent call last):\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 470, in engine_step\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     request_outputs = await self.engine.step_async()\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     output = await self.model_executor.execute_model_async(\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 424, in execute_model_async\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     all_outputs = await self._run_workers_async(\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 414, in _run_workers_async\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     all_outputs = await asyncio.gather(*coros)\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] asyncio.exceptions.CancelledError\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] \r\nERROR 04-23 16:19:04 async_llm_engine.py:43] During handling of the above exception, another exception occurred:\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] \r\nERROR 04-23 16:19:04 async_llm_engine.py:43] Traceback (most recent call last):\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     return fut.result()\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] asyncio.exceptions.CancelledError\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] \r\nERROR 04-23 16:19:04 async_llm_engine.py:43] The above exception was the direct cause of the following exception:\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] \r\nERROR 04-23 16:19:04 async_llm_engine.py:43] Traceback (most recent call last):\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     task.result()\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 496, in run_engine_loop\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]   File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\r\nERROR 04-23 16:19:04 async_llm_engine.py:43]     raise exceptions.TimeoutError() from exc\r\nERROR 04-23 16:19:04 async_llm_engine.py:43] asyncio.exceptions.TimeoutError\r\nERROR:asyncio:Exception in callback functools.partial(<function _raise_exception_on_finish at 0x7f4124ad08b0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f412c39bac0>>)\r\nhandle: <Handle functools.partial(<function _raise_exception_on_finish at 0x7f4124ad08b0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7f412c39bac0>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 470, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 424, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 414, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\r\n    return fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 496, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 45, in _raise_exception_on_finish\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nINFO 04-23 16:19:04 async_llm_engine.py:154] Aborted request cmpl-cb9ae6d5b74b48a28f23d9f4c323a104.\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 470, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 424, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 414, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\nasyncio.exceptions.CancelledError\r\n\r\nDuring handling of the above exception, another exception occurred:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/asyncio/tasks.py\", line 456, in wait_for\r\n    return fut.result()\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 419, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/uvicorn/middleware/proxy_headers.py\", line 84, in __call__\r\n    return await self.app(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/middleware/cors.py\", line 83, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/routing.py\", line 758, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/routing.py\", line 778, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/routing.py\", line 299, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/routing.py\", line 79, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/starlette/routing.py\", line 74, in app\r\n    response = await func(request)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/fastapi/routing.py\", line 299, in app\r\n    raise e\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/fastapi/routing.py\", line 294, in app\r\n    raw_response = await run_endpoint_function(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 89, in create_chat_completion\r\n    generator = await openai_serving_chat.create_chat_completion(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 95, in create_chat_completion\r\n    return await self.chat_completion_full_generator(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/entrypoints/openai/serving_chat.py\", line 258, in chat_completion_full_generator\r\n    async for res in result_generator:\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 661, in generate\r\n    raise e\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 655, in generate\r\n    async for request_output in stream:\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 77, in __anext__\r\n    raise result\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 496, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/asyncio/tasks.py\", line 458, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nasyncio.exceptions.TimeoutError\r\nINFO 04-23 16:19:04 async_llm_engine.py:154] Aborted request cmpl-ca47d0961c59407ab90792f513566cbd.\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 470, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 424, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\r\n  File \"/usr/local/miniconda3/envs/vllm_llama3/lib/python3.10/site-packages/vllm/executor/ray_gpu_executor.py\", line 414, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\nasyncio.exceptions.CancelledError\r\n\r\n`\r\n\r\nHow should I solve this problem? ?",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-04-23T11:30:19Z",
    "closed_at": "2024-11-29T02:06:30Z",
    "author": "blackblue9",
    "comments_count": 8,
    "comments": [
      {
        "author": "supdizh",
        "body": "looks like the same issue with https://github.com/vllm-project/vllm/issues/4135\r\nemerged after 0.4.0",
        "created_at": "2024-04-25T01:55:31Z"
      },
      {
        "author": "JPonsa",
        "body": "Having the same error with Mixtral-8x7B-Instruct-v0.1-GPTQ  and  tensor_parallel_size=2\r\n```\r\nINFO 04-25 09:50:28 api_server.py:149] vLLM API server version 0.4.0.post1\r\nINFO 04-25 09:50:28 api_server.py:150] args: Namespace(host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='half', kv_cache_dtype='auto', max_model_len=5000, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.8, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization='gptq', enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\nWARNING 04-25 09:50:29 config.py:767] Casting torch.bfloat16 to torch.float16.\r\nWARNING 04-25 09:50:29 config.py:211] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\nINFO 04-25 09:51:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\nINFO 04-25 09:51:20 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\nINFO 04-25 09:51:20 selector.py:25] Using XFormers backend.\r\n\u001b[36m(RayWorkerVllm pid=233183)\u001b[0m INFO 04-25 09:51:21 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\n\u001b[36m(RayWorkerVllm pid=233183)\u001b[0m INFO 04-25 09:51:21 selector.py:25] Using XFormers backend.\r\nINFO 04-25 09:51:33 pynccl_utils.py:45] vLLM is using nccl==2.18.1\r\n\u001b[36m(RayWorkerVllm pid=233183)\u001b[0m INFO 04-25 09:51:33 pynccl_utils.py:45] vLLM is using nccl==2.18.1\r\nINFO 04-25 09:52:18 custom_all_reduce.py:137] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\r\n\u001b[36m(RayWorkerVllm pid=233183)\u001b[0m INFO 04-25 09:52:18 custom_all_reduce.py:137] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\r\nINFO 04-25 09:52:23 weight_utils.py:177] Using model weights format ['*.safetensors']\r\n\u001b[36m(RayWorkerVllm pid=233183)\u001b[0m INFO 04-25 09:52:24 weight_utils.py:177] Using model weights format ['*.safetensors']\r\nINFO 04-25 09:53:12 model_runner.py:104] Loading model weights took 11.0906 GB\r\nI am awake\r\n   time         mem   processes  process usage\r\n  (secs)        (MB)  tot  actv  (sorted, %CPU)\r\n\u001b[36m(RayWorkerVllm pid=233183)\u001b[0m INFO 04-25 09:53:12 model_runner.py:104] Loading model weights took 11.0906 GB\r\nINFO 04-25 09:54:42 ray_gpu_executor.py:240] # GPU blocks: 12524, # CPU blocks: 4096\r\n```\r\n```\r\nException in callback functools.partial(<function _raise_exception_on_finish at 0x2aad73314ae0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x2aad836e63d0>>)\r\nhandle: <Handle functools.partial(<function _raise_exception_on_finish at 0x2aad73314ae0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x2aad836e63d0>>)>\r\nTraceback (most recent call last):\r\n  File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 490, in wait_for\r\n    return fut.result()\r\n           ^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 454, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 418, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 408, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 480, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 492, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nTimeoutError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 45, in _raise_exception_on_finish\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\nERROR:    Exception in ASGI application\r\nTraceback (most recent call last):\r\n  File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 490, in wait_for\r\n    return fut.result()\r\n           ^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 454, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 418, in execute_model_async\r\n    all_outputs = await self._run_workers_async(\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 408, in _run_workers_async\r\n    all_outputs = await asyncio.gather(*coros)\r\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\nasyncio.exceptions.CancelledError\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n    result = await app(  # type: ignore[func-returns-value]\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n    return await self.app(scope, receive, send)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n    await super().__call__(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n    raise exc\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n    await self.app(scope, receive, _send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n    await self.app(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 756, in __call__\r\n    await self.middleware_stack(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 776, in app\r\n    await route.handle(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 297, in handle\r\n    await self.app(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\r\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n    raise exc\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n    await app(scope, receive, sender)\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 72, in app\r\n    response = await func(request)\r\n               ^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 278, in app\r\n    raw_response = await run_endpoint_function(\r\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n    return await dependant.call(**values)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 103, in create_completion\r\n    generator = await openai_serving_completion.create_completion(\r\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_completion.py\", line 178, in create_completion\r\n    async for i, res in result_generator:\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_completion.py\", line 81, in consumer\r\n    raise item\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_completion.py\", line 66, in producer\r\n    async for item in iterator:\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 644, in generate\r\n    raise e\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 638, in generate\r\n    async for request_output in stream:\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 77, in __anext__\r\n    raise result\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n    task.result()\r\n  File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 480, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 492, in wait_for\r\n    raise exceptions.TimeoutError() from exc\r\nTimeoutError\r\n```",
        "created_at": "2024-04-25T17:59:20Z"
      },
      {
        "author": "ericzhou571",
        "body": "> Having the same error with Mixtral-8x7B-Instruct-v0.1-GPTQ and tensor_parallel_size=2\r\n> \r\n> ```\r\n> INFO 04-25 09:50:28 api_server.py:149] vLLM API server version 0.4.0.post1\r\n> INFO 04-25 09:50:28 api_server.py:150] args: Namespace(host=None, port=8001, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, served_model_name=None, lora_modules=None, chat_template=None, response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer=None, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=False, download_dir=None, load_format='auto', dtype='half', kv_cache_dtype='auto', max_model_len=5000, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.8, forced_num_gpu_blocks=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=5, disable_log_stats=False, quantization='gptq', enforce_eager=True, max_context_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', max_cpu_loras=None, device='auto', image_input_type=None, image_token_id=None, image_input_shape=None, image_feature_size=None, scheduler_delay_factor=0.0, enable_chunked_prefill=False, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\n> WARNING 04-25 09:50:29 config.py:767] Casting torch.bfloat16 to torch.float16.\r\n> WARNING 04-25 09:50:29 config.py:211] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\r\n> INFO 04-25 09:51:01 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer='TheBloke/Mixtral-8x7B-Instruct-v0.1-GPTQ', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=5000, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\r\n> INFO 04-25 09:51:20 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\n> INFO 04-25 09:51:20 selector.py:25] Using XFormers backend.\r\n> ÔøΩ[36m(RayWorkerVllm pid=233183)ÔøΩ[0m INFO 04-25 09:51:21 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\r\n> ÔøΩ[36m(RayWorkerVllm pid=233183)ÔøΩ[0m INFO 04-25 09:51:21 selector.py:25] Using XFormers backend.\r\n> INFO 04-25 09:51:33 pynccl_utils.py:45] vLLM is using nccl==2.18.1\r\n> ÔøΩ[36m(RayWorkerVllm pid=233183)ÔøΩ[0m INFO 04-25 09:51:33 pynccl_utils.py:45] vLLM is using nccl==2.18.1\r\n> INFO 04-25 09:52:18 custom_all_reduce.py:137] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\r\n> ÔøΩ[36m(RayWorkerVllm pid=233183)ÔøΩ[0m INFO 04-25 09:52:18 custom_all_reduce.py:137] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\r\n> INFO 04-25 09:52:23 weight_utils.py:177] Using model weights format ['*.safetensors']\r\n> ÔøΩ[36m(RayWorkerVllm pid=233183)ÔøΩ[0m INFO 04-25 09:52:24 weight_utils.py:177] Using model weights format ['*.safetensors']\r\n> INFO 04-25 09:53:12 model_runner.py:104] Loading model weights took 11.0906 GB\r\n> I am awake\r\n>    time         mem   processes  process usage\r\n>   (secs)        (MB)  tot  actv  (sorted, %CPU)\r\n> ÔøΩ[36m(RayWorkerVllm pid=233183)ÔøΩ[0m INFO 04-25 09:53:12 model_runner.py:104] Loading model weights took 11.0906 GB\r\n> INFO 04-25 09:54:42 ray_gpu_executor.py:240] # GPU blocks: 12524, # CPU blocks: 4096\r\n> ```\r\n> \r\n> ```\r\n> Exception in callback functools.partial(<function _raise_exception_on_finish at 0x2aad73314ae0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x2aad836e63d0>>)\r\n> handle: <Handle functools.partial(<function _raise_exception_on_finish at 0x2aad73314ae0>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x2aad836e63d0>>)>\r\n> Traceback (most recent call last):\r\n>   File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 490, in wait_for\r\n>     return fut.result()\r\n>            ^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 454, in engine_step\r\n>     request_outputs = await self.engine.step_async()\r\n>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n>     output = await self.model_executor.execute_model_async(\r\n>              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 418, in execute_model_async\r\n>     all_outputs = await self._run_workers_async(\r\n>                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 408, in _run_workers_async\r\n>     all_outputs = await asyncio.gather(*coros)\r\n>                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n> asyncio.exceptions.CancelledError\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n>     task.result()\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 480, in run_engine_loop\r\n>     has_requests_in_progress = await asyncio.wait_for(\r\n>                                ^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 492, in wait_for\r\n>     raise exceptions.TimeoutError() from exc\r\n> TimeoutError\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 45, in _raise_exception_on_finish\r\n>     raise AsyncEngineDeadError(\r\n> vllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for the actual cause.\r\n> ERROR:    Exception in ASGI application\r\n> Traceback (most recent call last):\r\n>   File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 490, in wait_for\r\n>     return fut.result()\r\n>            ^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 454, in engine_step\r\n>     request_outputs = await self.engine.step_async()\r\n>                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 213, in step_async\r\n>     output = await self.model_executor.execute_model_async(\r\n>              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 418, in execute_model_async\r\n>     all_outputs = await self._run_workers_async(\r\n>                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/executor/ray_gpu_executor.py\", line 408, in _run_workers_async\r\n>     all_outputs = await asyncio.gather(*coros)\r\n>                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n> asyncio.exceptions.CancelledError\r\n> \r\n> The above exception was the direct cause of the following exception:\r\n> \r\n> Traceback (most recent call last):\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/uvicorn/protocols/http/httptools_impl.py\", line 411, in run_asgi\r\n>     result = await app(  # type: ignore[func-returns-value]\r\n>              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py\", line 69, in __call__\r\n>     return await self.app(scope, receive, send)\r\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/fastapi/applications.py\", line 1054, in __call__\r\n>     await super().__call__(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/applications.py\", line 123, in __call__\r\n>     await self.middleware_stack(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 186, in __call__\r\n>     raise exc\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/errors.py\", line 164, in __call__\r\n>     await self.app(scope, receive, _send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/cors.py\", line 85, in __call__\r\n>     await self.app(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/middleware/exceptions.py\", line 65, in __call__\r\n>     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n>     raise exc\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n>     await app(scope, receive, sender)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 756, in __call__\r\n>     await self.middleware_stack(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 776, in app\r\n>     await route.handle(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 297, in handle\r\n>     await self.app(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 77, in app\r\n>     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 64, in wrapped_app\r\n>     raise exc\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\r\n>     await app(scope, receive, sender)\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/starlette/routing.py\", line 72, in app\r\n>     response = await func(request)\r\n>                ^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 278, in app\r\n>     raw_response = await run_endpoint_function(\r\n>                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/fastapi/routing.py\", line 191, in run_endpoint_function\r\n>     return await dependant.call(**values)\r\n>            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/api_server.py\", line 103, in create_completion\r\n>     generator = await openai_serving_completion.create_completion(\r\n>                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_completion.py\", line 178, in create_completion\r\n>     async for i, res in result_generator:\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_completion.py\", line 81, in consumer\r\n>     raise item\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/entrypoints/openai/serving_completion.py\", line 66, in producer\r\n>     async for item in iterator:\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 644, in generate\r\n>     raise e\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 638, in generate\r\n>     async for request_output in stream:\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 77, in __anext__\r\n>     raise result\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 38, in _raise_exception_on_finish\r\n>     task.result()\r\n>   File \"/lustre/scratch/scratch/rmhijpo/ctgov_rag/.venv/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 480, in run_engine_loop\r\n>     has_requests_in_progress = await asyncio.wait_for(\r\n>                                ^^^^^^^^^^^^^^^^^^^^^^^\r\n>   File \"/shared/ucl/apps/python/3.11.3/gnu-4.9.2/lib/python3.11/asyncio/tasks.py\", line 492, in wait_for\r\n>     raise exceptions.TimeoutError() from exc\r\n> TimeoutError\r\n> ```\r\n\r\nWould you kindly share the specifications of the GPU you utilized while encountering these issues? Also A800-80G?",
        "created_at": "2024-05-07T02:16:18Z"
      },
      {
        "author": "ywang96",
        "body": "@ericzhou571 @JPonsa @blackblue9 @supdizh Could you try `--disable-custom-all-reduce` when you launch the server and see if this issue persists?",
        "created_at": "2024-05-12T04:14:35Z"
      },
      {
        "author": "JPonsa",
        "body": "@ywang96  the issue persists when launching the server with `--disable-custom-all-reduce `",
        "created_at": "2024-05-12T18:46:07Z"
      },
      {
        "author": "Warrior0x1",
        "body": "I encountered a similar issue in version 0.4.2.",
        "created_at": "2024-05-22T07:25:20Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-29T02:00:47Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-29T02:06:29Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4422": {
    "issue_number": 4422,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4422",
    "title": "[Bug]: `assert num_new_tokens == 1` fails when `SamplingParams.n` is not `1` and `max_tokens` is large.",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.2.1+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.11.7 (main, Dec 15 2023, 18:12:31) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.11.0-27-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.91\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100 80GB PCIe\r\nGPU 1: NVIDIA A100 80GB PCIe\r\nGPU 2: NVIDIA A100 80GB PCIe\r\nGPU 3: NVIDIA A100 80GB PCIe\r\nGPU 4: NVIDIA A100 80GB PCIe\r\nGPU 5: NVIDIA A100 80GB PCIe\r\nGPU 6: NVIDIA A100 80GB PCIe\r\nGPU 7: NVIDIA A100 80GB PCIe\r\n\r\nNvidia driver version: 535.161.07\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nByte Order:                      Little Endian\r\nAddress sizes:                   43 bits physical, 48 bits virtual\r\nCPU(s):                          256\r\nOn-line CPU(s) list:             0-254\r\nOff-line CPU(s) list:            255\r\nThread(s) per core:              1\r\nCore(s) per socket:              64\r\nSocket(s):                       2\r\nNUMA node(s):                    2\r\nVendor ID:                       AuthenticAMD\r\nCPU family:                      23\r\nModel:                           49\r\nModel name:                      AMD EPYC 7742 64-Core Processor\r\nStepping:                        0\r\nFrequency boost:                 enabled\r\nCPU MHz:                         1500.000\r\nCPU max MHz:                     2250.0000\r\nCPU min MHz:                     1500.0000\r\nBogoMIPS:                        4500.41\r\nVirtualization:                  AMD-V\r\nL1d cache:                       2 MiB\r\nL1i cache:                       2 MiB\r\nL2 cache:                        32 MiB\r\nL3 cache:                        256 MiB\r\nNUMA node0 CPU(s):               0-63,128-191\r\nNUMA node1 CPU(s):               64-127,192-254\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Full AMD retpoline, IBPB conditional, IBRS_FW, STIBP conditional, RSB filling\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 hw_pstate sme ssbd mba sev ibrs ibpb stibp vmmcall sev_es fsgsbase bmi1 avx2 smep bmi2 cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif umip rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.19.3\r\n[pip3] torch==2.2.1\r\n[pip3] triton==2.2.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] blas                      1.0                         mkl    defaults\r\n[conda] mkl                       2023.1.0         h213fc3f_46344    defaults\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.19.3                   pypi_0    pypi\r\n[conda] pytorch-cuda              12.1                 ha16c6d3_5    pytorch\r\n[conda] pytorch-mutex             1.0                        cuda    pytorch\r\n[conda] torch                     2.2.1                    pypi_0    pypi\r\n[conda] triton                    2.2.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    NIC0    NIC1CPU Affinity     NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NODE    NODE    NODE    SYS     SYS     SYS     SYS     SYS SYS      0-63,128-191    0               N/A\r\nGPU1    NODE     X      NODE    NODE    SYS     SYS     SYS     SYS     SYS SYS      0-63,128-191    0               N/A\r\nGPU2    NODE    NODE     X      NODE    SYS     SYS     SYS     SYS     SYS SYS      0-63,128-191    0               N/A\r\nGPU3    NODE    NODE    NODE     X      SYS     SYS     SYS     SYS     SYS SYS      0-63,128-191    0               N/A\r\nGPU4    SYS     SYS     SYS     SYS      X      NODE    NODE    NODE    NODENODE     64-127,192-254  1               N/A\r\nGPU5    SYS     SYS     SYS     SYS     NODE     X      NODE    NODE    NODENODE     64-127,192-254  1               N/A\r\nGPU6    SYS     SYS     SYS     SYS     NODE    NODE     X      NODE    PHB PHB      64-127,192-254  1               N/A\r\nGPU7    SYS     SYS     SYS     SYS     NODE    NODE    NODE     X      NODENODE     64-127,192-254  1               N/A\r\nNIC0    SYS     SYS     SYS     SYS     NODE    NODE    PHB     NODE     X  PIX\r\nNIC1    SYS     SYS     SYS     SYS     NODE    NODE    PHB     NODE    PIX  X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: ibp194s0f0\r\n  NIC1: ibp194s0f1\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\n`assert num_new_tokens == 1` fails when `SamplingParams.n` and `max_tokens` are large, e.g.\r\n```python\r\nsampling_params = SamplingParams(n=32, best_of=32, temperature=1.6, top_p=0.95, max_tokens=2048)\r\n```\r\n\r\nError messages:\r\n```\r\nTraceback (most recent call last):\r\n  File \"./vllm-bug.py\", line 19, in <module>\r\n    outputs = llm.generate(prompts, sampling_params)\r\n              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 214, in generate\r\n    return self._run_engine(use_tqdm)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/entrypoints/llm.py\", line 242, in _run_engine\r\n    step_outputs = self.llm_engine.step()\r\n                   ^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/engine/llm_engine.py\", line 557, in step\r\n    seq_group_metadata_list, scheduler_outputs = self.scheduler.schedule()\r\n                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 890, in schedule\r\n    scheduler_outputs = self._schedule()\r\n                        ^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 863, in _schedule\r\n    return self._schedule_default()\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 733, in _schedule_default\r\n    remaining_swapped, swapped_in = self._schedule_swapped(\r\n                                    ^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/path/to/python/env/lib/python3.11/site-packages/vllm/core/scheduler.py\", line 548, in _schedule_swapped\r\n    assert num_new_tokens == 1\r\n           ^^^^^^^^^^^^^^^^^^^\r\nAssertionError\r\n```\r\n\r\nReproducing code:\r\n```\r\nfrom vllm import LLM, SamplingParams\r\nfrom datasets import load_dataset\r\nfrom transformers import AutoTokenizer\r\n\r\nmodel_id = \"deepseek-ai/deepseek-math-7b-rl\"\r\ntokenizer = AutoTokenizer.from_pretrained(model_id)\r\n\r\ngsm8k_ds = load_dataset(\"gsm8k\", \"main\")[\"test\"]\r\nprompts = [\r\n    tokenizer.apply_chat_template({\"role\": \"user\", \"content\": f\"{row['question']}\\nPlease reason step by step, and put your final answer within \\\\boxed{{}}.\"}, tokenize=False) for row in gsm8k_ds\r\n]\r\n\r\n# print(prompts)\r\n\r\nn_paths = 2\r\nsampling_params = SamplingParams(n=n_paths, best_of=n_paths, temperature=1.6, top_p=0.95, max_tokens=2048)\r\n\r\nllm = LLM(model=model_id, swap_space=60)\r\n\r\noutputs = llm.generate(prompts, sampling_params)\r\n\r\n# Print the outputs.\r\nfor output in outputs:\r\n    prompt = output.prompt\r\n    generated_text = output.outputs[0].text\r\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-04-28T03:06:10Z",
    "closed_at": "2024-05-02T02:24:14Z",
    "author": "tongyx361",
    "comments_count": 4,
    "comments": [
      {
        "author": "delingha",
        "body": "same issue, repeat N times to  generate with (n=1) to solve it temporarily",
        "created_at": "2024-04-28T09:38:33Z"
      },
      {
        "author": "simon-mo",
        "body": "cc @rkooo567 is this something you can fix in chunked scheduler?",
        "created_at": "2024-04-29T01:57:18Z"
      },
      {
        "author": "rkooo567",
        "body": "yeah let me fix this by today (and add tests)",
        "created_at": "2024-04-29T02:52:42Z"
      },
      {
        "author": "rkooo567",
        "body": "Fix PR here; https://github.com/vllm-project/vllm/pull/4451#pullrequestreview-2028061268",
        "created_at": "2024-04-29T10:22:49Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4742": {
    "issue_number": 4742,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4742",
    "title": "[Bug]: ValueError when using LoRA with CohereForCausalLM model",
    "body": "### Your current environment\n\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.2 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.2\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.13 (main, Sep 11 2023, 13:44:35) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-14-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 550.67\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.2\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.2\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             192\r\nOn-line CPU(s) list:                0-191\r\nVendor ID:                          AuthenticAMD\r\nModel name:                         AMD EPYC 7Y43 48-Core Processor\r\nCPU family:                         25\r\nModel:                              1\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 48\r\nSocket(s):                          2\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU max MHz:                        3630.7610\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           5090.45\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr rdpru wbnoinvd amd_ppin brs arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif v_spec_ctrl umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca fsrm\r\nVirtualization:                     AMD-V\r\nL1d cache:                          3 MiB (96 instances)\r\nL1i cache:                          3 MiB (96 instances)\r\nL2 cache:                           48 MiB (96 instances)\r\nL3 cache:                           512 MiB (16 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-47,96-143\r\nNUMA node1 CPU(s):                  48-95,144-191\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Mitigation; safe RET, no microcode\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] galore-torch==1.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu11==2.19.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0+cu121\r\n[pip3] torchaudio==2.3.0+cu121\r\n[pip3] torchvision==0.18.0+cu121\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] galore-torch              1.0                      pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu11          2.19.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0+cu121              pypi_0    pypi\r\n[conda] torchaudio                2.3.0+cu121              pypi_0    pypi\r\n[conda] torchvision               0.18.0+cu121             pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\n[conda] vllm-nccl-cu12            2.18.1.0.4.0             pypi_0    pypiROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU1\tSYS\t X \tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU2\tSYS\tSYS\t X \tSYS\tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU3\tSYS\tSYS\tSYS\t X \tSYS\tSYS\tSYS\tSYS\t0-47,96-143\t0\t\tN/A\r\nGPU4\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU5\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU6\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \tSYS\t48-95,144-191\t1\t\tN/A\r\nGPU7\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\tSYS\t X \t48-95,144-191\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\n\n### üêõ Describe the bug\n\nHi there,\r\nI encountered a ValueError when trying to run the vllm.entrypoints.openai.api_server with the CohereForCausalLM model and LoRA enabled.\r\n\r\n**Here are the details:**\r\n\r\n* **Model**: c4ai-command-r-plus\r\n* **Command**\r\n`CUDA_VISIBLE_DEVICES=4,5,6,7 python -m vllm.entrypoints.openai.api_server --model c4ai-command-r-plus --enable-lora --lora-modules test-lora=xxx --tensor-parallel-size 4 --gpu-memory-utilization 0.9`\r\n\r\n* **Error message:**\r\n\r\n> ValueError: Model CohereForCausalLM does not support LoRA, but LoRA is enabled. Support for this model may be added in the future.\r\n \r\n* **Command (without LoRA)Ôºö** the server runs successfully without any errors.\r\n`CUDA_VISIBLE_DEVICES=4,5,6,7 python -m vllm.entrypoints.openai.api_server --model c4ai-command-r-plus --tensor-parallel-size 4 --gpu-memory-utilization 0.9`\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-05-10T13:42:08Z",
    "closed_at": "2024-06-18T18:01:23Z",
    "author": "onlyfish79",
    "comments_count": 0,
    "comments": []
  },
  "https://github.com/vllm-project/vllm/issues/4756": {
    "issue_number": 4756,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4756",
    "title": "[Bug]: CUDA error when running mistral-7b + lora with tensor_para=8",
    "body": "### Your current environment\n\n```\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.28.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.213-201.855.amzn2.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.99\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration:\r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 535.161.08\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.0.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.0.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz\r\nCPU family:                         6\r\nModel:                              85\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nStepping:                           7\r\nBogoMIPS:                           5999.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          1.5 MiB (48 instances)\r\nL1i cache:                          1.5 MiB (48 instances)\r\nL2 cache:                           48 MiB (48 instances)\r\nL3 cache:                           71.5 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-23,48-71\r\nNUMA node1 CPU(s):                  24-47,72-95\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        KVM: Mitigation: VMX unsupported\r\nVulnerability L1tf:                 Mitigation; PTE Inversion\r\nVulnerability Mds:                  Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Meltdown:             Mitigation; PTI\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Vulnerable\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Vulnerable\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, STIBP disabled, RSB filling\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.15.0rc2\r\n[pip3] optree==0.10.0\r\n[pip3] pytorch-quantization==2.1.2\r\n[pip3] pytorch-triton==2.2.0+e28a256d7\r\n[pip3] torch==2.3.0\r\n[pip3] torch-tensorrt==2.3.0a0\r\n[pip3] torchdata==0.7.1a0\r\n[pip3] torchtext==0.17.0a0\r\n[pip3] torchvision==0.18.0a0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.1\r\nvLLM Build Flags:\r\nCUDA Archs: 5.2 6.0 6.1 7.0 7.2 7.5 8.0 8.6 8.7 9.0+PTX; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tGPU1\tGPU2\tGPU3\tGPU4\tGPU5\tGPU6\tGPU7\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t0-23,48-71\t0\t\tN/A\r\nGPU1\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t0-23,48-71\t0\t\tN/A\r\nGPU2\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\tNV12\t0-23,48-71\t0\t\tN/A\r\nGPU3\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\tNV12\t0-23,48-71\t0\t\tN/A\r\nGPU4\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\tNV12\t24-47,72-95\t1\t\tN/A\r\nGPU5\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\tNV12\t24-47,72-95\t1\t\tN/A\r\nGPU6\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \tNV12\t24-47,72-95\t1\t\tN/A\r\nGPU7\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\tNV12\t X \t24-47,72-95\t1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\n\n### üêõ Describe the bug\n\nWhen running below, enable lora for mistral-7b model with tensor parall=8, will throw cuda error. Full log is [here](https://gist.github.com/sfc-gh-zhwang/6e078b1621228cf9155a290cf268640c)\r\n```\r\nfrom vllm import LLM, SamplingParams\r\n\r\nllm = LLM(\r\n    model=\"/models/mistral-7b\",\r\n    enable_lora=True,\r\n    tensor_parallel_size=8,\r\n)\r\n```",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-05-11T05:55:43Z",
    "closed_at": "2024-11-27T02:08:42Z",
    "author": "sfc-gh-zhwang",
    "comments_count": 8,
    "comments": [
      {
        "author": "mgoin",
        "body": "Hi @sfc-gh-zhwang, FWIW I was able to run this with TP=2 on 2xA6000 using `vllm==0.4.2`.\r\n\r\n```python\r\nfrom vllm import LLM\r\n\r\nllm = LLM(\r\n    model=\"mistralai/Mistral-7B-Instruct-v0.2\",\r\n    enable_lora=True,\r\n    tensor_parallel_size=2,\r\n)\r\n\r\nprint(llm.generate(\"Hello\"))\r\n```\r\n\r\nOutput:\r\n```\r\n2024-05-22 06:43:47,998 INFO worker.py:1749 -- Started a local Ray instance.\r\nINFO 05-22 06:43:49 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='mistralai/Mistral-7B-Instruct-v0.2', speculative_config=None, tokenizer='mistralai/Mistral-7B-Instruct-v0.2', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=mistralai/Mistral-7B-Instruct-v0.2)\r\nINFO 05-22 06:43:53 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:43:53 utils.py:660] Found nccl from library /root/.config/vllm/nccl/cu12/libnccl.so.2.18.1\r\nINFO 05-22 06:43:54 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\r\nINFO 05-22 06:43:54 selector.py:32] Using XFormers backend.\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:43:54 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:43:54 selector.py:32] Using XFormers backend.\r\nINFO 05-22 06:43:56 pynccl_utils.py:43] vLLM is using nccl==2.18.1\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:43:56 pynccl_utils.py:43] vLLM is using nccl==2.18.1\r\nINFO 05-22 06:43:57 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:43:57 utils.py:132] reading GPU P2P access cache from /root/.config/vllm/gpu_p2p_access_cache_for_0,1.json\r\nINFO 05-22 06:43:58 weight_utils.py:199] Using model weights format ['*.safetensors']\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:43:58 weight_utils.py:199] Using model weights format ['*.safetensors']\r\nINFO 05-22 06:44:00 model_runner.py:175] Loading model weights took 6.7544 GB\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:44:01 model_runner.py:175] Loading model weights took 6.7544 GB\r\nINFO 05-22 06:44:07 distributed_gpu_executor.py:45] # GPU blocks: 33401, # CPU blocks: 4096\r\nINFO 05-22 06:44:08 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\nINFO 05-22 06:44:08 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:44:08 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:44:08 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\r\nINFO 05-22 06:44:13 custom_all_reduce.py:246] Registering 2275 cuda graph addresses\r\nINFO 05-22 06:44:13 model_runner.py:1017] Graph capturing finished in 5 secs.\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:44:13 custom_all_reduce.py:246] Registering 2275 cuda graph addresses\r\n(RayWorkerWrapper pid=523946) INFO 05-22 06:44:13 model_runner.py:1017] Graph capturing finished in 5 secs.\r\nProcessed prompts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.56it/s]\r\n[RequestOutput(request_id=0, prompt='Hello', prompt_token_ids=[1, 22557], prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=\", I'm quite new to Linux so I'm really sorry if this\", token_ids=[28725, 315, 28742, 28719, 3448, 633, 298, 19486, 579, 315, 28742, 28719, 1528, 7371, 513, 456], cumulative_logprob=-27.695576645433903, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=RequestMetrics(arrival_time=1716360253.5880315, last_token_time=1716360253.5880315, first_scheduled_time=1716360253.5906928, first_token_time=1716360253.6384084, time_in_queue=0.0026612281799316406, finished_time=1716360253.8712075), lora_request=None)]\r\n```",
        "created_at": "2024-05-22T06:44:45Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "@mgoin it's just tp=8 doesn't work.",
        "created_at": "2024-05-22T15:35:52Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "@FurtherAI in case you have some idea üòÉ ",
        "created_at": "2024-05-23T06:02:17Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "Further narrow down to this [line](https://github.com/vllm-project/vllm/blob/ee3eea0a1b2c690557455d97074d8829d5a98320/vllm/lora/punica.py#L138)\r\n```\r\npunica_kernels.dispatch_bgmv(buffer, x, wa_t_all, indicies, layer_idx, 1.0)\r\n```\r\nwhere, for tp=8 (error out), the tensor sizes are:\r\n```\r\nbuffer: [32768, 16]\r\nx: [32768, 512]\r\nwa_t_all: [1, 1, 16, 512]\r\n```\r\nwhile for tp=4 (working), the tensor sizes are \r\n```\r\nbuffer: [32768, 16]\r\nx: [32768, 1024]\r\nwa_t_all: [1, 1, 16, 1024]\r\n```\r\nStill trying to figure out what is the magic around 1024 -> 512",
        "created_at": "2024-05-23T07:57:48Z"
      },
      {
        "author": "FurtherAI",
        "body": "Tracked it a little further. Seems to be due to the sequence length. Not sure why, from a brief glance, the kernel shouldn't care about the sequence length. I found 65536 to work, 32768 and 16384 to not work and 8192, 4096 to work and didn't test more. So for now, @sfc-gh-zhwang, run with a different seq length.\r\n\r\nHere's some code to reproduce:\r\n```python\r\nimport vllm._punica_C as punica_kernels\r\nseq_length, rank = 32768, 16\r\nbuffer = torch.randn((seq_length, rank), device='cuda', dtype=torch.float32)\r\nx = torch.randn((seq_length, 512), device='cuda', dtype=torch.bfloat16)\r\nwa_t_all = torch.randn((1, 1, rank, 512), device='cuda', dtype=torch.bfloat16)\r\nindicies = torch.full((seq_length,), 1, device='cuda', dtype=torch.int64)\r\npunica_kernels.dispatch_bgmv(buffer, x, wa_t_all, indicies, 0, 1.0)\r\n\r\ntorch.cuda.synchronize()\r\n```",
        "created_at": "2024-05-23T16:47:27Z"
      },
      {
        "author": "sfc-gh-zhwang",
        "body": "I think I found the root cause, basically the code [here](https://github.com/vllm-project/vllm/blob/a377f0bd5e1fa0ca069e3dbf28f4de5af64d0bb1/csrc/punica/bgmv/bgmv_impl.cuh#L72-L75) will overflow X for certain tensor shapes. I think the solution should be adding a condition like `if (threadIdx.y * tx * vec_size < feat_in)`.\r\nBut I think we should fold this into [line 84](https://github.com/vllm-project/vllm/blob/a377f0bd5e1fa0ca069e3dbf28f4de5af64d0bb1/csrc/punica/bgmv/bgmv_impl.cuh#L84) and just change `for (tile_idx = 1;` -> `for (tile_idx = 0;`? ROCM is doing this anyway: https://github.com/vllm-project/vllm/blob/a377f0bd5e1fa0ca069e3dbf28f4de5af64d0bb1/csrc/punica/bgmv/bgmv_impl.cuh#L196",
        "created_at": "2024-05-31T15:42:06Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-27T02:06:31Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-27T02:08:42Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/4789": {
    "issue_number": 4789,
    "issue_url": "https://github.com/vllm-project/vllm/issues/4789",
    "title": "[Bug]: Async engine hangs with 0.4.* releases",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1020-oem-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA RTX 6000 Ada Generation\r\nNvidia driver version: 545.29.06\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          GenuineIntel\r\nModel name:                         13th Gen Intel(R) Core(TM) i9-13900K\r\nCPU family:                         6\r\nModel:                              183\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          1\r\nStepping:                           1\r\nCPU max MHz:                        5800.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           5990.40\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb intel_pt sha_ni xsaveopt xsavec xgetbv1 xsaves split_lock_detect avx_vnni dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req hfi vnmi umip pku ospke waitpkg gfni vaes vpclmulqdq tme rdpid movdiri movdir64b fsrm md_clear serialize pconfig arch_lbr ibt flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          896 KiB (24 instances)\r\nL1i cache:                          1.3 MiB (24 instances)\r\nL2 cache:                           32 MiB (12 instances)\r\nL3 cache:                           36 MiB (1 instance)\r\nNUMA node(s):                       1\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[pip3] vllm-nccl-cu12==2.18.1.0.4.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.2\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\n\u001b[4mGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\u001b[0m\r\nGPU0\t X \t0-31\t0\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n```\r\n\n\n### üêõ Describe the bug\n\nThe code below worked fine with vllm==0.3.3 (I could see the generated output printed to the console).\r\nHowever, when I try to use it with 0.4.0, 0.4.1 or 0.4.2 it hangs.\r\n```python\r\nimport argparse\r\nimport asyncio\r\n\r\nfrom vllm.engine.arg_utils import AsyncEngineArgs\r\nfrom vllm.engine.async_llm_engine import AsyncLLMEngine, AsyncStream\r\nfrom vllm.sampling_params import SamplingParams\r\n\r\n\r\ndef parse_args():\r\n    parser = argparse.ArgumentParser()\r\n    parser = AsyncEngineArgs.add_cli_args(parser)\r\n    return parser.parse_args()\r\n\r\n\r\nasync def iterate_over_output_for_one_prompt(output_iterator: AsyncStream) -> str:\r\n    last_text = \"\"\r\n    prompt = \"???\"\r\n\r\n    async for output in output_iterator:\r\n        prompt = output.prompt\r\n        last_text = output.outputs[0].text\r\n\r\n    return prompt + last_text\r\n\r\n\r\nasync def generate(engine: AsyncLLMEngine, prompts: list[str]) -> list[str]:\r\n    sampling_params = SamplingParams(n=1, max_tokens=100, ignore_eos=True)\r\n    output_iterators = [await engine.add_request(f\"req_{i}\", prompt, sampling_params)\r\n                        for i, prompt in enumerate(prompts)]\r\n    outputs = await asyncio.gather(*[iterate_over_output_for_one_prompt(output_iterator)\r\n                                     for output_iterator in output_iterators])\r\n    return list(outputs)\r\n\r\n\r\ndef main():\r\n    args = parse_args()\r\n    engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs.from_cli_args(args))\r\n    prompts = [\"I've never been to\", \"I would like to see\"]\r\n    outputs = asyncio.run(generate(engine, prompts))\r\n    for output in outputs:\r\n        print(output)\r\n    print(\"FINISHED\")\r\n\r\n\r\nif __name__ == '__main__':\r\n    main()\r\n```\r\nThe last logs before iterrupting the execution are\r\n```\r\nINFO 05-13 14:28:39 async_llm_engine.py:120] Finished request req_0.\r\nINFO 05-13 14:28:39 async_llm_engine.py:120] Finished request req_1.\r\n```\r\nAnd after finally pressing Ctrl+C I get\r\n```Traceback (most recent call last):\r\n  File \"/workdir/repo/llmvpr/vllm/minimal.py\", line 46, in <module>\r\n    main()\r\n  File \"/workdir/repo/llmvpr/vllm/minimal.py\", line 39, in main\r\n    outputs = asyncio.run(generate(engine, prompts))\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 47, in run\r\n    _cancel_all_tasks(loop)\r\n  File \"/usr/lib/python3.10/asyncio/runners.py\", line 63, in _cancel_all_tasks\r\n    loop.run_until_complete(tasks.gather(*to_cancel, return_exceptions=True))\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 636, in run_until_complete\r\n    self.run_forever()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\r\n    self._run_once()\r\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1871, in _run_once\r\n    event_list = self._selector.select(timeout)\r\n  File \"/usr/lib/python3.10/selectors.py\", line 469, in select\r\n    fd_event_list = self._selector.poll(timeout, max_ev)\r\nKeyboardInterrupt\r\nTask was destroyed but it is pending!\r\ntask: <Task pending name='Task-2' coro=<AsyncLLMEngine.run_engine_loop() running at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:490> wait_for=<Future pending cb=[Task.task_wakeup()]> cb=[_raise_exception_on_finish(error_callback=<bound method...7851eba052d0>>)() at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:30, <1 more>, gather.<locals>._done_callback() at /usr/lib/python3.10/asyncio/tasks.py:720]>\r\n```\r\nIs there something wrong with my code or is it a bug in `vllm`?",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-05-13T14:32:05Z",
    "closed_at": "2024-06-19T20:57:13Z",
    "author": "glos-nv",
    "comments_count": 9,
    "comments": [
      {
        "author": "youkaichao",
        "body": "> If you experienced crashes or hangs, it would be helpful to run vllm with export VLLM_TRACE_FUNCTION=1 . All the function calls in vllm will be recorded. Inspect these log files, and tell which function crashes or hangs.\r\n\r\nQuote from issue templates.",
        "created_at": "2024-05-13T15:54:48Z"
      },
      {
        "author": "glos-nv",
        "body": "[vllm_trace_function.log](https://github.com/vllm-project/vllm/files/15305960/vllm_trace_function.log)\r\nThe requested log file is too large to send in its entirety, but I am attaching the last 10MB.",
        "created_at": "2024-05-14T09:10:32Z"
      },
      {
        "author": "simon-mo",
        "body": "When you initialize the async engine I think it expects to be in an running event loop, not sure why though.\r\n\r\nIf you change the code to \r\n```diff\r\ndiff --git a/original.py b/repro.py\r\nindex 37ede83..8881b6f 100644\r\n--- a/original.py\r\n+++ b/repro.py\r\n@@ -32,15 +32,15 @@ async def generate(engine: AsyncLLMEngine, prompts: list[str]) -> list[str]:\r\n     return list(outputs)\r\n\r\n\r\n-def main():\r\n+async def main():\r\n     args = parse_args()\r\n     engine = AsyncLLMEngine.from_engine_args(AsyncEngineArgs.from_cli_args(args))\r\n     prompts = [\"I've never been to\", \"I would like to see\"]\r\n-    outputs = asyncio.run(generate(engine, prompts))\r\n+    outputs = await generate(engine, prompts)\r\n     for output in outputs:\r\n         print(output)\r\n     print(\"FINISHED\")\r\n\r\n\r\n if __name__ == '__main__':\r\n-    main()\r\n+    asyncio.run(main())\r\n```\r\n\r\nThe prints should succeed. \r\n\r\nHowever, the script still will not exit. So there's still a bug. \r\n\r\n",
        "created_at": "2024-05-15T00:13:54Z"
      },
      {
        "author": "zifeitong",
        "body": "Saw the same issue. I tested and the culprit commit should be #3015. It's not clear to me what's the root cause though.",
        "created_at": "2024-05-21T02:01:44Z"
      },
      {
        "author": "zifeitong",
        "body": "So the problem is that https://github.com/vllm-project/vllm/blob/97b030005c7f5cde7c1b97c718a8841db7d6220b/vllm/engine/async_llm_engine.py#L509 triggered https://github.com/python/cpython/issues/86296\r\n\r\nThe bug is fixed in Python 3.12 but will not be backported. It's very easy to workaround it in Python 3.11 with `asyncio.timeout`. However, the workaround for Python 3.8 - 3.10 is not straightforward.",
        "created_at": "2024-05-22T21:13:55Z"
      },
      {
        "author": "donglinz",
        "body": "@zifeitong I am facing exactly the same issue.\r\n\r\nSetting engine_use_ray and worker_use_ray to True can help me walk around this issue.",
        "created_at": "2024-06-13T14:44:57Z"
      },
      {
        "author": "Dineshkumar-Anandan-ZS0367",
        "body": "How to run any Vision Multi modal using this Async engine like \"Qwen2-VL-7B\"\n\nis it possible? Could you please share any reference on this.",
        "created_at": "2025-02-27T23:09:34Z"
      },
      {
        "author": "BrokenArrow1404",
        "body": "> How to run any Vision Multi modal using this Async engine like \"Qwen2-VL-7B\"\n> \n> is it possible? Could you please share any reference on this.\n\nsame problem",
        "created_at": "2025-04-22T06:42:03Z"
      },
      {
        "author": "DarkLight1337",
        "body": "This isn't really related to the original issue, can you open a new one?",
        "created_at": "2025-04-22T06:48:16Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5088": {
    "issue_number": 5088,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5088",
    "title": "[Bug]: Gemma model fails with GPTQ marlin",
    "body": "\r\n\r\n### üêõ Describe the bug\r\n\r\nUsing docker and gemma finetuned model \r\n\r\n--model /data/merged_model_GPTQ                     --max-model-len 8192                     --max-num-seqs 1024                     --served-model-name model --quantization gptq_marlin\r\n\r\nfails with \r\n\r\nRuntimeError: Some weights are not initialized from checkpoints: {'model.layers.3.mlp.gate_up_proj.g_idx_sort_indices', 'model.layers.8.self_attn.qkv_proj.g_idx_sort_indices', 'model.layers.9.mlp.gate_up_proj.g_idx_sort_indices .....\r\n\r\nThe same works with  --quantization gptq",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-05-28T13:55:47Z",
    "closed_at": "2024-06-04T23:13:58Z",
    "author": "arunpatala",
    "comments_count": 6,
    "comments": [
      {
        "author": "robertgshaw2-redhat",
        "body": "> ### üêõ Describe the bug\r\n> Using docker and gemma finetuned model\r\n> \r\n> --model /data/merged_model_GPTQ --max-model-len 8192 --max-num-seqs 1024 --served-model-name model --quantization gptq_marlin\r\n> \r\n> fails with\r\n> \r\n> RuntimeError: Some weights are not initialized from checkpoints: {'model.layers.3.mlp.gate_up_proj.g_idx_sort_indices', 'model.layers.8.self_attn.qkv_proj.g_idx_sort_indices', 'model.layers.9.mlp.gate_up_proj.g_idx_sort_indices .....\r\n> \r\n> The same works with --quantization gptq\r\n\r\n@alexm-neuralmagic \r\n\r\n@arunpatala Can you share the model checkpoint so we can take a look?",
        "created_at": "2024-05-28T14:53:42Z"
      },
      {
        "author": "arunpatala",
        "body": "    volume=$HF_HOME\r\n    docker run --runtime nvidia --gpus all \\\r\n        -v $volume:/root/.cache/huggingface \\\r\n        -p 8000:8000 \\\r\n        --ipc=host \\\r\n        vllm/vllm-openai:latest \\\r\n        --model TechxGenus/gemma-1.1-2b-it-GPTQ \\\r\n        --max-model-len 8192 \\\r\n        --max-num-seqs 32 \\\r\n        --quantization gptq_marlin\r\n\r\nYou can check with this public model. The engine throws error when using either gptq_marlin or marlin as quantization.\r\nRuntimeError: Some weights are not initialized from checkpoints: {'model.layers.6.self_attn.qkv_proj.g_idx_sort_indices',\r\n\r\nThe same model works with gptq as quantization.\r\n        \r\n    volume=$HF_HOME\r\n    docker run --runtime nvidia --gpus all \\\r\n        -v $volume:/root/.cache/huggingface \\\r\n        -p 8000:8000 \\\r\n        --ipc=host \\\r\n        vllm/vllm-openai:latest \\\r\n        --model TechxGenus/gemma-1.1-2b-it-GPTQ \\\r\n        --max-model-len 8192 \\\r\n        --max-num-seqs 32 \\\r\n        --quantization gptq",
        "created_at": "2024-05-29T06:26:24Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "Thanks\r\n\r\n@alexm-neuralmagic can you take a look?",
        "created_at": "2024-05-29T13:31:25Z"
      },
      {
        "author": "alexm-redhat",
        "body": "@arunpatala Here is the fix https://github.com/vllm-project/vllm/pull/5108 (will land soon)",
        "created_at": "2024-05-29T18:05:01Z"
      },
      {
        "author": "arunpatala",
        "body": "thanks a lot.",
        "created_at": "2024-05-29T19:32:51Z"
      },
      {
        "author": "alexm-redhat",
        "body": "no problem",
        "created_at": "2024-05-29T19:45:56Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5152": {
    "issue_number": 5152,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5152",
    "title": "[Bug] [spec decode] [flash_attn]: CUDA illegal memory access when calling flash_attn_cuda.fwd_kvcache",
    "body": "### My environment setup\r\n1st environment (running on ec2 `g6.4xlarge`)\r\n```\r\n[2024-06-01T10:14:23Z] Collecting environment information...\r\n[2024-06-01T10:14:26Z] PyTorch version: 2.3.0+cu121\r\n[2024-06-01T10:14:26Z] Is debug build: False\r\n[2024-06-01T10:14:26Z] CUDA used to build PyTorch: 12.1\r\n[2024-06-01T10:14:26Z] ROCM used to build PyTorch: N/A\r\n[2024-06-01T10:14:26Z]\r\n[2024-06-01T10:14:26Z] OS: Ubuntu 22.04.4 LTS (x86_64)\r\n[2024-06-01T10:14:26Z] GCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\n[2024-06-01T10:14:26Z] Clang version: Could not collect\r\n[2024-06-01T10:14:26Z] CMake version: version 3.29.3\r\n[2024-06-01T10:14:26Z] Libc version: glibc-2.35\r\n[2024-06-01T10:14:26Z]\r\n[2024-06-01T10:14:26Z] Python version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\n[2024-06-01T10:14:26Z] Python platform: Linux-6.1.90-99.173.amzn2023.x86_64-x86_64-with-glibc2.35\r\n[2024-06-01T10:14:26Z] Is CUDA available: True\r\n[2024-06-01T10:14:26Z] CUDA runtime version: Could not collect\r\n[2024-06-01T10:14:26Z] CUDA_MODULE_LOADING set to: LAZY\r\n[2024-06-01T10:14:26Z] GPU models and configuration: GPU 0: NVIDIA L4\r\n[2024-06-01T10:14:26Z] Nvidia driver version: 525.147.05\r\n[2024-06-01T10:14:26Z] cuDNN version: Could not collect\r\n[2024-06-01T10:14:26Z] HIP runtime version: N/A\r\n[2024-06-01T10:14:26Z] MIOpen runtime version: N/A\r\n[2024-06-01T10:14:26Z] Is XNNPACK available: True\r\n[2024-06-01T10:14:26Z]\r\n[2024-06-01T10:14:26Z] CPU:\r\n[2024-06-01T10:14:26Z] Architecture:                         x86_64\r\n[2024-06-01T10:14:26Z] CPU op-mode(s):                       32-bit, 64-bit\r\n[2024-06-01T10:14:26Z] Address sizes:                        48 bits physical, 48 bits virtual\r\n[2024-06-01T10:14:26Z] Byte Order:                           Little Endian\r\n[2024-06-01T10:14:26Z] CPU(s):                               16\r\n[2024-06-01T10:14:26Z] On-line CPU(s) list:                  0-15\r\n[2024-06-01T10:14:26Z] Vendor ID:                            AuthenticAMD\r\n[2024-06-01T10:14:26Z] Model name:                           AMD EPYC 7R13 Processor\r\n[2024-06-01T10:14:26Z] CPU family:                           25\r\n[2024-06-01T10:14:26Z] Model:                                1\r\n[2024-06-01T10:14:26Z] Thread(s) per core:                   2\r\n[2024-06-01T10:14:26Z] Core(s) per socket:                   8\r\n[2024-06-01T10:14:26Z] Socket(s):                            1\r\n[2024-06-01T10:14:26Z] Stepping:                             1\r\n[2024-06-01T10:14:26Z] BogoMIPS:                             5299.99\r\n[2024-06-01T10:14:26Z] Flags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm cmp_legacy cr8_legacy abm sse4a misalignsse 3dnowprefetch topoext invpcid_single ssbd ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 clzero xsaveerptr rdpru wbnoinvd arat npt nrip_save vaes vpclmulqdq rdpid\r\n[2024-06-01T10:14:26Z] Hypervisor vendor:                    KVM\r\n[2024-06-01T10:14:26Z] Virtualization type:                  full\r\n[2024-06-01T10:14:26Z] L1d cache:                            256 KiB (8 instances)\r\n[2024-06-01T10:14:26Z] L1i cache:                            256 KiB (8 instances)\r\n[2024-06-01T10:14:26Z] L2 cache:                             4 MiB (8 instances)\r\n[2024-06-01T10:14:26Z] L3 cache:                             32 MiB (1 instance)\r\n[2024-06-01T10:14:26Z] NUMA node(s):                         1\r\n[2024-06-01T10:14:26Z] NUMA node0 CPU(s):                    0-15\r\n[2024-06-01T10:14:26Z] Vulnerability Gather data sampling:   Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Itlb multihit:          Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability L1tf:                   Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Mds:                    Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Meltdown:               Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Mmio stale data:        Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Reg file data sampling: Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Retbleed:               Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Spec rstack overflow:   Mitigation; safe RET, no microcode\r\n[2024-06-01T10:14:26Z] Vulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl\r\n[2024-06-01T10:14:26Z] Vulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\n[2024-06-01T10:14:26Z] Vulnerability Spectre v2:             Mitigation; Retpolines; IBPB conditional; IBRS_FW; STIBP always-on; RSB filling; PBRSB-eIBRS Not affected; BHI Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Srbds:                  Not affected\r\n[2024-06-01T10:14:26Z] Vulnerability Tsx async abort:        Not affected\r\n[2024-06-01T10:14:26Z]\r\n[2024-06-01T10:14:26Z] Versions of relevant libraries:\r\n[2024-06-01T10:14:26Z] [pip3] mypy==1.9.0\r\n[2024-06-01T10:14:26Z] [pip3] mypy-extensions==1.0.0\r\n[2024-06-01T10:14:26Z] [pip3] numpy==1.26.4\r\n[2024-06-01T10:14:26Z] [pip3] nvidia-nccl-cu12==2.20.5\r\n[2024-06-01T10:14:26Z] [pip3] torch==2.3.0\r\n[2024-06-01T10:14:26Z] [pip3] triton==2.3.0\r\n[2024-06-01T10:14:26Z] [conda] Could not collectROCM Version: Could not collect\r\n[2024-06-01T10:14:26Z] Neuron SDK Version: N/A\r\n[2024-06-01T10:14:26Z] vLLM Version: 0.4.3\r\n[2024-06-01T10:14:26Z] vLLM Build Flags:\r\n[2024-06-01T10:14:26Z] CUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\n[2024-06-01T10:14:26Z] GPU Topology:\r\n[2024-06-01T10:14:26Z] GPU0\tCPU Affinity\tNUMA Affinity\r\n[2024-06-01T10:14:26Z] GPU0\t X \t0-15\t\tN/A\r\n```\r\n\r\n2nd environment (running on GCP `g2-standard-12`):\r\n```\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.0-29-cloud-amd64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L4\r\nNvidia driver version: 550.54.15\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                         x86_64\r\nCPU op-mode(s):                       32-bit, 64-bit\r\nAddress sizes:                        46 bits physical, 48 bits virtual\r\nByte Order:                           Little Endian\r\nCPU(s):                               12\r\nOn-line CPU(s) list:                  0-11\r\nVendor ID:                            GenuineIntel\r\nModel name:                           Intel(R) Xeon(R) CPU @ 2.20GHz\r\nCPU family:                           6\r\nModel:                                85\r\nThread(s) per core:                   2\r\nCore(s) per socket:                   6\r\nSocket(s):                            1\r\nStepping:                             7\r\nBogoMIPS:                             4400.45\r\nFlags:                                fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\r\nHypervisor vendor:                    KVM\r\nVirtualization type:                  full\r\nL1d cache:                            192 KiB (6 instances)\r\nL1i cache:                            192 KiB (6 instances)\r\nL2 cache:                             6 MiB (6 instances)\r\nL3 cache:                             38.5 MiB (1 instance)\r\nNUMA node(s):                         1\r\nNUMA node0 CPU(s):                    0-11\r\nVulnerability Gather data sampling:   Not affected\r\nVulnerability Itlb multihit:          Not affected\r\nVulnerability L1tf:                   Not affected\r\nVulnerability Mds:                    Mitigation; Clear CPU buffers; SMT Host state unknown\r\nVulnerability Meltdown:               Not affected\r\nVulnerability Mmio stale data:        Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Reg file data sampling: Not affected\r\nVulnerability Retbleed:               Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow:   Not affected\r\nVulnerability Spec store bypass:      Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:             Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:             Mitigation; Enhanced / Automatic IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                  Not affected\r\nVulnerability Tsx async abort:        Mitigation; Clear CPU buffers; SMT Host state unknown\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] triton==2.3.0\r\n[conda] Could not collectROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.4.3\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-11    0               N/A\r\n```\r\n\r\n- How to repro:\r\n  - Build vLLM Docker image from newly cloned vllm repo: `docker build --build-arg max_jobs=16 --tag vllm --target test .`\r\n  - Run the test in Docker container.\r\n  - `docker run -it --rm --gpus all vllm bash -c \"cd /vllm-workspace/tests && pytest -v -s spec_decode\"`\r\n\r\n\r\n\r\n### üêõ Describe the bug\r\n- Nothing changes in the tests/relevant code. The only difference is it's running in a different machine/environment compared to vLLM CI. I listed 2 environments which I tried and both failed.\r\n\r\n- The error showed when running this test in `tests/spec_decode/e2e/test_multistep_correctness.py`:\r\n\r\n- Test name is `test_spec_decode_e2e_greedy_correctness_tiny_model_large_bs_diff_output_len[1-32-256-test_llm_kwargs0-baseline_llm_kwargs0-per_test_common_llm_kwargs1-common_llm_kwargs0]` \r\n\r\n- kwargs={'enforce_eager': True, 'use_v2_block_manager': True, 'model': 'JackFram/llama-160m', 'speculative_model': 'JackFram/llama-68m', 'num_speculative_tokens': 5}\r\n\r\n- Failure message and stack trace starts here: https://buildkite.com/vllm/ci-aws/builds/82#018fcb54-3ae6-4a96-8e2a-67c66814003d/184-356\r\n- The error happens when `flash_attn_cuda.fwd_kvcache` is called in `/attention/backends/flash_attn.py` \r\n\r\n- Running the test with `VLLM_ATTENTION_BACKEND=XFORMERS` passes. Could this bug be related to flash attention?",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-05-31T09:00:45Z",
    "closed_at": "2024-10-01T00:51:41Z",
    "author": "khluu",
    "comments_count": 6,
    "comments": [
      {
        "author": "DeJoker",
        "body": "same problem happen to me. Is this bug in progress?",
        "created_at": "2024-06-18T08:10:33Z"
      },
      {
        "author": "khluu",
        "body": "@DeJoker  do you also see it in unit test or other places? How are you running it?",
        "created_at": "2024-06-18T08:29:56Z"
      },
      {
        "author": "khluu",
        "body": "This issue on Spec decoding tests should be fixed already ",
        "created_at": "2024-06-18T08:30:38Z"
      },
      {
        "author": "DeJoker",
        "body": "@khluu I don't have a demo right now that can at least reproduce the problem. \r\nJust same issue with `flash_attn_cuda.fwd_kvcache`.\r\nthe situation is vllm start in nvidia triton server(nvcr.io/nvidia/tritonserver:24.05-vllm-python-py3), then send request directly with grpc client\r\n\r\nMy environment setupÔºö\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: 14.0.0-1ubuntu1.1\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.10.134-13.1.al8.x86_64-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.4.131\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-80GB\r\nGPU 1: NVIDIA A100-SXM4-80GB\r\nGPU 2: NVIDIA A100-SXM4-80GB\r\nGPU 3: NVIDIA A100-SXM4-80GB\r\nGPU 4: NVIDIA A100-SXM4-80GB\r\nGPU 5: NVIDIA A100-SXM4-80GB\r\nGPU 6: NVIDIA A100-SXM4-80GB\r\nGPU 7: NVIDIA A100-SXM4-80GB\r\n\r\nNvidia driver version: 530.30.02\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_precompiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_engines_runtime_compiled.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_graph.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_heuristic.so.9.1.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops.so.9.1.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                    x86_64\r\nCPU op-mode(s):                  32-bit, 64-bit\r\nAddress sizes:                   46 bits physical, 48 bits virtual\r\nByte Order:                      Little Endian\r\nCPU(s):                          128\r\nOn-line CPU(s) list:             0-127\r\nVendor ID:                       GenuineIntel\r\nModel name:                      Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz\r\nCPU family:                      6\r\nModel:                           106\r\nThread(s) per core:              2\r\nCore(s) per socket:              32\r\nSocket(s):                       2\r\nStepping:                        6\r\nBogoMIPS:                        5800.00\r\nFlags:                           fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch cpuid_fault invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves wbnoinvd arat avx512vbmi avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm arch_capabilities\r\nHypervisor vendor:               KVM\r\nVirtualization type:             full\r\nL1d cache:                       3 MiB (64 instances)\r\nL1i cache:                       2 MiB (64 instances)\r\nL2 cache:                        80 MiB (64 instances)\r\nL3 cache:                        96 MiB (2 instances)\r\nNUMA node(s):                    2\r\nNUMA node0 CPU(s):               0-63\r\nNUMA node1 CPU(s):               64-127\r\nVulnerability Itlb multihit:     Not affected\r\nVulnerability L1tf:              Not affected\r\nVulnerability Mds:               Not affected\r\nVulnerability Meltdown:          Not affected\r\nVulnerability Mmio stale data:   Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:          Not affected\r\nVulnerability Spec store bypass: Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:        Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:        Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:             Not affected\r\nVulnerability Tsx async abort:   Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.0\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    0-127   0-1\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    0-127   0-1\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    0-127   0-1\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    0-127   0-1\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    0-127   0-1\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    0-127   0-1\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    0-127   0-1\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      0-127   0-1\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\nerror message with:\r\n```text\r\nINFO 06-18 08:04:36 metrics.py:341] Avg prompt throughput: 17673.7 tokens/s, Avg generation throughput: 204.0 tokens/s, Running: 233 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.8%, CPU KV cache usage: 0.0%.\r\nINFO 06-18 08:04:41 metrics.py:341] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 313.0 tokens/s, Running: 190 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 7.4%, CPU KV cache usage: 0.0%.\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] Engine background task failed\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] Traceback (most recent call last):\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return_value = task.result()\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     has_requests_in_progress = await asyncio.wait_for(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return fut.result()\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     request_outputs = await self.engine.step_async()\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     output = await self.model_executor.execute_model_async(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 117, in execute_model_async\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     output = await make_async(self.driver_worker.execute_model\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 280, in execute_model\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     output = self.model_runner.execute_model(seq_group_metadata_list,\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return func(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 749, in execute_model\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     hidden_states = model_executable(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 330, in forward\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     hidden_states = self.model(input_ids, positions, kv_caches,\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 254, in forward\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     hidden_states, residual = layer(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 206, in forward\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     hidden_states = self.self_attn(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 153, in forward\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return self._call_impl(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return forward_call(*args, **kwargs)\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 89, in forward\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/flash_attn.py\", line 355, in forward\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     output[num_prefill_tokens:] = flash_attn_with_kvcache(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]   File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\", line 1233, in flash_attn_with_kvcache\r\nERROR 06-18 08:04:44 async_llm_engine.py:52]     out, softmax_lse = flash_attn_cuda.fwd_kvcache(\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] RuntimeError: CUDA error: an illegal memory access was encountered\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] For debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\nERROR 06-18 08:04:44 async_llm_engine.py:52] \r\nException in callback _log_task_completion(error_callback=<bound method...7eff2e47e500>>)(<Task finishe...sertions.\\n')>) at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:32\r\nhandle: <Handle _log_task_completion(error_callback=<bound method...7eff2e47e500>>)(<Task finishe...sertions.\\n')>) at /usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py:32>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 42, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 532, in run_engine_loop\r\n    has_requests_in_progress = await asyncio.wait_for(\r\n  File \"/usr/lib/python3.10/asyncio/tasks.py\", line 445, in wait_for\r\n    return fut.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 506, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 235, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/gpu_executor.py\", line 117, in execute_model_async\r\n    output = await make_async(self.driver_worker.execute_model\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker.py\", line 280, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 749, in execute_model\r\n    hidden_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 330, in forward\r\n    hidden_states = self.model(input_ids, positions, kv_caches,\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 254, in forward\r\n    hidden_states, residual = layer(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 206, in forward\r\n    hidden_states = self.self_attn(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/qwen2.py\", line 153, in forward\r\n    attn_output = self.attn(q, k, v, kv_cache, attn_metadata)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/layer.py\", line 89, in forward\r\n    return self.impl.forward(query, key, value, kv_cache, attn_metadata,\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/attention/backends/flash_attn.py\", line 355, in forward\r\n    output[num_prefill_tokens:] = flash_attn_with_kvcache(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm_flash_attn/flash_attn_interface.py\", line 1233, in flash_attn_with_kvcache\r\n    out, softmax_lse = flash_attn_cuda.fwd_kvcache(\r\nRuntimeError: CUDA error: an illegal memory access was encountered\r\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\r\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\r\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\r\n\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\r\n    self._context.run(self._callback, *self._args)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 54, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\nI0618 08:04:44.709818 1084 model.py:368] \"[vllm] Error generating stream: CUDA error: an illegal memory access was encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\"\r\nI0618 08:04:44.710252 1084 model.py:368] \"[vllm] Error generating stream: CUDA error: an illegal memory access was encountered\\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\\n\"\r\n```\r\n  ",
        "created_at": "2024-06-18T08:48:42Z"
      },
      {
        "author": "rain7996",
        "body": "I get the same error. When I set the max_num_seqs=20, the error appears. When I set he max_num_seqs=18, everything goes well.  It seems like a kind of memory overflow? BTW, my gpu is H20 and the code runs well on my H800 machine. ",
        "created_at": "2024-08-16T02:58:26Z"
      },
      {
        "author": "LiuXiaoxuanPKU",
        "body": "The root cause for the spec dec failure is because block size is not passed correctly.\r\nAs shown [here](https://github.com/vllm-project/vllm/blob/1cabfcefb64a489c8ff9dcb289b4dd47cf8f89cf/tests/spec_decode/test_multi_step_worker.py#L676). When creating the batch, the test did not specify the block size. Therefore, within the `create_batch` function, a default block size is used, which is different from the block size defined [before](https://github.com/vllm-project/vllm/blob/1cabfcefb64a489c8ff9dcb289b4dd47cf8f89cf/tests/spec_decode/test_multi_step_worker.py#L659). Will fix it therefore we can use flash attention backend for spec dec CI test as well. @khluu ",
        "created_at": "2024-09-30T17:02:18Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5274": {
    "issue_number": 5274,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5274",
    "title": "[Bug]: high gpu_memory_utilization with 'OOM' and low gpu_memory_utilization with 'No available memory for the cache blocks'",
    "body": "### Your current environment\n\nv100 32G * 8\r\n\n\n### üêõ Describe the bug\n\nI tried to run a 32B model with lora adapters and test different GPU_MEMORY_UTILIZATION.\r\n\r\nWhen `gpu_memory_utilization = 0.9` , it came with OOM.\r\nWhen `gpu_memory_utilization = 0.8` , it came with 'No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.'\r\n\r\nDoes it mean that I need to find a suitable value for `gpu_memory_utilization`  or is there any other things going wrong?",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-06-05T10:26:43Z",
    "closed_at": "2024-11-29T02:05:56Z",
    "author": "mars-ch",
    "comments_count": 7,
    "comments": [
      {
        "author": "mgoin",
        "body": "@mars-ch what if you try using a smaller `max_model_len`? Could you share your script? It is important to know how many lora adapters and what tensor parallelism you are using.",
        "created_at": "2024-06-05T14:21:51Z"
      },
      {
        "author": "riverind",
        "body": "> ### Your current environment\r\n> v100 32G * 8\r\n> \r\n> ### üêõ Describe the bug\r\n> I tried to run a 32B model with lora adapters and test different GPU_MEMORY_UTILIZATION.\r\n> \r\n> When `gpu_memory_utilization = 0.9` , it came with OOM. When `gpu_memory_utilization = 0.8` , it came with 'No available memory for the cache blocks. Try increasing `gpu_memory_utilization` when initializing the engine.'\r\n> \r\n> Does it mean that I need to find a suitable value for `gpu_memory_utilization` or is there any other things going wrong?\r\n\r\nplease decrease max_model_len or increase gpu_memory_utilization or increase tensor-parallel-size gpus",
        "created_at": "2024-06-12T13:12:54Z"
      },
      {
        "author": "mars-ch",
        "body": "> @mars-ch what if you try using a smaller `max_model_len`? Could you share your script? It is important to know how many lora adapters and what tensor parallelism you are using.\r\n\r\n\r\nI used the LLM class like this:\r\n\r\nsampling_params = SamplingParams(temperature=0.7, top_p=0.8, repetition_penalty=1.05, max_tokens=512)\r\n\r\nllm = LLM(model=\"output_merged\",dtype=\"half\",gpu_memory_utilization=0.95, tensor_parallel_size=8, enforce_eager=True)\r\n\r\n\r\nThanks.",
        "created_at": "2024-06-13T02:38:35Z"
      },
      {
        "author": "mars-ch",
        "body": "What's more, the error shows that there are 2 processes:\r\n\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 234.00 MiB. GPU \u0001 has a total capacity of 31.75 GiB of which 98.75 MiB is free. Process 2852145 has 8.72 GiB memory in use. Process 2878797 has 22.92 GiB memory in use. Of the allocated memory 20.56 GiB is allocated by PyTorch, and 480.11 MiB is reserved by PyTorch but unallocated. \r\n\r\n",
        "created_at": "2024-06-13T10:08:38Z"
      },
      {
        "author": "DarkLight1337",
        "body": "There is currently a bug in the model profiling logic which causes the memory profiler to underestimate the amount of memory to be used by the model. To circumvent this, you can reduce the value of `gpu_memory_utilization`.",
        "created_at": "2024-06-23T01:23:30Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-26T01:59:40Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-29T02:05:56Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5337": {
    "issue_number": 5337,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5337",
    "title": "[Bug]: non-deterministic Python gc order leads to flaky tests",
    "body": "### Your current environment\r\n\r\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nI often see many flaky tests, and I think the Python gc system is one of the factor to blame.\r\n\r\nPython gc system is notoriously random. When we call `del x`, and `x`'s refcount is 0, it is not guaranteed that all resources held by `x` will be released immediately. This is true especially when `x` is a complicated object, and might contain self-reference inside it. That's one of the motivation for Python to propose the concept of context manager, to enforce some critical resource to be released immediately.\r\n\r\nTake the following code as an example:\r\n\r\n```python\r\nimport torch\r\nimport weakref\r\nimport gc\r\n\r\ndef tensor_destructed(tensor_ref):\r\n    # This function is called when the tensor is destructed.\r\n    print(f\"Tensor with id {id(tensor_ref)} is being destructed.\")\r\n\r\nclass A:\r\n    def __init__(self):\r\n        self.tensor = torch.tensor([1.0, 2.0, 3.0])\r\n\r\n    def __del__(self):\r\n        print(\"A is being destructed.\")\r\n        del self.tensor\r\n\r\ndef main():\r\n    # Create a complex object with a tensor.\r\n    data = A()\r\n    # the object is so complicated that it has a reference to itself.\r\n    data.self = data\r\n    # Attach a weak reference to the tensor with a callback for destruction.\r\n    tensor_ref = weakref.ref(data.tensor, tensor_destructed)\r\n\r\n    # Deleting the tensor manually.\r\n    print(\"before del data\")\r\n    del data\r\n    print(\"after del data, before gc collected\")\r\n    \r\n    # Manually trigger garbage collection to see the destruction callback in action.\r\n    gc.collect()\r\n    print(\"after gc collected\")\r\n\r\nif __name__ == \"__main__\":\r\n    main()\r\n\r\n```\r\n\r\nThe output is:\r\n\r\n```text\r\nbefore del data\r\nafter del data, before gc collected\r\nTensor with id 4379931520 is being destructed.\r\nA is being destructed.\r\nafter gc collected\r\n```\r\n\r\nAs it shows, the tensor is not destructed after we call `del data`. It is only destructed when we call `gc.collect()`, where Python will detect and break reference-cycles to destruct unreachable objects.\r\n\r\nThe following code in vLLM can suffer from Python gc:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/388596c91437a51d428a447594e9faec340c29b2/tests/models/test_models.py#L37-L44\r\n\r\nAfter `del hf_model`, chances are GPU memory held by `hf_model` is not released yet. This will cause OOM for the later vLLM model, leading to a flaky test. I have seen it broken for many times, and sometimes retrying the test helps.\r\n\r\nAdding `import gc; gc.collect()` solves the problem. However, to avoid code duplication, it is better to wrap `hf_model` as a context manager, and to call `import gc; gc.collect()` automatically after we exit the context manager:\r\n\r\n```python\r\n   with hf_runner(model, dtype=dtype) as hf_model:\r\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\r\n```",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-07T05:39:26Z",
    "closed_at": "2024-06-08T05:31:33Z",
    "author": "youkaichao",
    "comments_count": 13,
    "comments": [
      {
        "author": "rkooo567",
        "body": "Actually the destructor of hf_runner should call cleanup() which calls gc.collect() https://github.com/vllm-project/vllm/blob/388596c91437a51d428a447594e9faec340c29b2/tests/conftest.py#L359\r\n\r\nIs this not sufficient? ",
        "created_at": "2024-06-07T05:58:09Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> Actually the destructor of hf_runner should call cleanup() which calls gc.collect()\r\n> \r\n> https://github.com/vllm-project/vllm/blob/388596c91437a51d428a447594e9faec340c29b2/tests/conftest.py#L359\r\n> \r\n> Is this not sufficient?\r\n\r\n`__del__` is only called when the object is actually GC'ed, not when the `del` statement is used.\r\n\r\nIn any case, I think the context manager approach is cleaner. However, this will require refactoring the runner classes to only initialize the model when the context manager is entered.",
        "created_at": "2024-06-07T06:25:31Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Btw in general, we should prefer `weakref.finalize` over `__del__`. See [this documentation](https://docs.python.org/3/library/weakref.html#comparing-finalizers-with-del-methods) for more details.",
        "created_at": "2024-06-07T06:29:38Z"
      },
      {
        "author": "youkaichao",
        "body": "@rkooo567 I updated the code and the output. `__del__` is not enough here. You can see that `A.__del__` calls `del self.tensor`. However, the tensor is destructed before `A.__del__` is called. That's why Python gc is not reliable, especially when we need to enforce some destruction order.",
        "created_at": "2024-06-07T06:31:01Z"
      },
      {
        "author": "youkaichao",
        "body": "> In any case, I think the context manager approach is cleaner. However, this will require refactoring the runner classes to only initialize the model when the context manager is entered.\r\n\r\n@DarkLight1337 not necessary. We can make the runner itself a context manager. e.g. adding the following lines:\r\n\r\n```python\r\nclass HfRunner:\r\n    ...\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n        del self\r\n        cleanup()\r\n```\r\n\r\nThen the following change should be enough:\r\n\r\n```diff\r\n-    hf_model = hf_runner(model, dtype=dtype)\r\n-    hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\r\n-    del hf_model\r\n+    with hf_runner(model, dtype=dtype) as hf_model:\r\n+         hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\r\n```\r\n\r\nWe should avoid using `weakref.finalize` or `__del__ `. They are just not reliable.",
        "created_at": "2024-06-07T06:35:09Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> ```python\r\n> class HfRunner:\r\n>     ...\r\n>     def __enter__(self):\r\n>         return self\r\n> \r\n>     def __exit__(self, exc_type, exc_value, traceback):\r\n>         del self\r\n>         cleanup()\r\n> ```\r\n\r\nI don't think this will work. In particular, `del self` does not really do anything since the `HfRunner` is still inside the scope of the test function. We should instead operate on the wrapped model (`self.model`) directly.",
        "created_at": "2024-06-07T06:36:51Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> We should avoid using `weakref.finalize` or `__del__`. They are just not reliable.\r\n\r\nYeah, they are both dependant on GC so they aren't suitable in situations where GC is not even being invoked.",
        "created_at": "2024-06-07T06:39:41Z"
      },
      {
        "author": "youkaichao",
        "body": "> I don't think this will work. In particular, `del self` does not really do anything since the `HfRunner` is still inside the scope of the test function. We should instead operate on the wrapped model (`self.model`) directly.\r\n\r\nGood point. How about `del self.model` inside `__exit__ `?",
        "created_at": "2024-06-07T06:40:15Z"
      },
      {
        "author": "DarkLight1337",
        "body": "> I don't think this will work. In particular, `del self` does not really do anything since the `HfRunner` is still inside the scope of the test function. We should instead operate on the wrapped model (`self.model`) directly.\r\n\r\nThis is what I mean:\r\n\r\n```diff,py\r\nclass HfRunner:\r\n    def __init__(self, ...):\r\n-       self.model = create_model(...)\r\n+       self.model = None\r\n\r\n    def __enter__(self):\r\n+       self.model = create_model(...)\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n-       del self\r\n+       self.model = None\r\n        cleanup()\r\n```\r\n\r\nThere is no need to `del self.model` explicitly. Setting `self.model = None` should be sufficient to remove the reference to the HuggingFace model (or `LLM` object for `VllmRunner`).",
        "created_at": "2024-06-07T06:42:52Z"
      },
      {
        "author": "youkaichao",
        "body": "I think they are equivalent. For the following code:\r\n\r\n```python\r\n   with hf_runner(model, dtype=dtype) as hf_model:\r\n        hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)\r\n```\r\n\r\nThe function call chain is:\r\n- `hf_model = hf_runner() # essentially HfRunner. __init__`\r\n- `hf_model.__enter__()`\r\n- `hf_outputs = hf_model.generate_greedy(example_prompts, max_tokens)`\r\n- `hf_model. __exit__()`",
        "created_at": "2024-06-07T06:48:02Z"
      },
      {
        "author": "DarkLight1337",
        "body": "To avoid having to check `self.model is not None` each time in the other methods, it may be better to have a separate context manager function that returns the runner object. For example:\r\n\r\n```py\r\n@contextlib.contextmanager\r\ndef hf_model_runner(*args, **kwargs):\r\n    model = create_model(...)\r\n    yield HfRunner(model, ...)\r\n    del model\r\n    cleanup()\r\n\r\nclass HfRunner:\r\n    def __init__(self, model, ...):\r\n        self.model = model\r\n        # The rest is the same as original HfRunner without context manager. Also we can now remove `__del__` method\r\n```\r\n\r\n",
        "created_at": "2024-06-07T06:48:44Z"
      },
      {
        "author": "DarkLight1337",
        "body": "After some offline discussion, we have settled on the following code:\r\n\r\n```py\r\nclass HfRunner:\r\n    def __init__(self, ...):\r\n        self.model = create_model(...)\r\n\r\n    def __enter__(self):\r\n        return self\r\n\r\n    def __exit__(self, exc_type, exc_value, traceback):\r\n        del self.model\r\n        cleanup()\r\n```\r\n\r\nAlthough `HfRunner` can only be used once as a context manager, it is sufficient for testing. This passes the type checker without needlessly complicating the existing code.",
        "created_at": "2024-06-07T08:25:01Z"
      },
      {
        "author": "robertgshaw2-redhat",
        "body": "Love this!",
        "created_at": "2024-06-07T13:47:30Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5544": {
    "issue_number": 5544,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5544",
    "title": "[Bug]: Distribute Tests PR test fails",
    "body": "### Your current environment\n\n```\r\nvLLM version 0.5.0.post1\r\n```\n\n### üêõ Describe the bug\n\nHello!\r\n\r\nI would like to know if the `tests/distributed/test_utils.py` file (Merged at #5473) might be causing errors during the **Distribute Tests** process on BuildKite.\r\n\r\nWhen I checked #5422 and #5412, I found that both PRs failed during the Distribute Tests process. The reason for the failure is as follows:\r\n\r\n```\r\n[2024-06-14T00:24:15Z] Running 1 items in this shard: tests/distributed/test_utils.py::test_cuda_device_count_stateless\r\n[2024-06-14T00:24:15Z]\r\n[2024-06-14T00:24:30Z] distributed/test_utils.py::test_cuda_device_count_stateless 2024-06-14 00:24:30,636\tINFO worker.py:1753 -- Started a local Ray instance.\r\n[2024-06-14T00:24:33Z] FAILED\r\n[2024-06-14T00:24:33Z]\r\n[2024-06-14T00:24:33Z] =================================== FAILURES ===================================\r\n[2024-06-14T00:24:33Z] _______________________ test_cuda_device_count_stateless _______________________\r\n[2024-06-14T00:24:33Z]\r\n[2024-06-14T00:24:33Z]     def test_cuda_device_count_stateless():\r\n[2024-06-14T00:24:33Z]         \"\"\"Test that cuda_device_count_stateless changes return value if\r\n[2024-06-14T00:24:33Z]         CUDA_VISIBLE_DEVICES is changed.\"\"\"\r\n[2024-06-14T00:24:33Z]\r\n[2024-06-14T00:24:33Z]         actor = _CUDADeviceCountStatelessTestActor.options(num_gpus=2).remote()\r\n[2024-06-14T00:24:33Z] >       assert ray.get(actor.get_cuda_visible_devices.remote()) == \"0,1\"\r\n[2024-06-14T00:24:33Z] E       AssertionError: assert '1,0' == '0,1'\r\n[2024-06-14T00:24:33Z] E         \r\n[2024-06-14T00:24:33Z] E         - 0,1\r\n[2024-06-14T00:24:33Z] E         + 1,0\r\n[2024-06-14T00:24:33Z]\r\n[2024-06-14T00:24:33Z] distributed/test_utils.py:26: AssertionError\r\n[2024-06-14T00:24:33Z] =========================== short test summary info ============================\r\n[2024-06-14T00:24:33Z] FAILED distributed/test_utils.py::test_cuda_device_count_stateless - AssertionError: assert '1,0' == '0,1'\r\n[2024-06-14T00:24:33Z]\r\n[2024-06-14T00:24:33Z]   - 0,1\r\n[2024-06-14T00:24:33Z]   + 1,0\r\n[2024-06-14T00:24:33Z] ============================== 1 failed in 17.55s ==============================\r\n[2024-06-14T00:24:36Z] üö® Error: The command exited with status 1\r\n[2024-06-14T00:24:36Z] user command error: The plugin docker command hook exited with status 1\r\n```\r\nSince I am not an expert in the Ray framework, so I am not sure how critical the difference between `0, 1` and `1, 0`.\r\n**I think the fact that \"1,0\" was output in a simple test code using Ray indicates that under certain conditions, the result can be \"1,0\". Therefore, it might be reasonable to conclude that the assert line should allow \"1,0\".**\r\n\r\nWould there be any issues if the assert line is modified as shown below?\r\n```Python\r\n# assert ray.get(actor.get_cuda_visible_devices.remote()) == \"0,1\"\r\nassert ray.get(actor.get_cuda_visible_devices.remote()) in [\"0,1\", \"1,0\"] \r\n```\r\n\r\nI am concerned that this issue might be affecting the correct check of the **Distribute Tests**, and would like to inquire about it.\r\nIf this issue is not the cause of the test fail problem, I would greatly appreciate it if you could check the **Distribute Tests** logs and proved some hints on what might be causing the errors. :bow: \r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-14T15:26:21Z",
    "closed_at": "2024-06-14T17:02:24Z",
    "author": "bong-furiosa",
    "comments_count": 0,
    "comments": []
  },
  "https://github.com/vllm-project/vllm/issues/5550": {
    "issue_number": 5550,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5550",
    "title": "[Bug]: OOM when setting prompt_logprobs=1",
    "body": "### Your current environment\r\n\r\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.0 (default, Mar  3 2022, 09:58:08) [GCC 7.5.0] (64-bit runtime)\r\nPython platform: Linux-5.4.0-162-generic-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: Could not collect\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A100-SXM4-80GB\r\nNvidia driver version: 535.54.03\r\ncuDNN version: Probably one of the following:\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.0/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.1/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.3/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.4/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.5/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.6/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.7/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_adv_train.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8.4.1\r\n/usr/local/cuda-11.8/targets/x86_64-linux/lib/libcudnn_ops_train.so.8.4.1\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      48 bits physical, 48 bits virtual\r\nCPU(s):                             128\r\nOn-line CPU(s) list:                0-127\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 32\r\nSocket(s):                          2\r\nNUMA node(s):                       2\r\nVendor ID:                          AuthenticAMD\r\nCPU family:                         25\r\nModel:                              1\r\nModel name:                         AMD EPYC 7543 32-Core Processor\r\nStepping:                           1\r\nFrequency boost:                    enabled\r\nCPU MHz:                            1657.430\r\nCPU max MHz:                        2800.0000\r\nCPU min MHz:                        1500.0000\r\nBogoMIPS:                           5599.81\r\nVirtualization:                     AMD-V\r\nL1d cache:                          2 MiB\r\nL1i cache:                          2 MiB\r\nL2 cache:                           32 MiB\r\nL3 cache:                           512 MiB\r\nNUMA node0 CPU(s):                  0-31,64-95\r\nNUMA node1 CPU(s):                  32-63,96-127\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Retpolines, IBPB conditional, IBRS_FW, STIBP always-on, RSB filling, PBRSB-eIBRS Not affected\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good nopl nonstop_tsc cpuid extd_apicid aperfmperf pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpb cat_l3 cdp_l3 invpcid_single hw_pstate ssbd mba ibrs ibpb stibp vmmcall fsgsbase bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local clzero irperf xsaveerptr wbnoinvd arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold v_vmsave_vmload vgif umip pku ospke vaes vpclmulqdq rdpid overflow_recov succor smca\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] transformers              4.41.2                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0\tCPU Affinity\tNUMA Affinity\tGPU NUMA ID\r\nGPU0\t X \t0-7,64-71\t0-1\t\tN/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\n\r\nI have a standard setup like: \r\n\r\n```python\r\nmodel = LLM(\r\n            model=model,\r\n            download_dir=download_dir,\r\n            dtype=dtype,\r\n            tensor_parallel_size=tensor_parallel_size,\r\n            quantization=quantization if quantization != \"none\" else None,\r\n        )\r\n```\r\n\r\nAnd running a function like:\r\n```python\r\ndef batch_prompt(\r\n        self, \r\n        prompts: List[str], \r\n        max_new_tokens: Optional[int] = 500,\r\n        do_sample: Optional[bool] = True,\r\n        top_p: Optional[float] = 0.9,\r\n        top_k: Optional[int] = -1,\r\n        temperature: Optional[float] = 0.1,\r\n        num_return_sequences: Optional[int] = 1,\r\n        best_of: Optional[int] = 1,\r\n        use_beam_search: Optional[bool] = False,\r\n        presence_penalty: Optional[float] = 0.0,\r\n        frequency_penalty: Optional[float] = 0.0,\r\n    ) -> List[str]:\r\n        \"\"\"Batched text generation.\"\"\"     \r\n        sampling_params = SamplingParams(\r\n            temperature=temperature,\r\n            top_p=top_p,\r\n            top_k=top_k,\r\n            max_tokens=max_new_tokens,\r\n            n=num_return_sequences,\r\n            best_of=1,\r\n            use_beam_search=use_beam_search,\r\n            presence_penalty=presence_penalty,\r\n            frequency_penalty=frequency_penalty,\r\n        )\r\n        \r\n        outputs = self.model.generate(\r\n            prompts=prompts,\r\n            sampling_params=sampling_params,\r\n        )\r\n        \r\n        generations = []\r\n        for output in outputs: \r\n            for generated_sequence in output.outputs:\r\n                generations.append(generated_sequence.text)\r\n                \r\n        return generations\r\n```\r\n\r\nworks fine with very long prompts and a very large batch size.\r\n\r\nHowever, as soon as I do something like \r\n\r\n```python\r\nsampling_params = SamplingParams(\r\n            temperature=0,\r\n            max_tokens=1,\r\n            n=1,\r\n            prompt_logprobs=1,\r\n            spaces_between_special_tokens=False,\r\n        )\r\n```\r\n\r\nI.e., ```prompt_logprobs=1``` instead of default (None), I immediately get OOM for the exact same prompts which does not make sense to me? It should just return the logprobs in addition to the generations but not affect things otherwise? \r\n\r\nMy OOM error: \r\n```\r\nProcessed prompts:   0%|    | 0/5 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]Error executing job with overrides: []\r\nTraceback (most recent call last):\r\n  File \"/sailhome/jphilipp/research_projects/star-gate-human-eval/experiments/v1/logprobs.py\", line 47, in main\r\n    batch_logprobs = model.prompt_logprobs(\r\n  File \"/sailhome/jphilipp/research_projects/star-gate-human-eval/src/stargate/vllm_inference_model.py\", line 43, in prompt_logprobs\r\n    output_responses = self.model.generate(\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/utils.py\", line 691, in inner\r\n    return fn(*args, **kwargs)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 304, in generate\r\n    outputs = self._run_engine(use_tqdm=use_tqdm)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/entrypoints/llm.py\", line 556, in _run_engine\r\n    step_outputs = self.llm_engine.step()\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 776, in step\r\n    output = self.model_executor.execute_model(\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/executor/gpu_executor.py\", line 91, in execute_model\r\n    output = self.driver_worker.execute_model(execute_model_req)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/worker/worker.py\", line 280, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 765, in execute_model\r\n    output = self.model.sample(\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/model_executor/models/llama.py\", line 386, in sample\r\n    next_tokens = self.sampler(logits, sampling_metadata)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/scr/jphilipp/miniconda3/envs/stargate/lib/python3.10/site-packages/vllm/model_executor/layers/sampler.py\", line 91, in forward\r\n    probs = torch.softmax(logits, dim=-1, dtype=torch.float)\r\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.61 GiB. GPU \r\n```\r\n\r\nNote that the above works if len(prompts) == 1.",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-06-14T18:24:15Z",
    "closed_at": "2024-11-27T02:06:40Z",
    "author": "janphilippfranken",
    "comments_count": 5,
    "comments": [
      {
        "author": "zifeitong",
        "body": "There is a pending PR trying to address this problem: https://github.com/vllm-project/vllm/pull/5355.\r\n\r\nMeanwhile, you can try the chunked prefill feature which worked for me as a workaround: https://docs.vllm.ai/en/latest/models/performance.html#chunked-prefill.",
        "created_at": "2024-06-15T00:12:32Z"
      },
      {
        "author": "janphilippfranken",
        "body": "would you mind sharing your code? let's say i have ```n_prompts=10```, and set ```prompt_logprobs=0```, i'd ideally get the logprobs for all 10 prompts using a single call ```model.generate(prompts=prompts, sampling_params=sampling_params)```.",
        "created_at": "2024-06-15T17:18:38Z"
      },
      {
        "author": "zifeitong",
        "body": "Something like this: `model = LLM(..., enable_chunked_prefill=True, max_num_batched_tokens=512, gpu_memory_utilization=0.9)`\r\n\r\nTry smaller values of `gpu_memory_utilization` and/or `max_num_batched_tokens` if you still see OOM.",
        "created_at": "2024-06-16T18:59:46Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-26T01:58:40Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-27T02:06:40Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5657": {
    "issue_number": 5657,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5657",
    "title": "[Bug]: Ray distributed backend does not support out-of-tree models via ModelRegistry APIs",
    "body": "### Your current environment\r\n\r\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Amazon Linux 2 (x86_64)\r\nGCC version: (GCC) 7.3.1 20180712 (Red Hat 7.3.1-17)\r\nClang version: Could not collect\r\nCMake version: version 3.29.5\r\nLibc version: glibc-2.26\r\n\r\nPython version: 3.10.14 | packaged by conda-forge | (main, Mar 20 2024, 12:45:18) [GCC 12.3.0] (64-bit runtime)\r\nPython platform: Linux-5.10.217-205.860.amzn2.x86_64-x86_64-with-glibc2.26\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA A100-SXM4-40GB\r\nGPU 1: NVIDIA A100-SXM4-40GB\r\nGPU 2: NVIDIA A100-SXM4-40GB\r\nGPU 3: NVIDIA A100-SXM4-40GB\r\nGPU 4: NVIDIA A100-SXM4-40GB\r\nGPU 5: NVIDIA A100-SXM4-40GB\r\nGPU 6: NVIDIA A100-SXM4-40GB\r\nGPU 7: NVIDIA A100-SXM4-40GB\r\n\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:        x86_64\r\nCPU op-mode(s):      32-bit, 64-bit\r\nByte Order:          Little Endian\r\nCPU(s):              96\r\nOn-line CPU(s) list: 0-95\r\nThread(s) per core:  2\r\nCore(s) per socket:  24\r\nSocket(s):           2\r\nNUMA node(s):        2\r\nVendor ID:           GenuineIntel\r\nCPU family:          6\r\nModel:               85\r\nModel name:          Intel(R) Xeon(R) Platinum 8275CL CPU @ 3.00GHz\r\nStepping:            7\r\nCPU MHz:             3599.164\r\nBogoMIPS:            5999.99\r\nHypervisor vendor:   KVM\r\nVirtualization type: full\r\nL1d cache:           32K\r\nL1i cache:           32K\r\nL2 cache:            1024K\r\nL3 cache:            36608K\r\nNUMA node0 CPU(s):   0-23,48-71\r\nNUMA node1 CPU(s):   24-47,72-95\r\nFlags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single pti fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves ida arat pku ospke\r\n\r\nVersions of relevant libraries:\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0+cu121\r\n[pip3] transformers==4.41.2\r\n[pip3] triton==2.3.0\r\n[conda] No relevant packages\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    GPU2    GPU3    GPU4    GPU5    GPU6    GPU7    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      NV12    NV12    NV12    NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\r\nGPU1    NV12     X      NV12    NV12    NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\r\nGPU2    NV12    NV12     X      NV12    NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\r\nGPU3    NV12    NV12    NV12     X      NV12    NV12    NV12    NV12    0-23,48-71      0               N/A\r\nGPU4    NV12    NV12    NV12    NV12     X      NV12    NV12    NV12    24-47,72-95     1               N/A\r\nGPU5    NV12    NV12    NV12    NV12    NV12     X      NV12    NV12    24-47,72-95     1               N/A\r\nGPU6    NV12    NV12    NV12    NV12    NV12    NV12     X      NV12    24-47,72-95     1               N/A\r\nGPU7    NV12    NV12    NV12    NV12    NV12    NV12    NV12     X      24-47,72-95     1               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nThe ray distributed backend does not support out-of-tree models (on a single node). \r\n\r\nRepro:\r\n```python\r\n\r\nfrom vllm import ModelRegistry\r\nfrom vllm.model_executor.models.mixtral import MixtralForCausalLM\r\nModelRegistry.register_model(\"SomeModel\", MixtralForCausalLM)\r\n\r\nfrom vllm import LLM, SamplingParams\r\n\r\nif __name__ == \"__main__\": \r\n    llm = LLM(\r\n        model=\"SomeModel/\", # just use a downloaded mixtral model from huggingface with config.json   \"architectures\": [\"SomeModel\"]\r\n        tensor_parallel_size=8,\r\n        # distributed_executor_backend=\"ray\", # ray backend fails!\r\n    )\r\n```\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-18T20:41:39Z",
    "closed_at": "2024-08-13T23:25:27Z",
    "author": "SamKG",
    "comments_count": 19,
    "comments": [
      {
        "author": "youkaichao",
        "body": "will take a look later",
        "created_at": "2024-06-20T19:47:40Z"
      },
      {
        "author": "youkaichao",
        "body": "sorry i don't get it. the usage of oot model registration, is that you register the architecture name appearing in the huggingface config file, not the `LLM` argument.\r\n\r\nsee https://huggingface.co/facebook/opt-125m/blob/main/config.json#L6 for example.",
        "created_at": "2024-06-24T21:08:14Z"
      },
      {
        "author": "SamKG",
        "body": "> sorry i don't get it. the usage of oot model registration, is that you register the architecture name appearing in the huggingface config file, not the `LLM` argument.\r\n> \r\n> see https://huggingface.co/facebook/opt-125m/blob/main/config.json#L6 for example.\r\n\r\nYes, this is how I am using it.\r\nFor context, the \"SomeModel/\" directory here contains a config.json file which references my custom architecture. For clarity, can use this example:\r\n\r\n\r\n```python\r\nfrom vllm import ModelRegistry\r\nfrom vllm.model_executor.models.mixtral import MixtralForCausalLM\r\nModelRegistry.register_model(\"SomeModel\", MixtralForCausalLM)\r\n\r\nfrom vllm import LLM, SamplingParams\r\n\r\nif __name__ == \"__main__\": \r\n    llm = LLM(\r\n        model=\"path_to_directory/\", # directory which has a config.json with architectures: [\"SomeModel\"]\r\n        tensor_parallel_size=8,\r\n        # distributed_executor_backend=\"ray\", # ray backend fails!\r\n    )\r\n```",
        "created_at": "2024-06-24T21:30:17Z"
      },
      {
        "author": "youkaichao",
        "body": "then it makes sense to me. `ray` workers does not know \"SomeModel\", the following code:\r\n\r\n```python\r\nfrom vllm import ModelRegistry\r\nfrom vllm.model_executor.models.mixtral import MixtralForCausalLM\r\nModelRegistry.register_model(\"SomeModel\", MixtralForCausalLM)\r\n```\r\n\r\nis not executed in ray workers.",
        "created_at": "2024-06-24T21:34:37Z"
      },
      {
        "author": "SamKG",
        "body": "thanks!\r\nis there a way to do this initialization on the ray workers?",
        "created_at": "2024-06-24T21:40:21Z"
      },
      {
        "author": "youkaichao",
        "body": "@SamKG so the default backend (multiprocessing) should work out-of-the-box, right?",
        "created_at": "2024-06-26T05:01:22Z"
      },
      {
        "author": "richardliaw",
        "body": "also cc @rkooo567 - maybe this is solvable via runtime env\n\n@SamKG is there a full repro somewhere we can look at?",
        "created_at": "2024-06-26T05:12:40Z"
      },
      {
        "author": "SamKG",
        "body": "> @SamKG is there a full repro somewhere we can look at?\r\n\r\n@richardliaw  Try attached. Note that the default backend will also fail (but with an expected error), since I added a stub tensor to keep the model directory small.\r\n\r\n\r\n[repro.tar.gz](https://github.com/user-attachments/files/15983048/repro.tar.gz)\r\n\r\n@youkaichao yes, default backend works fine (as long as the OOT definition happens outside of __main__)\r\n",
        "created_at": "2024-06-26T05:58:36Z"
      },
      {
        "author": "rkooo567",
        "body": "ray.init(runtime_env={\"worker_process_setup_hook\": })... allows to execute code on all workers. Would this suffice? ",
        "created_at": "2024-06-26T13:13:54Z"
      },
      {
        "author": "youkaichao",
        "body": "@rkooo567 this functionality seems related, but how can we expose it to users?",
        "created_at": "2024-06-26T19:26:49Z"
      },
      {
        "author": "SamKG",
        "body": "> ray.init(runtime_env={\"worker_process_setup_hook\": })... allows to execute code on all workers. Would this suffice?\r\n\r\nthis seems to fix the issue!\r\n\r\n```python\r\nimport ray\r\nfrom vllm import ModelRegistry, LLM\r\n\r\ndef _init_worker():\r\n    from vllm.model_executor.models.mixtral import MixtralForCausalLM\r\n    ModelRegistry.register_model(\"SomeModel\", MixtralForCausalLM)\r\n\r\n_init_worker()\r\n\r\nif __name__ == \"__main__\":\r\n    ray.init(runtime_env={\"worker_process_setup_hook\": _init_worker})\r\n    llm = LLM(\r\n        model=\"model/\",\r\n        tensor_parallel_size=8,\r\n        distributed_executor_backend=\"ray\",\r\n    )\r\n    llm.generate(\"test\")\r\n```\r\n\r\n\r\n",
        "created_at": "2024-06-26T20:50:27Z"
      },
      {
        "author": "richardliaw",
        "body": "very nice! \r\n\r\n@youkaichao maybe we can just print out a warning linking to the vllm docs about this? \r\n\r\nand in the vllm docs let's have an example snippet like above!",
        "created_at": "2024-06-26T23:53:14Z"
      },
      {
        "author": "youkaichao",
        "body": "closed by https://github.com/vllm-project/vllm/pull/7426/\r\n\r\nthe recommended way for registering oot models should be using plugins.\r\n\r\nI will write docs later.\r\n\r\ncc @SamKG you can have a try for the plugin system!",
        "created_at": "2024-08-13T23:25:20Z"
      },
      {
        "author": "chensiye-csy",
        "body": "> closed by #7426\r\n> \r\n> the recommended way for registering oot models should be using plugins.\r\n> \r\n> I will write docs later.\r\n> \r\n> cc @SamKG you can have a try for the plugin system!\r\n\r\nIs there any solution to the multi tensor parallel plugin method for theOOT model problem (except using ray backend)? I can't find the relevant documentation for the plugin.",
        "created_at": "2024-09-18T07:20:29Z"
      },
      {
        "author": "youkaichao",
        "body": "@chensiye-csy  sorry I didn't have time to write doc yet, but you can follow https://github.com/vllm-project/vllm/tree/main/tests/plugins/vllm_add_dummy_model . it is fairly easy.",
        "created_at": "2024-09-18T07:23:20Z"
      },
      {
        "author": "chensiye-csy",
        "body": "> @chensiye-csy sorry I didn't have time to write doc yet, but you can follow https://github.com/vllm-project/vllm/tree/main/tests/plugins/vllm_add_dummy_model . it is fairly easy.\r\n\r\nIn fact, I have registered the OOT model according to the `ModelRegistry.register_model` method, but after using the configuration of `tensor_parallel_size>1` (the default parallel mode is mp (multiprocessing) instead of ray), the default mp does not take effect on the OOT model, Can the `Plugin` method solve this problem?",
        "created_at": "2024-09-18T07:31:35Z"
      },
      {
        "author": "youkaichao",
        "body": "yes this is exactly the problem plugin will solve. the plugin function will be called in every vllm process.",
        "created_at": "2024-09-18T07:33:15Z"
      },
      {
        "author": "chensiye-csy",
        "body": "> yes this is exactly the problem plugin will solve. the plugin function will be called in every vllm process.\r\n\r\nThat's good! How can I use plugin in OOT model?",
        "created_at": "2024-09-18T07:34:36Z"
      },
      {
        "author": "youkaichao",
        "body": "the easiest way:\r\n\r\nrun `pip install -e .` in https://github.com/vllm-project/vllm/tree/main/tests/plugins/vllm_add_dummy_model , and change this line:\r\n\r\nhttps://github.com/vllm-project/vllm/blob/95965d31b6ac2c9557816a6ffabe4a3117a5ccb2/tests/plugins/vllm_add_dummy_model/vllm_add_dummy_model/__init__.py#L23",
        "created_at": "2024-09-18T07:39:12Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5767": {
    "issue_number": 5767,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5767",
    "title": "[Bug]: Different Image Size support with Phi-3-Vision and torchvision dependency",
    "body": "### Your current environment\r\nI encountered a few issues while running phi-3-vision with the vllm built from current main branch.\r\n\r\n1. Dependency:  \r\n   `torchvision` is a dependency under [image_processing_phi3_v.py](https://huggingface.co/microsoft/Phi-3-vision-128k-instruct/blob/main/image_processing_phi3_v.py)  \r\n   Currently it is only included in requirements-test.txt, not requirements-common.txt. But importing the image processor also needs `torchvision` be available during imports.\r\n\r\n2. Different Image Size Support\r\n\r\nI have built a vllm docker based on the latest main branch.\r\nI have the following script to resize the `stop_sign.jpg` before sending to vllm API server. \r\n```text\r\npython send_phi3v_request.py\r\n```\r\n```python\r\nimport base64\r\nimport requests\r\nimport time\r\nimport random\r\nimport numpy as np  # To calculate the mean\r\nimport json\r\nimport os\r\nfrom PIL import Image\r\n\r\n# Parameters\r\nnum_iterations = 20  # Number of times to repeat the request\r\n\r\n# To store latencies for each iteration\r\nlatencies = []\r\nttfts = []\r\noutput_processing_times = []\r\noutput_throughputs = []\r\n\r\n# Function to encode the image\r\ndef encode_image(image_path):\r\n  with open(image_path, \"rb\") as image_file:\r\n    return base64.b64encode(image_file.read()).decode('utf-8')\r\n\r\n\r\ndef get_image_size(image_path):\r\n    with Image.open(image_path) as img:\r\n        # Get the dimensions\r\n        width, height = img.size\r\n    return width, height\r\n\r\n\r\ndef resize_image(image_path, sizes, output_dir):\r\n    # Open the image\r\n    with Image.open(image_path) as img:\r\n        # Iterate over the desired sizes\r\n        for size in sizes:\r\n            # Resize the image\r\n            width, height = img.size\r\n            print(f\"Image original size: {width, height}\")\r\n            resized_img = img.resize(size)\r\n            # Create a file name for the resized image\r\n            base_name = os.path.basename(image_path)\r\n            name, ext = os.path.splitext(base_name)\r\n            resized_img_name = f\"{name}_{size[0]}x{size[1]}{ext}\"\r\n            resized_img_path = os.path.join(output_dir, resized_img_name)\r\n            # Save the resized image\r\n            resized_img.save(resized_img_path)\r\n            print(f\"Saved resized image: {resized_img_path}\")\r\n\r\n\r\n# Path to your image\r\nimage_path = \"/home/changsu/images/stop_sign.jpg\"\r\nimage_files = [image_path]\r\n\r\nsizes = [(256, 256), (384, 384), (512, 512), (1024, 1024)]\r\noutput_dir = 'resized_images'\r\nos.makedirs(output_dir, exist_ok=True)\r\nresize_image(image_path, sizes, output_dir)\r\n\r\nimage_files = []\r\ndirectory_path = \"/home/changsu/resized_images\"\r\nfor root, dirs, filenames in os.walk(directory_path):\r\n    for filename in filenames:\r\n        image_files.append(os.path.join(root, filename))\r\n        \r\n        \r\n# Loop for the specified number of iterations\r\nfor _ in range(num_iterations):\r\n    # Generate 96 lines with varying numbers of tokens\r\n    # inputs = [\"1 \" * 256 for _ in range(num_lines)]\r\n    for image_idx, image in enumerate(image_files):\r\n        # Getting the base64 string\r\n        base64_image = encode_image(image)\r\n\r\n        payload = {\r\n        \"model\": \"/models/Phi-3-vision-128k-instruct\",\r\n        # \"model\": \"/models/llava-v1.6-mistral-7b-hf\",\r\n        # \"model\": \"/models/llava-v1.6-34b-hf\",\r\n        \"messages\": [\r\n            {\r\n                \"role\": \"user\",\r\n                \"content\": [\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": \"What's the content of the image?\"\r\n                },\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                    \"url\": f\"data:image/jpeg;base64,{base64_image}\"\r\n                    }\r\n                }\r\n                ]\r\n            }\r\n        ],\r\n        \"max_tokens\": 300,\r\n        \"temperature\": 0.0,\r\n        \"stream\": True,\r\n        }\r\n\r\n        # metrics\r\n        ttft = 0\r\n        total_request_time = 0\r\n        tokens_received = 0\r\n        time_to_next_token = []\r\n        generated_text = \"\"\r\n        output_throughput = 0\r\n        total_request_time = 0\r\n        output_start_time = 0\r\n        output_processing_time = 0\r\n\r\n        # Start the timer\r\n        start_time = time.monotonic()\r\n\r\n        # Send the POST request\r\n        response = requests.post(\r\n            \"http://localhost:8000/v1/chat/completions\",\r\n            # \"http://localhost:9922/v1/chat/completions\",\r\n            headers={\"Content-Type\": \"application/json\"},\r\n            json=payload,\r\n            stream=True,\r\n        )\r\n```\r\n\r\n### Server Launch Command\r\n```bash\r\ndocker run -tid --gpus \\\"device=0\\\" --shm-size 5g \\\r\n        -p 8086:8000 -v /mnt/data/models:/models \\\r\n        --ulimit nofile=65535:65535 \\\r\n        -v $(pwd)/entrypoint.sh:/entrypoint.sh \\\r\n        --entrypoint /entrypoint.sh \\\r\n        --name vllm-main-phi3-v-1gpu-p8086 \\\r\n        vllm:main-phi3-v \\\r\n        --tensor-parallel-size=1 \\\r\n        --model=/models/Phi-3-vision-128k-instruct \\\r\n        --image-input-type=\"pixel_values\" \\\r\n        --image-feature-size=1921 \\\r\n        --image-token-id=32044 \\\r\n        --image-input-shape=\"1, 3, 1008, 1344\" \\\r\n        --trust-remote-code # Need for Phi-3-vision only as its config needs to be loaded from HF model files\r\n```\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nTo run the file, there would raise an error:\r\n```text\r\nTraceback (most recent call last):\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    return_value = task.result()\r\n                   ^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 550, in run_engine_loop\r\n    has_requests_in_progress = await self.engine_step()\r\n                               ^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 523, in engine_step\r\n    request_outputs = await self.engine.step_async()\r\n                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/engine/async_llm_engine.py\", line 236, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/executor/gpu_executor.py\", line 117, in execute_model_async\r\n    output = await make_async(self.driver_worker.execute_model\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/worker/worker.py\", line 281, in execute_model\r\n    output = self.model_runner.execute_model(seq_group_metadata_list,\r\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/worker/model_runner.py\", line 749, in execute_model\r\n    hidden_states = model_executable(\r\n                    ^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/phi3v.py\", line 315, in forward\r\n    inputs_embeds = self.vision_embed_tokens(\r\n                    ^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\r\n  File \"/opt/conda/envs/vllm/lib/python3.11/site-packages/vllm/model_executor/models/phi3v.py\", line 253, in forward\r\n    hidden_states[positions[idx, 0],\r\nRuntimeError: The expanded size of the tensor (1937) must match the existing size (2509) at non-singleton dimension 0.  Target sizes: [1937, 3072].  Tensor sizes: [2509, 3072]\r\n\r\n```\r\n\r\n### Debugging Progress\r\n\r\n\r\nUpon checking, I didn't see any resize done for images input to Phi-3-vision. See https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/phi3v.py#L271. Is this intended? \r\nI saw there is a resize in Llava-Next. And by doing something similar to https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/llava_next.py#L94-L104, I was able to run my test script successfully.\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-23T04:34:08Z",
    "closed_at": "2024-06-24T04:11:54Z",
    "author": "CatherineSue",
    "comments_count": 2,
    "comments": [
      {
        "author": "ywang96",
        "body": "cc @Isotr0py if you can take a look",
        "created_at": "2024-06-23T06:28:59Z"
      },
      {
        "author": "Isotr0py",
        "body": "vLLM hasn't fully supported dynamic shape image input yet. And we can add a patch to resize images temporarily.\r\n\r\nSeems that #5276 can support dynamic shape image input if it's merged. Let's wait for it to be finished.",
        "created_at": "2024-06-23T11:31:54Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5793": {
    "issue_number": 5793,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5793",
    "title": "[Bug]: Different quality responses using GPTQ / marlin kernels on A10 vs A100 GPUs",
    "body": "### üêõ Describe the bug\r\n\r\nHello, \r\n\r\n\r\nI am running llama3-70b and mixtral with VLLM on a bunch of different kinds of machines. I encountered wildly different quality performance on A10 GPUs vs A100/H100 GPUs for ONLY gptq models and marlin kernels. GPTQ with marlin kernels is way faster than AWQ but with AWQ, i see roughly the same response on my test queries on either kind of GPU environment. For A10 deployments, the only difference in the settings is that I use 2 A10 24GB GPUs instead of 1 A100 or H100 (using the tensor parallelism param).  I am running vllm = 0.4.2. I am using examples from llama3-70b testing on a very simple test query but I also saw the similar flavor of quality issues with mixtral-awq vs mixtral-gptq as well and I also saw the same flavor of issues on all my other more complicated RAG test queries as well. Generally the gptq models on A10s are psychotic and unusable and behave as if the model files are screwed up, which is why i tested two publicly available quantizations of llama3... \r\n\r\nI also tried to simulate a GPU environment of 2 A10s on 1 single A100 by setting --gpu-memory-utilization to 0.55 i.e. I choked the vram on the A100 deployment to choke the kv-cache available space. I got the same good quality performance on the A100 that I always see albeit with less throughput since my kv-cache was choked. No psychotic responses like I see on A10s. \r\n\r\nHere's my examples,\r\n\r\n\r\nTest prompt: \"what model are you and what quantization do you use?\"\r\n\r\n\r\nModel : https://huggingface.co/casperhansen/llama-3-70b-instruct-awq\r\nModel Args: --max-model-len 8192 --quantization awq --dtype half --gpu-memory-utilization 0.99 --trust-remote-code --enforce-eager --kv-cache-dtype fp8\r\n\r\nResponse on A10 or A100 (doesn't matter where I deploy it, I get pretty much the same response):\r\n```\r\n\"I'm an AI assistant, and I'm happy to share some information about my architecture.\r\n\r\nI'm a large language model, specifically a transformer-based model, which is a type of neural network designed primarily for natural language processing tasks. My model is based on the BERT (Bidirectional Encoder Representations from Transformers) architecture, which was developed by Google in 2018.\r\n\r\nRegarding quantization, I don't use traditional quantization techniques like fixed-point or floating-point quantization. Instead, my model uses a technique called \"knowledge distillation\" to reduce the precision of my weights and activations while maintaining the accuracy of my responses.\r\n\r\nKnowledge distillation is a method that involves training a smaller, simpler model (the \"student\") to mimic the behavior of a larger, more complex model (the \"teacher\"). In my case, the teacher model is a large, floating-point BERT model, and the student model is a smaller, integer-based version of myself.\r\n\r\nDuring training, the student model learns to approximate the behavior of the teacher model by minimizing the difference between their outputs. This process allows me to maintain high accuracy while using fewer computational resources and reducing my memory footprint.\r\n\r\nSo, to summarize: I'm a BERT-based language model that uses knowledge distillation to reduce precision and improve efficiency, rather than traditional quantization techniques.\" \r\n```\r\n\r\n\r\n\r\n\r\n\r\nModel : https://huggingface.co/TechxGenus/Meta-Llama-3-70B-GPTQ or https://huggingface.co/MaziyarPanahi/Meta-Llama-3-70B-Instruct-GPTQ (tested with both)\r\nModel Args: --max-model-len 8192 --quantization marlin --dtype half --gpu-memory-utilization 0.99 --trust-remote-code --enforce-eager --kv-cache-dtype fp8\r\n\r\nResponse on A100:\r\n```\r\n\"Thank you for asking!\r\n\r\nI am a type of artificial intelligence language model, to be specific, I'm a transformer-based language model. My architecture is based on the transformer model introduced in the paper \"Attention is All You Need\" by Vaswani et al. in 2017.\r\n\r\nAs for quantization, I don't use traditional quantization methods like 8-bit or 16-bit integer quantization. Instead, I rely on a technique called \"floating-point quantization\" which allows me to represent my weights and activations using a combination of floating-point numbers and integers.\r\n\r\nIn more detail, my model uses a combination of the following techniques:\r\n\r\n1. **Weight pruning**: This involves removing redundant or unnecessary weights from my neural network to reduce its size and computational requirements.\r\n2. **Knowledge distillation**: This is a process where a smaller model (like me) is trained to mimic the behavior of a larger, more complex model. This helps to transfer knowledge from the larger model to me.\r\n3. **Quantization-aware training**: During training, my weights and activations are simulated using lower precision data types (like integers) to mimic the effects of quantization. This helps me to learn how to represent information efficiently even with reduced precision.\r\n\r\nThese techniques allow me to maintain a good balance between accuracy and computational efficiency, I can provide helpful responses quickly while still being able to understand and respond accurately.\r\n\r\nPlease let me know if you have any other questions or if there's anything else I can help with!\"\r\n```\r\n\r\nResponse on 2 A10s (tensor parallelism):\r\n```\r\n\"into the 2000s and the 2000s of the 2000s and the 2000s of the 2000s and the 2000s of the 2000s.\"\r\n```\r\nor something else equally psychotic and random (it produces a different response every time so I have a feeling this has to do with nondeterminism). Heres another example of a response on the same prompt:\r\n\r\n```\r\n\"Anita; and the last; and the first; and the last; and the first; and the last.\r\n\r\nAnita;\r\nand the last;\r\nand the first;\r\nand the last;\r\nand the first;\r\nand the last.\r\n[Anita];\r\n[the last];\r\n[the first];\r\n[the last];\r\n[the first];\r\n[the last].\r\nAnita;\r\nand the last;\r\nand the first;\r\nand the last;\r\nand the first;\r\nand the last.\r\n[Anita]];\r\n[the last]];\r\n[[the first]];\r\n[[the last]];\r\n[[the first]];\r\n[[the last]].\r\nAnita:;\r\nand [[the last]];\r\n[[the first]]];\r\n[[the last]]];\r\n[[the first]]];\r\n[[the last]].\r\nAnita:;\r\n[[last]]];\r\n[[first]]];\r\n[[last]]];\r\n[[first]];\r\n[[last]].\r\nAnita:;\r\n [[[last]]],\r\n [[[first]]],\r\n [[[last]]],\r\n [[[first]]],\r\n [[[last]]].\r\nAnita:;\r\n [[[last]],\r\n [[[first]],\r\n [[[last]],\r\n{{{first}},\r\n{{{last}}].\r\nAnita:;\r\n{{{last}},\r\n{{{first}},\r\n{{{last}},\r\n{{{first}},\r\n{{{last}}].\r\nAnita:;\r\n{{{last}},\r\n{{{first}},\r\n{{{last}},\r\n{{{first}},\r\n{{{last}}].\r\n\r\nThis is a series of nested brackets,2019. The output is a series of nested brackets.\"\r\n```\r\n\r\n\r\nLet me know what you all think, I am very confused and surprised at this bug since both GPUs are ampere architecture and I wasn't able to pin the bug down on anything EXCEPT the difference in GPU in the deployment hardware. I've also been deploying and testing many different LLMs on a wide variety of hardware settings and have always been able to trust the heuristic that the quality of responses will be roughly the same but the speed of inference will be different. \r\n\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-24T17:32:32Z",
    "closed_at": "2024-07-27T21:52:35Z",
    "author": "certara-joe-schwartz",
    "comments_count": 8,
    "comments": [
      {
        "author": "mgoin",
        "body": "Thanks a lot for posting this issue @joe-schwartz-certara ! We will take a close look at the results (appreciate the repro steps). I'll be interested to see if with or without `--kv-cache-dtype fp8` has any impact on the sensitivity.",
        "created_at": "2024-06-24T20:32:54Z"
      },
      {
        "author": "certara-joe-schwartz",
        "body": "> Thanks a lot for posting this issue @joe-schwartz-certara ! We will take a close look at the results (appreciate the repro steps). I'll be interested to see if with or without `--kv-cache-dtype fp8` has any impact on the sensitivity.\r\n\r\nI can try to find a different model to use and test but the issue is I won't even be able to run llama3-70b on 2 a10s without that optimization. In the a100 situation, it's the same kind of dependable llama3 that I know and love whether or not I turn on fp8 for the cache. Remember too that I run llama3-70b awq just fine with fp8 kv cache on any GPU and get pretty much the same responses for my more structured prompts. From my understanding, kv cache dtype won't affect anything at inference time because vllm upscales back to regular precision when its time to use it. ",
        "created_at": "2024-06-24T21:59:28Z"
      },
      {
        "author": "JaheimLee",
        "body": "Similar issue. I run Qwen2-72B-Instruct-GPTQ-Int4 on 4 3090. The result may be different even though `use_beam_search=True`",
        "created_at": "2024-06-27T03:44:29Z"
      },
      {
        "author": "llmpros",
        "body": "I have various types of GPU, including A10, and can reproduce to poke it around",
        "created_at": "2024-07-02T04:04:28Z"
      },
      {
        "author": "mgoin",
        "body": "This should be fixed by this PR: https://github.com/vllm-project/vllm/pull/6795",
        "created_at": "2024-07-25T19:54:26Z"
      },
      {
        "author": "JohnTheNerd",
        "body": "I can confirm this issue no longer occurs on my setup after building #6795. Running on 2xRTX3090, with the following arguments/environment:\r\n\r\n```\r\nexport LD_LIBRARY_PATH=/usr/local/cuda-12.2/lib64:$LD_LIBRARY_PATH\r\nexport PATH=/usr/local/cuda-12.2/bin:$PATH\r\nexport CUDACXX=/usr/local/cuda-12.2/bin/nvcc\r\n\r\nexport VLLM_ATTENTION_BACKEND=FLASHINFER\r\n\r\nset -e\r\n\r\n/usr/bin/python3 -m vllm.entrypoints.openai.api_server \\\r\n--model=\"/mnt/disk/models/Meta-Llama-3.1-70B-Instruct-AWQ-INT4\" \\\r\n--served-model-name=\"llama-3.1-70b\" \\\r\n--max-model-len=\"11264\" \\\r\n--dtype=\"auto\" \\\r\n--gpu-memory-utilization=\"0.983\" \\\r\n--distributed-executor-backend=\"mp\" \\\r\n--enable-chunked-prefill=false \\\r\n--quantization=\"awq_marlin\" \\\r\n--enforce-eager \\\r\n--tensor-parallel-size=\"2\" \\\r\n--swap-space=\"1\" \\\r\n--enable-prefix-caching \\\r\n--disable-log-requests \\\r\n--disable-log-stats \\\r\n--host=\"0.0.0.0\" --port=\"5000\"\r\n```",
        "created_at": "2024-07-27T01:02:03Z"
      },
      {
        "author": "962086838",
        "body": "@JohnTheNerd \r\n\r\nHi John, I still find the problem exists on 4 A800 card. \r\nI run your command and serving the llama 3.1-70B. But I got the error:\r\n```\r\nProcess Process-1:\r\nTraceback (most recent call last):\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\r\n    self.run()\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/multiprocessing/process.py\", line 108, in run\r\n    self._target(*self._args, **self._kwargs)\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 217, in run_rpc_server                                              server = AsyncEngineRPCServer(async_engine_args, usage_context, port)\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/entrypoints/openai/rpc/server.py\", line 25, in __init__\r\n    self.engine = AsyncLLMEngine.from_engine_args(async_engine_args,\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 471, in from_engine_args\r\n    engine = cls(\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 381, in __init__\r\n    self.engine = self._init_engine(*args, **kwargs)                                                                                                                                  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 552, in _init_engine\r\n    return engine_class(*args, **kwargs)                                                                                                                                              File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 249, in __init__\r\n    self.model_executor = executor_class(                                                                                                                                             File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 215, in __init__\r\n    super().__init__(*args, **kwargs)                                                                                                                                                 File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 25, in __init__\r\n    super().__init__(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/executor/executor_base.py\", line 47, in __init__\r\n    self._init_executor()\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 138, in _init_executor\r\n    self._run_workers(\"load_model\",\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 192, in _run_workers\r\n    driver_worker_output = driver_worker_method(*args, **kwargs)\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/worker/worker.py\", line 139, in load_model\r\n    self.model_runner.load_model()                                                                                                                                                    File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 722, in load_model\r\n    self.model = get_model(model_config=self.model_config,                                                                                                                            File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/model_executor/model_loader/__init__.py\", line 21, in get_model\r\n    return loader.load_model(model_config=model_config,\r\n  File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 324, in load_model\r\n    model = _initialize_model(model_config, self.load_config,                                                                                                                         File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 152, in _initialize_model\r\n    quant_config = _get_quantization_config(model_config, load_config)                                                                                                                File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/model_executor/model_loader/loader.py\", line 93, in _get_quantization_config\r\n    quant_config = get_quant_config(model_config, load_config)                                                                                                                        File \"/root/anaconda3/envs/hurry_up_hhw/lib/python3.10/site-packages/vllm/model_executor/model_loader/weight_utils.py\", line 173, in get_quant_config\r\n    raise ValueError(                                                                                                                                                               ValueError: Cannot find the config file for awq_marlin\r\n```\r\n\r\nMy environment is \r\n```\r\nvllm==0.5.4\r\ntorch==2.4.0\r\ncuda==12.1\r\n```\r\nIs it a problem of cuda version?\r\n\r\n",
        "created_at": "2024-08-08T09:44:44Z"
      },
      {
        "author": "JohnTheNerd",
        "body": "@962086838 I'm not sure, i currently run llama 3.1 without any issues. is it possible you're trying to run the unquantized version with the awq_marlin option? it is meant to be used on AWQ quants only.",
        "created_at": "2024-08-08T12:50:37Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5938": {
    "issue_number": 5938,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5938",
    "title": "[Bug]: Illegal memory access for MoE kernel with large workloads",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 20.04.6 LTS (x86_64)\r\nGCC version: (Ubuntu 9.4.0-1ubuntu1~20.04.2) 9.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.29.3\r\nLibc version: glibc-2.31\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-6.5.0-1022-gcp-x86_64-with-glibc2.31\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA L4\r\nNvidia driver version: 535.183.01\r\ncuDNN version: Probably one of the following:\r\n/usr/lib/x86_64-linux-gnu/libcudnn.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_adv_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_cnn_train.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_infer.so.8.9.0\r\n/usr/lib/x86_64-linux-gnu/libcudnn_ops_train.so.8.9.0\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nByte Order:                         Little Endian\r\nAddress sizes:                      46 bits physical, 48 bits virtual\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nNUMA node(s):                       1\r\nVendor ID:                          GenuineIntel\r\nCPU family:                         6\r\nModel:                              85\r\nModel name:                         Intel(R) Xeon(R) CPU @ 2.20GHz\r\nStepping:                           7\r\nCPU MHz:                            2200.174\r\nBogoMIPS:                           4400.34\r\nHypervisor vendor:                  KVM\r\nVirtualization type:                full\r\nL1d cache:                          512 KiB\r\nL1i cache:                          512 KiB\r\nL2 cache:                           16 MiB\r\nL3 cache:                           38.5 MiB\r\nNUMA node0 CPU(s):                  0-31\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced / Automatic IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI Syscall hardening, KVM SW loop\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology nonstop_tsc cpuid tsc_known_freq pni pclmulqdq ssse3 fma cx16 pcid sse4_1 sse4_2 x2apic movbe popcnt aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced fsgsbase tsc_adjust bmi1 hle avx2 smep bmi2 erms invpcid rtm mpx avx512f avx512dq rdseed adx smap clflushopt clwb avx512cd avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves arat avx512_vnni md_clear arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.24.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.1\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.24.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.1                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      0-31    0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\n```\r\n\n\n### üêõ Describe the bug\n\nIllegal memory access for MoE triton kernel when the workload (e.g., batch size) is too large. To reproduce:\r\n\r\n```\r\npython3 ./benchmarks/kernels/benchmark_moe.py --tp-size 2 --batch-size 74899\r\n```\r\n\r\nOutput\r\n\r\n```text\r\n  File \"/home/ray/default/vllm/./benchmarks/kernels/benchmark_moe.py\", line 70, in run\r\n    fused_moe(\r\n  File \"/tmp/ray/session_2024-06-27_10-06-48_118980_5595/runtime_resources/working_dir_files/_ray_pkg_ef0e5109bc8b4140628503119c10e0b2c9ea3f17/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 519, in fused_moe\r\n    return fused_experts(hidden_states,\r\n  File \"/tmp/ray/session_2024-06-27_10-06-48_118980_5595/runtime_resources/working_dir_files/_ray_pkg_ef0e5109bc8b4140628503119c10e0b2c9ea3f17/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 449, in fused_experts\r\n    invoke_fused_moe_kernel(intermediate_cache2,\r\n  File \"/tmp/ray/session_2024-06-27_10-06-48_118980_5595/runtime_resources/working_dir_files/_ray_pkg_ef0e5109bc8b4140628503119c10e0b2c9ea3f17/vllm/model_executor/layers/fused_moe/fused_moe.py\", line 245, in invoke_fused_moe_kernel\r\n    fused_moe_kernel[grid](\r\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/triton/runtime/jit.py\", line 167, in <lambda>\r\n    return lambda *args, **kwargs: self.run(grid=grid, warmup=False, *args, **kwargs)\r\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/triton/runtime/jit.py\", line 425, in run\r\n    kernel.run(grid_0, grid_1, grid_2, kernel.num_warps, kernel.num_ctas,  # number of warps/ctas per instance\r\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 255, in __getattribute__\r\n    self._init_handles()\r\n  File \"/home/ray/anaconda3/lib/python3.10/site-packages/triton/compiler/compiler.py\", line 250, in _init_handles\r\n    self.module, self.function, self.n_regs, self.n_spills = driver.utils.load_binary(\r\nRuntimeError: Triton Error [CUDA]: an illegal memory access was encountered\r\n```\r\n\r\nWe have seen this problem on L4 and A100 GPUs. I also tried to tune this particular workload using different block sizes, but none of the configs could bypass the error. Since we usually don't use such a large batch size (number of tokens), this bug should not be critical at least for now.\r\n\r\n\r\nAlso cc @pcmoritz @WoosukKwon @Yard1 \r\n",
    "state": "closed",
    "labels": [
      "bug",
      "stale"
    ],
    "created_at": "2024-06-27T22:07:57Z",
    "closed_at": "2024-11-24T02:09:13Z",
    "author": "comaniac",
    "comments_count": 3,
    "comments": [
      {
        "author": "Minami-su",
        "body": "+1,me too.\r\ntp_size 2 \r\nQwen2_72b\r\n2 X A800 80G",
        "created_at": "2024-06-28T00:42:59Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically marked as stale because it has not had any activity within 90 days. It will be automatically closed if no further activity occurs within 30 days. Leave a comment if you feel this issue should remain open. Thank you!",
        "created_at": "2024-10-25T02:03:40Z"
      },
      {
        "author": "github-actions[bot]",
        "body": "This issue has been automatically closed due to inactivity. Please feel free to reopen if you feel it is still relevant. Thank you!",
        "created_at": "2024-11-24T02:09:13Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5982": {
    "issue_number": 5982,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5982",
    "title": "[Bug]: AttributeError: 'NoneType' object has no attribute 'prefill_metadata'",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nrun LLaVA-NeXT | llava-hf/llava-v1.6-mistral-7b-hf  \r\n\r\n\r\n\r\n\r\n\r\npython -m vllm.entrypoints.openai.api_server --model /ai/LLaVA-NeXT --image-token-id 32000 --image-input-shape 1,3,336,336  --image-input-type pixel_values --image-feature-size 65856 --chat-template template_llava.jinja --host 19*** --port 10860   --trust-remote-code --tensor-parallel-size 1  --dtype=half --disable-custom-all-reduce \r\n\r\n\r\n![image](https://github.com/vllm-project/vllm/assets/40717349/e20a7cab-5052-4f04-b960-c5426e2717f2)\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-29T06:31:25Z",
    "closed_at": "2024-07-04T05:15:15Z",
    "author": "lonngxiang",
    "comments_count": 7,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Please provide more information on your environment by running the command at the beginning of your post (under \"Your current environment\"). The issue seems to come from having incompatible packages installed so you might want to reset your Python environment.",
        "created_at": "2024-06-29T09:22:11Z"
      },
      {
        "author": "tboser",
        "body": "> Please provide more information on your environment by running the command at the beginning of your post (under \"Your current environment\"). The issue seems to come from having incompatible packages installed so you might want to reset your Python environment.\r\n\r\nHello, I've run into this same issue. Here's the the output of `collect_env.py` when I run it.\r\n\r\n```\r\n(michaelocr_ai) (dev_tools) [07/03/2024 09:54:17PM] [thomas:~/vllm]$ python collect_env.py\r\nCollecting environment information...\r\nWARNING 07-03 22:01:59 _custom_ops.py:14] Failed to import from vllm._C with ModuleNotFoundError(\"No module named 'vllm._C'\")\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.11.9 (main, Apr  6 2024, 17:59:24) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.5.40\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 3080\r\nNvidia driver version: 546.17\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      39 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             16\r\nOn-line CPU(s) list:                0-15\r\nVendor ID:                          GenuineIntel\r\nModel name:                         11th Gen Intel(R) Core(TM) i9-11900KF @ 3.50GHz\r\nCPU family:                         6\r\nModel:                              167\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 8\r\nSocket(s):                          1\r\nStepping:                           1\r\nBogoMIPS:                           7007.99\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap avx512ifma clflushopt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves avx512vbmi umip avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          384 KiB (8 instances)\r\nL1i cache:                          256 KiB (8 instances)\r\nL2 cache:                           4 MiB (8 instances)\r\nL3 cache:                           16 MiB (1 instance)\r\nVulnerability Gather data sampling: Unknown: Dependent on hypervisor status\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.16.2\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X                              N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\r\nFWIW, my deps are generated using pip compile with vllm pinned to `vllm>=0.5.0.post1` so they should in theory be compatible with vllm. LMK if you see anything wrong/if I can share other outputs to help debug this. Thanks!\r\n",
        "created_at": "2024-07-04T05:04:37Z"
      },
      {
        "author": "DarkLight1337",
        "body": "How did you run vLLM? Can you show the command/code?",
        "created_at": "2024-07-04T05:07:18Z"
      },
      {
        "author": "tboser",
        "body": "> How did you run vLLM? Can you show the command/code?\r\n\r\nSure, here's the \"model\" code. It gets initialized in a webserver from which I want to serve the model:\r\n```\r\nfrom typing import Any\r\n\r\nfrom PIL import Image\r\nfrom vllm import LLM, SamplingParams\r\n\r\nfrom ai.lib.py.models.model import ModelBase\r\n\r\n\r\nclass Llava(ModelBase):\r\n    PROMPT = \"[INST] <image>\\ label this. [/INST]\"\r\n\r\n    def __init__(self) -> None:\r\n        model_id = \"llava-hf/llava-v1.6-mistral-7b-hf\"\r\n        self.llm = LLM(\r\n            model=model_id,\r\n            trust_remote_code=True,\r\n            max_model_len=4096,\r\n            tensor_parallel_size=1,\r\n            image_input_type=\"pixel_values\",\r\n            image_token_id=32000,\r\n            image_input_shape=\"1,3,640,760\",\r\n            image_feature_size=17752,\r\n            dtype='half',\r\n        )\r\n        self.sampling_params = SamplingParams(\r\n            temperature=0.1, top_p=0.95, max_tokens=2048\r\n        )\r\n\r\n    def predict(self, input: Any) -> str:\r\n        image = Image.open(input).convert(\"RGB\").resize((640, 760))\r\n        outputs = self.llm.generate(\r\n            {\r\n                \"prompt\": self.PROMPT,\r\n                \"multi_modal_data\": {\"image\": image},\r\n            },\r\n            sampling_params=self.sampling_params,\r\n        )\r\n        for o in outputs:\r\n            generated_text = o.outputs[0].text\r\n            print(generated_text)\r\n        return \" \".join([o.outputs[0].text for o in outputs])\r\n```",
        "created_at": "2024-07-04T05:10:25Z"
      },
      {
        "author": "DarkLight1337",
        "body": "I got a similar issue recently and it turns out that it's because vLLM cannot allocate blocks for the model. Here, I think you set `image_feature_size` to a value that is too high (normally it should be around 2k or so, not 60k or 17k).\r\n\r\nAnyways, the `--image-feature-size` argument has since been removed (it is now computed automatically by #6089) so you should not run into this issue anymore.",
        "created_at": "2024-07-04T05:11:48Z"
      },
      {
        "author": "tboser",
        "body": "> I got a similar issue recently and it turns out that it's because vLLM cannot allocate blocks for the model. Here, I think you set `image_feature_size` to a value that is too high (normally it should be around 2k or so, not 60k or 17k).\r\n> \r\n> Anyways, the `--image-feature-size` argument has since been removed (it is now computed automatically by #6089) so you should not run into this issue anymore.\r\n\r\nYeah I noticed this was removed - I wasn't able to build from source unfortunately so I'm stuck on the older version. Not a huge deal, I can wait for the next pypi release. \r\nAppreciate you taking a look, thanks!",
        "created_at": "2024-07-04T05:14:12Z"
      },
      {
        "author": "DarkLight1337",
        "body": "The next release should be just around the corner! See #5806 for more details.",
        "created_at": "2024-07-04T05:14:42Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/5983": {
    "issue_number": 5983,
    "issue_url": "https://github.com/vllm-project/vllm/issues/5983",
    "title": "[Bug]: TypeError: FlashAttentionMetadata.__init__() missing 10 required positional arguments",
    "body": "### Your current environment\n\n```text\r\nThe output of `python collect_env.py`\r\n```\r\n\n\n### üêõ Describe the bug\n\nrun LLaVA-NeXT errorÔºö\r\n\r\npython -m vllm.entrypoints.openai.api_server --model /ai/LLaVA-NeXT --image-token-id 32000 --image-input-shape 1,3,336,336  --image-input-type pixel_values --image-feature-size 65856 --chat-template template_llava.jinja --host 19*** --port 10860   --trust-remote-code --tensor-parallel-size 2  --dtype=half --disable-custom-all-reduce \r\n\r\n![image](https://github.com/vllm-project/vllm/assets/40717349/70d6a8e0-fcf6-425a-85a8-52854b9dabbf)\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-06-29T06:36:42Z",
    "closed_at": "2024-07-04T05:10:01Z",
    "author": "lonngxiang",
    "comments_count": 6,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "Please provide more information on your environment by running the command at the beginning of your post (under \"Your current environment\")",
        "created_at": "2024-06-29T09:18:52Z"
      },
      {
        "author": "lonngxiang",
        "body": "/home/anaconda3/envs/llm/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/anaconda3/envs/llm/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\nINFO 06-29 02:39:20 api_server.py:177] vLLM API server version 0.5.0.post1\r\nINFO 06-29 02:39:20 api_server.py:178] args: Namespace(host='192.168.2.238', port=10860, uvicorn_log_level='info', allow_credentials=False, allowed_origins=['*'], allowed_methods=['*'], allowed_headers=['*'], api_key=None, lora_modules=None, chat_template='template_llava.jinja', response_role='assistant', ssl_keyfile=None, ssl_certfile=None, ssl_ca_certs=None, ssl_cert_reqs=0, root_path=None, middleware=[], model='/ai/LLaVA-NeXT', tokenizer=None, skip_tokenizer_init=False, revision=None, code_revision=None, tokenizer_revision=None, tokenizer_mode='auto', trust_remote_code=True, download_dir=None, load_format='auto', dtype='auto', kv_cache_dtype='auto', quantization_param_path=None, max_model_len=None, guided_decoding_backend='outlines', distributed_executor_backend=None, worker_use_ray=False, pipeline_parallel_size=1, tensor_parallel_size=2, max_parallel_loading_workers=None, ray_workers_use_nsight=False, block_size=16, enable_prefix_caching=False, disable_sliding_window=False, use_v2_block_manager=False, num_lookahead_slots=0, seed=0, swap_space=4, gpu_memory_utilization=0.9, num_gpu_blocks_override=None, max_num_batched_tokens=None, max_num_seqs=256, max_logprobs=20, disable_log_stats=False, quantization=None, rope_scaling=None, rope_theta=None, enforce_eager=False, max_context_len_to_capture=None, max_seq_len_to_capture=8192, disable_custom_all_reduce=True, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config=None, enable_lora=False, max_loras=1, max_lora_rank=16, lora_extra_vocab_size=256, lora_dtype='auto', long_lora_scaling_factors=None, max_cpu_loras=None, fully_sharded_loras=False, device='auto', image_input_type='pixel_values', image_token_id=32000, image_input_shape='1,3,336,336', image_feature_size=65856, image_processor=None, image_processor_revision=None, disable_image_processor=False, scheduler_delay_factor=0.0, enable_chunked_prefill=False, speculative_model=None, num_speculative_tokens=None, speculative_max_model_len=None, speculative_disable_by_batch_size=None, ngram_prompt_lookup_max=None, ngram_prompt_lookup_min=None, model_loader_extra_config=None, preemption_mode=None, served_model_name=None, qlora_adapter_name_or_path=None, engine_use_ray=False, disable_log_requests=False, max_log_len=None)\r\n2024-06-29 02:39:23,558 INFO worker.py:1749 -- Started a local Ray instance.\r\nINFO 06-29 02:39:24 config.py:623] Defaulting to use mp for distributed inference\r\nINFO 06-29 02:39:24 llm_engine.py:161] Initializing an LLM engine (v0.5.0.post1) with config: model='/ai/LLaVA-NeXT', speculative_config=None, tokenizer='/ai/LLaVA-NeXT', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=/ai/LLaVA-NeXT)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\n/home/anaconda3/envs/llm/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/anaconda3/envs/llm/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\r\n  warn(\r\n(VllmWorkerProcess pid=20086) INFO 06-29 02:39:29 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\r\nINFO 06-29 02:39:29 utils.py:637] Found nccl from library libnccl.so.2\r\n(VllmWorkerProcess pid=20086) INFO 06-29 02:39:29 utils.py:637] Found nccl from library libnccl.so.2\r\nINFO 06-29 02:39:29 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=20086) INFO 06-29 02:39:29 pynccl.py:63] vLLM is using nccl==2.20.5\r\n(VllmWorkerProcess pid=20086) INFO 06-29 02:39:36 model_runner.py:160] Loading model weights took 7.3588 GB\r\nINFO 06-29 02:39:37 model_runner.py:160] Loading model weights took 7.3588 GB\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method determine_num_available_blocks: FlashAttentionMetadata.__init__() missing 10 required positional arguments: 'seq_lens', 'seq_lens_tensor', 'max_query_len', 'max_prefill_seq_len', 'max_decode_seq_len', 'query_start_loc', 'seq_start_loc', 'context_lens_tensor', 'block_tables', and 'use_cuda_graph', Traceback (most recent call last):\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 162, in determine_num_available_blocks\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     self.model_runner.profile_run()\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 844, in profile_run\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     self.execute_model(seqs, kv_caches)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 735, in execute_model\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     ) = self.prepare_input_tensors(seq_group_metadata_list)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 712, in prepare_input_tensors\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     attn_metadata = self.attn_backend.make_metadata(\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/attention/backends/flash_attn.py\", line 29, in make_metadata\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226]     return FlashAttentionMetadata(*args, **kwargs)\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226] TypeError: FlashAttentionMetadata.__init__() missing 10 required positional arguments: 'seq_lens', 'seq_lens_tensor', 'max_query_len', 'max_prefill_seq_len', 'max_decode_seq_len', 'query_start_loc', 'seq_start_loc', 'context_lens_tensor', 'block_tables', and 'use_cuda_graph'\r\n(VllmWorkerProcess pid=20086) ERROR 06-29 02:39:37 multiproc_worker_utils.py:226] \r\n[rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/entrypoints/openai/api_server.py\", line 196, in <module>\r\n[rank0]:     engine = AsyncLLMEngine.from_engine_args(\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 398, in from_engine_args\r\n[rank0]:     engine = cls(\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 349, in __init__\r\n[rank0]:     self.engine = self._init_engine(*args, **kwargs)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/engine/async_llm_engine.py\", line 473, in _init_engine\r\n[rank0]:     return engine_class(*args, **kwargs)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 236, in __init__\r\n[rank0]:     self._initialize_kv_caches()\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/engine/llm_engine.py\", line 313, in _initialize_kv_caches\r\n[rank0]:     self.model_executor.determine_num_available_blocks())\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/executor/distributed_gpu_executor.py\", line 38, in determine_num_available_blocks\r\n[rank0]:     num_blocks = self._run_workers(\"determine_num_available_blocks\", )\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/executor/multiproc_gpu_executor.py\", line 119, in _run_workers\r\n[rank0]:     driver_worker_output = driver_worker_method(*args, **kwargs)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/worker.py\", line 162, in determine_num_available_blocks\r\n[rank0]:     self.model_runner.profile_run()\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 844, in profile_run\r\n[rank0]:     self.execute_model(seqs, kv_caches)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n[rank0]:     return func(*args, **kwargs)\r\n[rank0]:   File \"/home/anaconda3/envs/llm/lib/python3.10/site-packages/vllm/worker/model_runner.py\", line 741, in execute_model\r\n[rank0]:     prefill_meta = attn_metadata.prefill_metadata\r\n[rank0]: AttributeError: 'NoneType' object has no attribute 'prefill_metadata'",
        "created_at": "2024-06-29T09:22:00Z"
      },
      {
        "author": "DarkLight1337",
        "body": "This doesn't look like the output of `python collect_env.py`.",
        "created_at": "2024-06-29T09:24:37Z"
      },
      {
        "author": "lonngxiang",
        "body": "Collecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: CentOS Linux 7 (Core) (x86_64)\r\nGCC version: (GCC) 4.8.5 20150623 (Red Hat 4.8.5-44)\r\nClang version: Could not collect\r\nCMake version: version 3.29.6\r\nLibc version: glibc-2.17\r\n\r\nPython version: 3.10.0 | packaged by conda-forge | (default, Nov 20 2021, 02:24:10) [GCC 9.4.0] (64-bit runtime)\r\nPython platform: Linux-3.10.0-1160.118.1.el7.x86_64-x86_64-with-glibc2.17\r\nIs CUDA available: True\r\nCUDA runtime version: 12.2.91\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: \r\nGPU 0: NVIDIA GeForce RTX 4090\r\nGPU 1: NVIDIA GeForce RTX 4090\r\n\r\nNvidia driver version: 550.78\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:          x86_64\r\nCPU op-mode(s):        32-bit, 64-bit\r\nByte Order:            Little Endian\r\nCPU(s):                8\r\nOn-line CPU(s) list:   0-7\r\nThread(s) per core:    1\r\nCore(s) per socket:    1\r\nSocket(s):             8\r\nNUMA node(s):          1\r\nVendor ID:             GenuineIntel\r\nCPU family:            6\r\nModel:                 106\r\nModel name:            Intel(R) Xeon(R) Platinum 8352V CPU @ 2.10GHz\r\nStepping:              6\r\nCPU MHz:               2099.998\r\nBogoMIPS:              4199.99\r\nVirtualization:        VT-x\r\nHypervisor vendor:     KVM\r\nVirtualization type:   full\r\nL1d cache:             32K\r\nL1i cache:             32K\r\nL2 cache:              4096K\r\nL3 cache:              16384K\r\nNUMA node0 CPU(s):     0-7\r\nFlags:                 fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl xtopology eagerfpu pni pclmulqdq vmx ssse3 fma cx16 pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd rsb_ctxsw ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rtm avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 arat avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq md_clear spec_ctrl intel_stibp arch_capabilities\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] sentence-transformers==2.7.0\r\n[pip3] torch==2.3.0\r\n[pip3] torchaudio==2.1.2+cu118\r\n[pip3] torchvision==0.16.2+cu118\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] numpy                     1.26.3                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] sentence-transformers     2.7.0                    pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchaudio                2.1.2+cu118              pypi_0    pypi\r\n[conda] torchvision               0.16.2+cu118             pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.0.post1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    GPU1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      PHB     0-7     0               N/A\r\nGPU1    PHB      X      0-7     0               N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks",
        "created_at": "2024-06-29T09:28:16Z"
      },
      {
        "author": "DarkLight1337",
        "body": "There may be some mismatched Python packages. Try reinstalling your Python environment.",
        "created_at": "2024-06-29T10:07:03Z"
      },
      {
        "author": "DarkLight1337",
        "body": "I got a similar issue recently and it turns out that it's because vLLM cannot allocate blocks for the model. Here, I think you set `image_feature_size` to a value that is too high (normally it should be around 2k or so, not 60k).\r\n\r\nAnyways, the `--image-feature-size` argument has since been removed (it is now computed automatically by #6089) so you should not run into this issue anymore.",
        "created_at": "2024-07-04T05:09:58Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6231": {
    "issue_number": 6231,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6231",
    "title": "[Bug]: relative path doesn't work for Lora adapter model",
    "body": "### Your current environment\r\n\r\n<details>\r\n  <summary>Environment Details</summary>\r\n\r\n\r\n```text\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.3 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)\r\nPython platform: Linux-5.15.0-94-generic-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA A40\r\nNvidia driver version: 535.154.05\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      52 bits physical, 57 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             96\r\nOn-line CPU(s) list:                0-95\r\nVendor ID:                          GenuineIntel\r\nModel name:                         Intel(R) Xeon(R) Gold 6342 CPU @ 2.80GHz\r\nCPU family:                         6\r\nModel:                              106\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 24\r\nSocket(s):                          2\r\nStepping:                           6\r\nCPU max MHz:                        3500.0000\r\nCPU min MHz:                        800.0000\r\nBogoMIPS:                           5600.00\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat_l3 invpcid_single ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect wbnoinvd dtherm ida arat pln pts avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid fsrm md_clear pconfig flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nL1d cache:                          2.3 MiB (48 instances)\r\nL1i cache:                          1.5 MiB (48 instances)\r\nL2 cache:                           60 MiB (48 instances)\r\nL3 cache:                           72 MiB (2 instances)\r\nNUMA node(s):                       2\r\nNUMA node0 CPU(s):                  0-23,48-71\r\nNUMA node1 CPU(s):                  24-47,72-95\r\nVulnerability Gather data sampling: Mitigation; Microcode\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Mitigation; Clear CPU buffers; SMT vulnerable\r\nVulnerability Retbleed:             Not affected\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] mypy==1.9.0\r\n[pip3] mypy-extensions==1.0.0\r\n[pip3] numpy==1.26.3\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] onnx==1.14.1\r\n[pip3] onnxruntime==1.18.1\r\n[pip3] sentence-transformers==3.0.1\r\n[pip3] torch==2.3.0\r\n[pip3] torchaudio==2.2.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] Could not collect\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    NIC0    NIC1    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X      SYS     SYS     0-23,48-71      0               N/A\r\nNIC0    SYS      X      PIX\r\nNIC1    SYS     PIX      X \r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n\r\nNIC Legend:\r\n\r\n  NIC0: mlx5_0\r\n  NIC1: mlx5_1\r\n```\r\n</details>\r\n\r\n\r\n### üêõ Describe the bug\r\n\r\nThis is a follow up issue of https://github.com/vllm-project/vllm/issues/6229. \r\nRelative path with `~` doesn't work for lora adapter path. We should fix it \r\n\r\nRelative Path - doesn't work\r\n![image](https://github.com/vllm-project/vllm/assets/4739316/b63a54a3-2b2d-4ea5-bf86-891a06143f76)\r\n\r\nAbsolute Path - works\r\n![image](https://github.com/vllm-project/vllm/assets/4739316/fc5aa124-83c0-4df2-8372-35d3a52671bb)\r\n\r\nI will submit a PR to expands the path to make it work.\r\n\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-08T23:32:55Z",
    "closed_at": "2024-07-22T22:42:41Z",
    "author": "Jeffwan",
    "comments_count": 0,
    "comments": []
  },
  "https://github.com/vllm-project/vllm/issues/6322": {
    "issue_number": 6322,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6322",
    "title": "[Bug]: VLLM 0.5.1 with LLaVA 1.6 exceptions",
    "body": "### Your current environment\n\nSee https://github.com/vllm-project/vllm/issues/6176\r\n\n\n### üêõ Describe the bug\n\nI have lots of image, where the service throws exception and after that must be restarted, because it stucks in exception mode, even for images, that worked before.\r\nExample image below.\r\n\r\n```\r\ncurl 'https://ai1.dev.init/multimodal-llava/v1/chat/completions' -k -H 'Content-Type: application/json' -d @- <<EOF\r\n{\r\n    \"model\": \"llava-hf/llava-v1.6-mistral-7b-hf\",\r\n    \"messages\": [\r\n        {\r\n            \"role\": \"user\",\r\n            \"content\": [\r\n                {\r\n                    \"type\": \"image_url\",\r\n                    \"image_url\": {\r\n                        \"url\": \"data:image/jpeg;base64,$(base64 -w 0 /opt/initai_copilot/data/images/image2015-7-20_16_17_54.png)\"\r\n                    }\r\n                },\r\n                {\r\n                    \"type\": \"text\",\r\n                    \"text\": \"Was ist in dem Bild?\"\r\n                }\r\n            ]\r\n        }\r\n    ],\r\n    \"temperature\": 0.2,\r\n    \"top_p\": 0.1,\r\n    \"top_k\": 20,\r\n    \"frequency_penalty\": 0.2\r\n}\r\nEOF\r\n```\r\n\r\nImage:\r\n![image2015-7-20_16_17_54](https://github.com/vllm-project/vllm/assets/2312884/7cde0c86-0b25-4b78-b662-129d584f4f8c)\r\n\r\nException\r\n\r\n```\r\nINFO:     Started server process [1]\r\nINFO:     Waiting for application startup.\r\nINFO:     Application startup complete.\r\nINFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\r\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\r\nINFO 07-11 06:04:31 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-11 06:04:36 metrics.py:295] Avg prompt throughput: 208.2 tokens/s, Avg generation throughput: 6.7 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.0%, CPU KV cache usage: 0.0%.\r\nINFO 07-11 06:04:42 metrics.py:295] Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 7.9 tokens/s, Running: 1 reqs, Swapped: 0 reqs, Pending: 0 reqs, GPU KV cache usage: 2.1%, CPU KV cache usage: 0.0%.\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226] Exception in worker VllmWorkerProcess while processing method start_worker_execution_loop: Attempted to assign 776 = 776 image tokens to 826 placeholders, Traceback (most recent call last):\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_worker_utils.py\", line 223, in _run_worker_process\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     output = executor(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 64, in start_worker_execution_loop\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     output = self.execute_model(execute_model_req=None)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     output = self.model_runner.execute_model(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return func(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     hidden_or_intermediate_states = model_executable(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return self._call_impl(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     return forward_call(*args, **kwargs)\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     inputs_embeds = merge_vision_embeddings(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]     raise ValueError(\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226] ValueError: Attempted to assign 776 = 776 image tokens to 826 placeholders\r\n(VllmWorkerProcess pid=76) ERROR 07-11 06:04:42 multiproc_worker_utils.py:226]\r\nERROR 07-11 06:04:42 async_llm_engine.py:53] Engine background task failed\r\nERROR 07-11 06:04:42 async_llm_engine.py:53] Traceback (most recent call last):\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return_value = task.result()\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     result = task.result()\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     request_outputs = await self.engine.step_async(virtual_engine)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     output = await self.model_executor.execute_model_async(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 173, in execute_model_async\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return await self._driver_execute_model_async(execute_model_req)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 160, in _driver_execute_model_async\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return await self.driver_exec_model(execute_model_req)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     result = self.fn(*self.args, **self.kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     output = self.model_runner.execute_model(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return func(*args, **kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     hidden_or_intermediate_states = model_executable(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return self._call_impl(*args, **kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     return forward_call(*args, **kwargs)\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     inputs_embeds = merge_vision_embeddings(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]   File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\nERROR 07-11 06:04:42 async_llm_engine.py:53]     raise ValueError(\r\nERROR 07-11 06:04:42 async_llm_engine.py:53] ValueError: Attempted to assign 776 = 776 image tokens to 826 placeholders\r\nException in callback functools.partial(<function _log_task_completion at 0x7d88c3437880>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7d88ab11c1f0>>)\r\nhandle: <Handle functools.partial(<function _log_task_completion at 0x7d88c3437880>, error_callback=<bound method AsyncLLMEngine._error_callback of <vllm.engine.async_llm_engine.AsyncLLMEngine object at 0x7d88ab11c1f0>>)>\r\nTraceback (most recent call last):\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 43, in _log_task_completion\r\n    return_value = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 595, in run_engine_loop\r\n    result = task.result()\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 540, in engine_step\r\n    request_outputs = await self.engine.step_async(virtual_engine)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 241, in step_async\r\n    output = await self.model_executor.execute_model_async(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/distributed_gpu_executor.py\", line 173, in execute_model_async\r\n    return await self._driver_execute_model_async(execute_model_req)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/executor/multiproc_gpu_executor.py\", line 160, in _driver_execute_model_async\r\n    return await self.driver_exec_model(execute_model_req)\r\n  File \"/usr/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\r\n    result = self.fn(*self.args, **self.kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/worker_base.py\", line 271, in execute_model\r\n    output = self.model_runner.execute_model(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\r\n    return func(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/worker/model_runner.py\", line 1243, in execute_model\r\n    hidden_or_intermediate_states = model_executable(\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1532, in _wrapped_call_impl\r\n    return self._call_impl(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1541, in _call_impl\r\n    return forward_call(*args, **kwargs)\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/llava_next.py\", line 494, in forward\r\n    inputs_embeds = merge_vision_embeddings(\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/model_executor/models/utils.py\", line 35, in merge_vision_embeddings\r\n    raise ValueError(\r\nValueError: Attempted to assign 776 = 776 image tokens to 826 placeholders\r\n\r\nThe above exception was the direct cause of the following exception:\r\n\r\nTraceback (most recent call last):\r\n  File \"uvloop/cbhandles.pyx\", line 63, in uvloop.loop.Handle._run\r\n  File \"/usr/local/lib/python3.10/dist-packages/vllm/engine/async_llm_engine.py\", line 55, in _log_task_completion\r\n    raise AsyncEngineDeadError(\r\nvllm.engine.async_llm_engine.AsyncEngineDeadError: Task finished unexpectedly. This should never happen! Please open an issue on Github. See stack trace above for theactual cause.\r\nINFO:     192.168.6.1:46738 - \"POST /v1/chat/completions HTTP/1.0\" 400 Bad Request\r\n```\r\n\r\nConfig of Docker image:\r\n\r\n```\r\nservices:\r\n  vllm-llava:\r\n    image: vllm/vllm-openai:v0.5.1\r\n    container_name: vllm-llava\r\n    ipc: host\r\n    deploy:\r\n      resources:\r\n        reservations:\r\n          devices:\r\n            - driver: nvidia\r\n              capabilities: [ gpu ]\r\n    environment:\r\n      - HTTPS_PROXY=http://proxy.dev.init:3128\r\n      - HTTP_PROXY=http://proxy.dev.init:3128\r\n      - NO_PROXY=10.9.240.0/22,127.0.0.0/8\r\n      - NVIDIA_VISIBLE_DEVICES=2,3\r\n      - HF_TOKEN=$HF_TOKEN\r\n      - VLLM_NO_USAGE_STATS=1\r\n    volumes:\r\n      - /mnt/sda/huggingface:/root/.cache/huggingface\r\n      - .:/opt/vllm\r\n    ports:\r\n      - \"8003:8000\"\r\n    command:\r\n      - --model=llava-hf/llava-v1.6-mistral-7b-hf\r\n      # - --chat-template=/opt/vllm/template_mixtral.jinja\r\n      # - --max-model-len=24576\r\n      - --tensor-parallel-size=2\r\n      # - --swap-space=5\r\n      - --gpu-memory-utilization=0.3\r\n      # - --max-num-batched-tokens=2048\r\n      - --disable-log-requests\r\n      - --enforce-eager\r\n      # - --enable-chunked-prefill\r\n    restart: unless-stopped\r\n\r\n```\r\n",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-11T06:11:25Z",
    "closed_at": "2024-07-11T17:21:12Z",
    "author": "andrePankraz",
    "comments_count": 1,
    "comments": [
      {
        "author": "DarkLight1337",
        "body": "I'm able to repro this locally. Seems that there is *still* some problem with the feature size calculation...",
        "created_at": "2024-07-11T12:16:38Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6342": {
    "issue_number": 6342,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6342",
    "title": "[Bug]: OpenAI batch file format pydantic validation error",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...\r\nPyTorch version: 2.3.0+cu121\r\nIs debug build: False\r\nCUDA used to build PyTorch: 12.1\r\nROCM used to build PyTorch: N/A\r\n\r\nOS: Ubuntu 22.04.4 LTS (x86_64)\r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0\r\nClang version: Could not collect\r\nCMake version: version 3.30.0\r\nLibc version: glibc-2.35\r\n\r\nPython version: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0] (64-bit runtime)\r\nPython platform: Linux-5.15.153.1-microsoft-standard-WSL2-x86_64-with-glibc2.35\r\nIs CUDA available: True\r\nCUDA runtime version: 12.1.105\r\nCUDA_MODULE_LOADING set to: LAZY\r\nGPU models and configuration: GPU 0: NVIDIA GeForce RTX 4080 Laptop GPU\r\nNvidia driver version: 556.12\r\ncuDNN version: Could not collect\r\nHIP runtime version: N/A\r\nMIOpen runtime version: N/A\r\nIs XNNPACK available: True\r\n\r\nCPU:\r\nArchitecture:                       x86_64\r\nCPU op-mode(s):                     32-bit, 64-bit\r\nAddress sizes:                      39 bits physical, 48 bits virtual\r\nByte Order:                         Little Endian\r\nCPU(s):                             32\r\nOn-line CPU(s) list:                0-31\r\nVendor ID:                          GenuineIntel\r\nModel name:                         13th Gen Intel(R) Core(TM) i9-13900HX\r\nCPU family:                         6\r\nModel:                              183\r\nThread(s) per core:                 2\r\nCore(s) per socket:                 16\r\nSocket(s):                          1\r\nStepping:                           1\r\nBogoMIPS:                           4838.39\r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc rep_good nopl xtopology tsc_reliable nonstop_tsc cpuid pni pclmulqdq vmx ssse3 fma cx16 sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand hypervisor lahf_lm abm 3dnowprefetch ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid rdseed adx smap clflushopt clwb sha_ni xsaveopt xsavec xgetbv1 xsaves avx_vnni umip waitpkg gfni vaes vpclmulqdq rdpid movdiri movdir64b fsrm md_clear serialize flush_l1d arch_capabilities\r\nVirtualization:                     VT-x\r\nHypervisor vendor:                  Microsoft\r\nVirtualization type:                full\r\nL1d cache:                          768 KiB (16 instances)\r\nL1i cache:                          512 KiB (16 instances)\r\nL2 cache:                           32 MiB (16 instances)\r\nL3 cache:                           36 MiB (1 instance)\r\nVulnerability Gather data sampling: Not affected\r\nVulnerability Itlb multihit:        Not affected\r\nVulnerability L1tf:                 Not affected\r\nVulnerability Mds:                  Not affected\r\nVulnerability Meltdown:             Not affected\r\nVulnerability Mmio stale data:      Not affected\r\nVulnerability Retbleed:             Mitigation; Enhanced IBRS\r\nVulnerability Spec rstack overflow: Not affected\r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp\r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization\r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\r\nVulnerability Srbds:                Not affected\r\nVulnerability Tsx async abort:      Not affected\r\n\r\nVersions of relevant libraries:\r\n[pip3] flashinfer==0.0.8+cu121torch2.3\r\n[pip3] numpy==1.26.4\r\n[pip3] nvidia-nccl-cu12==2.20.5\r\n[pip3] torch==2.3.0\r\n[pip3] torchvision==0.18.0\r\n[pip3] transformers==4.42.3\r\n[pip3] triton==2.3.0\r\n[conda] flashinfer                0.0.8+cu121torch2.3          pypi_0    pypi\r\n[conda] numpy                     1.26.4                   pypi_0    pypi\r\n[conda] nvidia-nccl-cu12          2.20.5                   pypi_0    pypi\r\n[conda] torch                     2.3.0                    pypi_0    pypi\r\n[conda] torchvision               0.18.0                   pypi_0    pypi\r\n[conda] transformers              4.42.3                   pypi_0    pypi\r\n[conda] triton                    2.3.0                    pypi_0    pypi\r\nROCM Version: Could not collect\r\nNeuron SDK Version: N/A\r\nvLLM Version: 0.5.1\r\nvLLM Build Flags:\r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled\r\nGPU Topology:\r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID\r\nGPU0     X                              N/A\r\n\r\nLegend:\r\n\r\n  X    = Self\r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\r\n  PIX  = Connection traversing at most a single PCIe bridge\r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nRunning this command to do batch inference through API, returns the following error. The input.jsonl is as per required format.\r\n\r\n`python -m vllm.entrypoints.openai.run_batch -i input.jsonl -o results.jsonl --model Granther/Gemma-2-9B-Instruct-4Bit-GPTQ --max_model_len 3000\r\n`\r\n\r\nError Traceback:\r\n\r\n> [rank0]: Traceback (most recent call last):\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\r\n[rank0]:     return _run_code(code, main_globals, None,\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/runpy.py\", line 86, in _run_code\r\n[rank0]:     exec(code, run_globals)\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/run_batch.py\", line 146, in <module>\r\n[rank0]:     asyncio.run(main(args))\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/asyncio/runners.py\", line 44, in run\r\n[rank0]:     return loop.run_until_complete(main)\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/asyncio/base_events.py\", line 649, in run_until_complete\r\n[rank0]:     return future.result()\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/run_batch.py\", line 130, in main\r\n[rank0]:     responses = await asyncio.gather(*response_futures)\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/site-packages/vllm/entrypoints/openai/run_batch.py\", line 93, in run_request\r\n[rank0]:     response=BatchResponseData(\r\n[rank0]:   File \"/home/arsal/anaconda3/envs/vllm/lib/python3.10/site-packages/pydantic/main.py\", line 193, in __init__\r\n[rank0]:     self.__pydantic_validator__.validate_python(data, self_instance=self)\r\n[rank0]: pydantic_core._pydantic_core.ValidationError: 1 validation error for BatchResponseData\r\n[rank0]: body\r\n[rank0]:   Field required [type=missing, input_value={'status_code': 400, 'req...5c4a7799ad445a114ebff4'}, input_type=dict]\r\n[rank0]:     For further information visit https://errors.pydantic.dev/2.8/v/missing",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-11T16:24:17Z",
    "closed_at": "2024-07-15T04:06:10Z",
    "author": "ArsalShakil",
    "comments_count": 1,
    "comments": [
      {
        "author": "zifeitong",
        "body": "The \"body\" field should be Optional (e.g. when there are errors), let me send a fix.",
        "created_at": "2024-07-11T18:03:30Z"
      }
    ]
  },
  "https://github.com/vllm-project/vllm/issues/6461": {
    "issue_number": 6461,
    "issue_url": "https://github.com/vllm-project/vllm/issues/6461",
    "title": "[Bug]: No metrics exposed at /metrics with 0.5.2 (0.5.1 is fine), possible regression?",
    "body": "### Your current environment\n\n```text\r\nCollecting environment information...                                                     \r\nPyTorch version: 2.3.1+cu121                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \r\nIs debug build: False                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \r\nCUDA used to build PyTorch: 12.1                                                          \r\nROCM used to build PyTorch: N/A                                                                                                                                                      \r\n                                                                                                                                                                                     \r\nOS: Ubuntu 22.04.4 LTS (x86_64)                                                                                                                                                      \r\nGCC version: (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0                                        \r\nClang version: Could not collect                                                          \r\nCMake version: version 3.30.0                                                             \r\nLibc version: glibc-2.35                                                                  \r\n                                                                                                                                                                                     \r\nPython version: 3.10.12 (main, Nov 20 2023, 15:14:05) [GCC 11.4.0] (64-bit runtime)                                                                                                  \r\nPython platform: Linux-5.15.0-107-generic-x86_64-with-glibc2.35                                                                                                                      \r\nIs CUDA available: True                                                                   \r\nCUDA runtime version: Could not collect                                                   \r\nCUDA_MODULE_LOADING set to: LAZY                                                          \r\nGPU models and configuration:                                                             \r\nGPU 0: NVIDIA H100 80GB HBM3                                                              \r\n  MIG 3g.40gb     Device  0:                                                              \r\n                                                                                          \r\nNvidia driver version: 550.54.15                                                          \r\ncuDNN version: Could not collect                                                          \r\nHIP runtime version: N/A                                                                  \r\nMIOpen runtime version: N/A                                                               \r\nIs XNNPACK available: True                                                                \r\n                                                                                          \r\nCPU:                                                                                      \r\nArchitecture:                       x86_64                                                \r\nCPU op-mode(s):                     32-bit, 64-bit                                        \r\nAddress sizes:                      46 bits physical, 57 bits virtual                     \r\nByte Order:                         Little Endian                                                                                                                                    \r\nCPU(s):                             192                                                                                                                                              \r\nOn-line CPU(s) list:                0-191                                                                                                                                            \r\nVendor ID:                          GenuineIntel                                                                                                                                     \r\nModel name:                         Intel(R) Xeon(R) Platinum 8468                        \r\nCPU family:                         6                                                     \r\nModel:                              143                                                   \r\nThread(s) per core:                 2                                                                                                                                                                                                                        \r\nCore(s) per socket:                 48                                                                                                                                                                                                                       \r\nSocket(s):                          2                                                                                                                                                                                                                        \r\nStepping:                           8                                                                                                                                                                                                                        \r\nCPU max MHz:                        3800.0000                                                                                 \r\nCPU min MHz:                        800.0000                                                                                  \r\nBogoMIPS:                           4200.00                                                                                   \r\nFlags:                              fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb cat\r\n_l3 cat_l2 cdp_l3 invpcid_single cdp_l2 ssbd mba ibrs ibpb stibp ibrs_enhanced tpr_shadow vnmi flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local split_lock_detect avx_vnni avx512_bf16 wbnoinvd dtherm ida arat pln pts hwp hwp_act_window hwp_epp hwp_pkg_req avx512vbmi umip pku ospke\r\n waitpkg avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg tme avx512_vpopcntdq la57 rdpid bus_lock_detect cldemote movdiri movdir64b enqcmd fsrm md_clear serialize tsxldtrk pconfig arch_lbr amx_bf16 avx512_fp16 amx_tile amx_int8 flush_l1d arch_capabilities\r\nVirtualization:                     VT-x                                                                                      \r\nL1d cache:                          4.5 MiB (96 instances)                                                                    \r\nL1i cache:                          3 MiB (96 instances)                                                                      \r\nL2 cache:                           192 MiB (96 instances)                                                                    \r\nL3 cache:                           210 MiB (2 instances)                                                                     \r\nNUMA node(s):                       2                                                                                         \r\nNUMA node0 CPU(s):                  0,2,4,6,8,10,12,14,16,18,20,22,24,26,28,30,32,34,36,38,40,42,44,46,48,50,52,54,56,58,60,62,64,66,68,70,72,74,76,78,80,82,84,86,88,90,92,94,96,98,100,102,104,106,108,110,112,114,116,118,120,122,124,126,128,130,132,134,136,138,140,142,144,146,148,150,152,154,156,158,160,162,164,166,168,170,172,174,176,178,180,182,184,186,188,190\r\nNUMA node1 CPU(s):                  1,3,5,7,9,11,13,15,17,19,21,23,25,27,29,31,33,35,37,39,41,43,45,47,49,51,53,55,57,59,61,63,65,67,69,71,73,75,77,79,81,83,85,87,89,91,93,95,97,99,101,103,105,107,109,111,113,115,117,119,121,123,125,127,129,131,133,135,137,139,141,143,145,147,149,151,153,155,157,159,161,163,165,167,169,171,173,175,177,179,181,183,185,187,189,191\r\nVulnerability Gather data sampling: Not affected                                                                              \r\nVulnerability Itlb multihit:        Not affected                                                                              \r\nVulnerability L1tf:                 Not affected                                                                              \r\nVulnerability Mds:                  Not affected                                                                              \r\nVulnerability Meltdown:             Not affected                                                                              \r\nVulnerability Mmio stale data:      Not affected                                                                              \r\nVulnerability Retbleed:             Not affected                                                                              \r\nVulnerability Spec rstack overflow: Not affected                                                                              \r\nVulnerability Spec store bypass:    Mitigation; Speculative Store Bypass disabled via prctl and seccomp                                                                                                                                                      \r\nVulnerability Spectre v1:           Mitigation; usercopy/swapgs barriers and __user pointer sanitization                                                                                                                                                     \r\nVulnerability Spectre v2:           Mitigation; Enhanced IBRS; IBPB conditional; RSB filling; PBRSB-eIBRS SW sequence; BHI BHI_DIS_S                                                                                                                         \r\nVulnerability Srbds:                Not affected                                                                              \r\nVulnerability Tsx async abort:      Not affected                                                                              \r\n\r\nVersions of relevant libraries:                                                                                               \r\n[pip3] flashinfer==0.0.9+cu121torch2.3                                                                                        \r\n[pip3] numpy==1.26.4                                                                                                          \r\n[pip3] nvidia-nccl-cu12==2.20.5                                                                                               \r\n[pip3] torch==2.3.1                                                                                                           \r\n[pip3] torchvision==0.18.1                                                                                                    \r\n[pip3] transformers==4.42.4                                                                                                   \r\n[pip3] triton==2.3.1                                                                                                          \r\n[conda] Could not collect                                                                                                     \r\nROCM Version: Could not collect                                                                                               \r\nNeuron SDK Version: N/A                                                                                                       \r\nvLLM Version: 0.5.2                                                                                                           \r\nvLLM Build Flags:                                                                                                             \r\nCUDA Archs: Not Set; ROCm: Disabled; Neuron: Disabled                                                                         \r\nGPU Topology:                                                                                                                 \r\nGPU0    CPU Affinity    NUMA Affinity   GPU NUMA ID                                                                           \r\nGPU0     X      1,3,5,7,9,11    1               N/A                                                                           \r\n\r\nLegend:                                                                                                                       \r\n\r\n  X    = Self                                                                                                                 \r\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)                                                                                                                                                       \r\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node                                                                                                                                                 \r\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)                                                                                                                                                                        \r\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)                                                                                                                                                               \r\n  PIX  = Connection traversing at most a single PCIe bridge                                                                   \r\n  NV#  = Connection traversing a bonded set of # NVLinks\r\n```\r\n\n\n### üêõ Describe the bug\n\nWhen switching the Container image from `0.5.1` to `0.5.2` the Prometheus metrics usually exposed at /metrics are gone.\r\n\r\n0.5.1 - `INFO:     xxx.xxx.xxx.xxx:56742 - \"GET /metrics HTTP/1.1\" 200 OK`\r\n0.5.2 - `INFO:     xxx.xxx.xxx.xxx:45698 - \"GET /metrics HTTP/1.1\" 404 Not Found`\r\n\r\n\r\n\r\nI looked at the release notes, but did not see any change about a required config parameter or similar to have the metrics enabled.",
    "state": "closed",
    "labels": [
      "bug"
    ],
    "created_at": "2024-07-16T06:32:29Z",
    "closed_at": "2024-07-19T03:55:14Z",
    "author": "frittentheke",
    "comments_count": 8,
    "comments": [
      {
        "author": "youkaichao",
        "body": "cc @simon-mo ",
        "created_at": "2024-07-16T06:34:59Z"
      },
      {
        "author": "frittentheke",
        "body": "@simon-mo @youkaichao Maybe it's this PR: https://github.com/vllm-project/vllm/pull/5090/\r\nLooking around the metrics endpoint, I believe there is an annotation `@router.get(\"/metrics\")` missing somewhere around\r\nhttps://github.com/EthanqX/vllm/blob/8cf22579e36ea95686611ac7e697c02dd29a8aee/vllm/entrypoints/openai/api_server.py#L71-L77\r\n\r\n\r\n\r\nIt might be worth adding a test case for the metrics endpoint being present and containing a certain set of metrics?",
        "created_at": "2024-07-16T06:45:29Z"
      },
      {
        "author": "youkaichao",
        "body": "cc @EthanqX \r\nand cc @DarkLight1337  if you can fix it in #6431 by the way, or add another pr to fix it.",
        "created_at": "2024-07-16T06:53:34Z"
      },
      {
        "author": "frittentheke",
        "body": "@youkaichao @simon-mo  I suppose there is not going to be a bugfix release just for the broken metrics?\r\nBut may I gently and kindly ask when the next regular release is planned? \r\n\r\nBut would be sad of people would not be running latest (and downgraded to 0.5.1) due to the lack of metrics ...",
        "created_at": "2024-07-22T12:28:39Z"
      },
      {
        "author": "DarkLight1337",
        "body": "Please refer to #6434 for more details on release timeline.",
        "created_at": "2024-07-22T12:30:26Z"
      },
      {
        "author": "frittentheke",
        "body": "> Please refer to #6434 for more details on release timeline.\r\n\r\nArgh .. sorry for the noise @DarkLight1337. I was even subscribed to that one :disappointed: ",
        "created_at": "2024-07-22T12:47:08Z"
      },
      {
        "author": "servient-ashwin",
        "body": "1. How do you get certain metrics like say for example `avg generation throughput` or something similar that is available in the `/metrics` endpoint to the caller?\r\nAs it is currently, these metrics being part of the vLLM engine to be specific, sit on the server and the caller isn't aware of them. The server however has a endpoint to monitor, but I can't seem to see how I can access those individually rather than see the hosted ones.\r\nAre there any examples to maybe `curl` this endpoint and grab them individually per request and get it back to the caller or a way to send them with the request. I believe sending it with the request would be easy but would also mean adding things to the OpenAI `chat completions` function itself from what I understand.\r\n\r\n2. @frittentheke how do you collect/use these metrics?",
        "created_at": "2024-07-25T20:26:23Z"
      },
      {
        "author": "frittentheke",
        "body": "\n> 2. @frittentheke how do you collect/use these metrics?\n\nScaping via Prometheus. These are aggregated metrics of the vLLM instance, NOT per request.",
        "created_at": "2024-07-25T20:38:59Z"
      }
    ]
  }
}